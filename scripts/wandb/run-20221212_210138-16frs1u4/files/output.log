Cuda installed! Running on GPU!
Device: cuda:0 Quadro RTX 8000
4.486768969918806 29 29 29 29
Epoch: 50, Loss: 0.0001719198490749916
Epoch: 100, Loss: 0.0001541171517770175
Epoch: 150, Loss: 8.285930189890975e-05
Epoch: 200, Loss: 4.8998845972322306e-05
Epoch: 250, Loss: 0.0001675271787711139
Epoch: 300, Loss: 6.612255218617095e-05
Epoch: 350, Loss: 3.214434377074476e-05
Epoch: 400, Loss: 3.351521123600426e-05
Epoch: 450, Loss: 2.4596214903226206e-05
Epoch: 500, Loss: 1.3794470132011855e-05
Epoch: 550, Loss: 2.480073532354813e-05
Epoch: 600, Loss: 2.226354848102342e-05
Epoch: 650, Loss: 2.858484519009371e-05
Epoch: 700, Loss: 2.572756364036852e-05
Epoch: 750, Loss: 1.1248879480578767e-05
Epoch: 800, Loss: 9.668772708232582e-06
Epoch: 850, Loss: 1.2604858277489595e-05
Epoch: 900, Loss: 1.067881551207133e-05
Epoch: 950, Loss: 1.203634237494612e-05
Epoch: 1000, Loss: 1.6411834142896746e-05
Epoch: 1050, Loss: 2.5168417094858953e-05
Epoch: 1100, Loss: 1.9961800622115938e-05
Epoch: 1150, Loss: 8.876351819753781e-06
Epoch: 1200, Loss: 2.1814802359010556e-05
Epoch: 1250, Loss: 1.1900871419620519e-05
Epoch: 1300, Loss: 1.5419033039986917e-05
Epoch: 1350, Loss: 7.543423857847749e-06
Epoch: 1400, Loss: 7.07897135712966e-06
Epoch: 1450, Loss: 9.777484010815329e-06
Epoch: 1500, Loss: 7.391980218144735e-06
Epoch: 1550, Loss: 7.427011661394998e-06
Epoch: 1600, Loss: 7.269210690495698e-06
Epoch: 1650, Loss: 1.000858291167036e-05
Epoch: 1700, Loss: 9.340901931639269e-06
Epoch: 1750, Loss: 1.3728850398572022e-05
Epoch: 1800, Loss: 4.124661633680053e-06
Epoch: 1850, Loss: 7.593857354873334e-06
Epoch: 1900, Loss: 2.0934146577085386e-05
Epoch: 1950, Loss: 8.8440184698596e-06
Epoch 01981: reducing learning rate of group 0 to 1.0000e-03.
Epoch: 2000, Loss: 2.3347038871576283e-06
Epoch: 2050, Loss: 2.7835357352644406e-06
Epoch: 2100, Loss: 3.0235280040724923e-06
Epoch: 2150, Loss: 3.016807811526309e-06
Epoch: 2200, Loss: 3.565724948936061e-06
Epoch: 2250, Loss: 3.4387688711797762e-06
Epoch: 2300, Loss: 3.2619782867967387e-06
Epoch 02340: reducing learning rate of group 0 to 5.0000e-04.
Epoch: 2350, Loss: 2.1534435440198985e-06
Epoch: 2400, Loss: 1.6340960508378418e-06
Epoch: 2450, Loss: 1.7413804881697174e-06
Epoch: 2500, Loss: 1.494336229954654e-06
Epoch: 2550, Loss: 1.6748826543577116e-06
Epoch: 2600, Loss: 1.5649299282761535e-06
Epoch: 2650, Loss: 1.7580410868708652e-06
Epoch: 2700, Loss: 2.2182982312803303e-06
Epoch: 2750, Loss: 2.072312344916555e-06
Epoch: 2800, Loss: 2.020442396717345e-06
Epoch: 2850, Loss: 2.3490537853363217e-06
Epoch: 2900, Loss: 1.3481936498740842e-06
Epoch: 2950, Loss: 1.3279660670248006e-06
Epoch: 3000, Loss: 1.849502632304976e-06
Epoch: 3050, Loss: 1.7428648419210057e-06
Epoch: 3100, Loss: 1.7665731831971865e-06
Epoch: 3150, Loss: 1.4517645846509572e-06
Epoch: 3200, Loss: 1.7201312702463654e-06
Epoch: 3250, Loss: 1.782921442384325e-06
Epoch: 3300, Loss: 1.6531620954013042e-06
Epoch: 3350, Loss: 1.661951920464643e-06
Epoch 03374: reducing learning rate of group 0 to 2.5000e-04.
Epoch: 3400, Loss: 1.0048741513965157e-06
Epoch: 3450, Loss: 1.1034572917813206e-06
Epoch: 3500, Loss: 1.0368735199122743e-06
Epoch: 3550, Loss: 1.0329078461627898e-06
Epoch: 3600, Loss: 1.050738455995912e-06
Epoch 03638: reducing learning rate of group 0 to 1.2500e-04.
Epoch: 3650, Loss: 9.054641391993108e-07
Epoch: 3700, Loss: 9.379845757224137e-07
Epoch: 3750, Loss: 9.016262178231778e-07
Epoch: 3800, Loss: 9.455328377806189e-07
Epoch: 3850, Loss: 8.657260067951556e-07
Epoch: 3900, Loss: 9.240288083163944e-07
Epoch: 3950, Loss: 9.044565802049551e-07
Epoch: 4000, Loss: 9.041744784158075e-07
Epoch: 4050, Loss: 8.668020028325698e-07
Epoch: 4100, Loss: 8.572747181538591e-07
Epoch: 4150, Loss: 9.058763676728893e-07
Epoch: 4200, Loss: 9.608387765668065e-07
Epoch: 4250, Loss: 8.173174524098776e-07
Epoch: 4300, Loss: 8.966445225069059e-07
Epoch: 4350, Loss: 8.253979589324652e-07
Epoch: 4400, Loss: 9.275027493180834e-07
Epoch: 4450, Loss: 8.70169125790352e-07
Epoch: 4500, Loss: 9.61084787854248e-07
Epoch: 4550, Loss: 8.084871145485076e-07
Epoch: 4600, Loss: 8.410939787525824e-07
Epoch: 4650, Loss: 8.222184281788395e-07
Epoch: 4700, Loss: 8.483417623080925e-07
Epoch: 4750, Loss: 8.554029607545008e-07
Epoch: 4800, Loss: 9.381589288847877e-07
Epoch: 4850, Loss: 7.913377559900934e-07
Epoch: 4900, Loss: 8.710552934670466e-07
Epoch: 4950, Loss: 8.440675879016943e-07
./Example-1.py:362: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:204.)
  Td_pred = model(torch.FloatTensor(X_all).T)
Loading time: 0.05 minutes
/home/taufikmh/KAUST/fall_2022/external_repos/HCPINNsEikonal-dev/src/hcpinnseikonal/plot.py:99: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  plt.figure(figsize=(5,3))
Epoch 0, Loss 0.0326298
Epoch 3, Loss 0.0138097
Epoch 6, Loss 0.0109537
Epoch 9, Loss 0.0080759
Epoch 12, Loss 0.0097209
Epoch 15, Loss 0.0079959
Epoch 18, Loss 0.0058692
Epoch 21, Loss 0.0056364
Epoch 24, Loss 0.0051961
Epoch 27, Loss 0.0051129
Epoch 30, Loss 0.0050157
Epoch 33, Loss 0.0047946
Epoch 36, Loss 0.0119959
Epoch 39, Loss 0.0051957
Epoch 42, Loss 0.0046444
Epoch 45, Loss 0.0041513
Epoch 48, Loss 0.0039895
Epoch 51, Loss 0.0039724
Epoch 54, Loss 0.0036291
Epoch 57, Loss 0.0036607
Epoch 60, Loss 0.0035207
Epoch 63, Loss 0.0037001
Epoch 66, Loss 0.0032817
Epoch 69, Loss 0.0031651
Epoch 72, Loss 0.0028978
Epoch 75, Loss 0.0027481
Epoch 78, Loss 0.0025041
Epoch 81, Loss 0.0024797
Epoch 84, Loss 0.0023867
Epoch 87, Loss 0.0024901
Epoch 90, Loss 0.0023304
Epoch 93, Loss 0.0022302
Epoch 96, Loss 0.0021241
Epoch 99, Loss 0.0021007
Epoch 102, Loss 0.0021164
Epoch 105, Loss 0.0022592
Epoch 108, Loss 0.0020557
Epoch 111, Loss 0.0019510
Epoch 114, Loss 0.0019674
Epoch 117, Loss 0.0021776
Epoch 120, Loss 0.0019895
Epoch 123, Loss 0.0018172
Epoch 126, Loss 0.0018406
Epoch 129, Loss 0.0020548
Epoch 132, Loss 0.0016989
Epoch 135, Loss 0.0018501
Epoch 138, Loss 0.0016839
Epoch 141, Loss 0.0016836
Epoch 144, Loss 0.0016344
Epoch 147, Loss 0.0017162
Epoch 150, Loss 0.0015797
Epoch 153, Loss 0.0015569
Epoch 156, Loss 0.0015629
Epoch 159, Loss 0.0014880
Epoch 162, Loss 0.0016108
Epoch 165, Loss 0.0015496
Epoch 168, Loss 0.0014519
Epoch 171, Loss 0.0016975
Epoch 174, Loss 0.0014244
Epoch 177, Loss 0.0015520
Epoch 180, Loss 0.0014807
Epoch 183, Loss 0.0013702
Epoch 186, Loss 0.0013950
Epoch 189, Loss 0.0014653
Epoch 192, Loss 0.0013867
Epoch 195, Loss 0.0013151
Epoch 198, Loss 0.0013728
Epoch 201, Loss 0.0013322
Epoch 204, Loss 0.0012830
Epoch 207, Loss 0.0012532
Epoch 210, Loss 0.0012673
Epoch 213, Loss 0.0013896
Epoch 216, Loss 0.0013626
Epoch 219, Loss 0.0018282
Epoch 222, Loss 0.0016044
Epoch 225, Loss 0.0015873
Epoch 228, Loss 0.0015021
Epoch 231, Loss 0.0013768
Epoch 234, Loss 0.0014289
Epoch 237, Loss 0.0013033
Epoch 240, Loss 0.0014379
Epoch 243, Loss 0.0013211
Epoch 246, Loss 0.0014152
Epoch 249, Loss 0.0012822
Epoch 252, Loss 0.0012060
Epoch 255, Loss 0.0012581
Epoch 258, Loss 0.0012375
Epoch 261, Loss 0.0011597
Epoch 264, Loss 0.0012541
Epoch 267, Loss 0.0011589
Epoch 270, Loss 0.0011543
Epoch 273, Loss 0.0011708
Epoch 276, Loss 0.0013190
Epoch 279, Loss 0.0011378
Epoch 282, Loss 0.0011611
Epoch 285, Loss 0.0011388
Epoch 288, Loss 0.0011641
Epoch 291, Loss 0.0010980
Epoch 294, Loss 0.0011360
Epoch 297, Loss 0.0011838
Epoch 300, Loss 0.0011843
Epoch 303, Loss 0.0010934
Epoch 306, Loss 0.0011284
Epoch 309, Loss 0.0010914
Epoch 312, Loss 0.0012288
Epoch 315, Loss 0.0011193
Epoch 318, Loss 0.0010640
Epoch 321, Loss 0.0026843
Epoch 324, Loss 0.0010950
Epoch 327, Loss 0.0010792
Epoch 330, Loss 0.0011314
Epoch 333, Loss 0.0010697
Epoch 336, Loss 0.0013058
Epoch 339, Loss 0.0010463
Epoch 342, Loss 0.0011373
Epoch 345, Loss 0.0010444
Epoch 348, Loss 0.0011652
Epoch 351, Loss 0.0011028
Epoch 354, Loss 0.0010262
Epoch 357, Loss 0.0010726
Epoch 360, Loss 0.0010311
Epoch 363, Loss 0.0010330
Epoch 366, Loss 0.0011227
Epoch 369, Loss 0.0010291
Epoch 372, Loss 0.0010237
Epoch 375, Loss 0.0010555
Epoch 378, Loss 0.0010852
Epoch 381, Loss 0.0010756
Epoch 384, Loss 0.0010912
Epoch 387, Loss 0.0010378
Epoch 390, Loss 0.0010805
Epoch 393, Loss 0.0010420
Epoch 396, Loss 0.0009980
Epoch 399, Loss 0.0009853
Epoch 402, Loss 0.0009740
Epoch 405, Loss 0.0009821
Epoch 408, Loss 0.0010001
Epoch 411, Loss 0.0010532
Epoch 414, Loss 0.0009796
Epoch 417, Loss 0.0010233
Epoch 420, Loss 0.0009891
Epoch 423, Loss 0.0009540
Epoch 426, Loss 0.0010872
Epoch 429, Loss 0.0014807
Epoch 432, Loss 0.0010845
Epoch 435, Loss 0.0009674
Epoch 438, Loss 0.0009812
Epoch 441, Loss 0.0010827
Epoch 444, Loss 0.0009990
Epoch 447, Loss 0.0009992
Epoch 450, Loss 0.0009609
Epoch 453, Loss 0.0009454
Epoch 456, Loss 0.0010499
Epoch 459, Loss 0.0009765
Epoch 462, Loss 0.0009987
Epoch 465, Loss 0.0010403
Epoch 468, Loss 0.0009671
Epoch 471, Loss 0.0009228
Epoch 474, Loss 0.0009478
Epoch 477, Loss 0.0010322
Epoch 480, Loss 0.0009470
Epoch 483, Loss 0.0010290
Epoch 486, Loss 0.0009388
Epoch 489, Loss 0.0009105
Epoch 492, Loss 0.0009855
Epoch 495, Loss 0.0010004
Epoch 498, Loss 0.0009628
Epoch 501, Loss 0.0009516
Epoch 504, Loss 0.0009343
Epoch 507, Loss 0.0010035
Epoch 510, Loss 0.0009425
Epoch 513, Loss 0.0008869
Epoch 516, Loss 0.0010452
Epoch 519, Loss 0.0009920
Epoch 522, Loss 0.0010115
