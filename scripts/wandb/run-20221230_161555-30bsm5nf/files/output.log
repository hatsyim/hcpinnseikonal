
Global seed set to 1234
/home/taufikmh/miniconda3/envs/my_env_plot/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py:352: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
  rank_zero_warn(
Using 16bit native Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Global seed set to 1234
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 1 processes
----------------------------------------------------------------------------------------------------
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/taufikmh/KAUST/fall_2022/external_repos/copy-HCPINNsEikonal-dev/src/hcpinnseikonal/distributed.py:690: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  scheduler = {'scheduler': ReduceLROnPlateau(self.optimizer, mode='min', factor=0.5, patience=self.num_epochs//self.reduce_after, verbose=True),
  | Name      | Type                  | Params
----------------------------------------------------
0 | tau_model | FullyConnectedNetwork | 11.8 K
1 | v_model   | FullyConnectedNetwork | 3.1 K
----------------------------------------------------
14.9 K    Trainable params
0         Non-trainable params
14.9 K    Total params
0.030     Total estimated model params size (MB)
Epoch 0:   0%|                                         | 0/5712 [00:00<?, ?it/s]
/home/taufikmh/miniconda3/envs/my_env_plot/lib/python3.8/site-packages/torch/autograd/__init__.py:173: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [1, 32], strides() = [1, 1]
bucket_view.sizes() = [1, 32], strides() = [32, 1] (Triggered internally at  ../torch/csrc/distributed/c10d/reducer.cpp:326.)













Epoch 0:   9%|â–Š        | 500/5712 [00:42<07:21, 11.81it/s, loss=inf, v_num=m5nf]
/home/taufikmh/miniconda3/envs/my_env_plot/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:653: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
  rank_zero_warn("Detected KeyboardInterrupt, attempting graceful shutdown...")
Exception in thread Thread-27:
Traceback (most recent call last):
  File "/home/taufikmh/miniconda3/envs/my_env_plot/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home/taufikmh/miniconda3/envs/my_env_plot/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home/taufikmh/miniconda3/envs/my_env_plot/lib/python3.8/site-packages/torch/utils/data/_utils/pin_memory.py", line 28, in _pin_memory_loop
    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
  File "/home/taufikmh/miniconda3/envs/my_env_plot/lib/python3.8/multiprocessing/queues.py", line 116, in get
    return _ForkingPickler.loads(res)
  File "/home/taufikmh/miniconda3/envs/my_env_plot/lib/python3.8/site-packages/torch/multiprocessing/reductions.py", line 297, in rebuild_storage_fd
    fd = df.detach()
  File "/home/taufikmh/miniconda3/envs/my_env_plot/lib/python3.8/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/home/taufikmh/miniconda3/envs/my_env_plot/lib/python3.8/multiprocessing/resource_sharer.py", line 87, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/home/taufikmh/miniconda3/envs/my_env_plot/lib/python3.8/multiprocessing/connection.py", line 509, in Client
    deliver_challenge(c, authkey)
  File "/home/taufikmh/miniconda3/envs/my_env_plot/lib/python3.8/multiprocessing/connection.py", line 740, in deliver_challenge
    response = connection.recv_bytes(256)        # reject large message
  File "/home/taufikmh/miniconda3/envs/my_env_plot/lib/python3.8/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/home/taufikmh/miniconda3/envs/my_env_plot/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/home/taufikmh/miniconda3/envs/my_env_plot/lib/python3.8/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
ConnectionResetError: [Errno 104] Connection reset by peer
Traceback (most recent call last):
  File "./Example-5.py", line 161, in <module>
    v_pred = model.evaluate_velocity(init_loader,batch_size=BATCH_SIZE,num_pts=X.size)
  File "/home/taufikmh/KAUST/fall_2022/external_repos/copy-HCPINNsEikonal-dev/src/hcpinnseikonal/distributed.py", line 706, in evaluate_velocity
    for i, X in enumerate(data_loader):
  File "/home/taufikmh/KAUST/fall_2022/external_repos/copy-HCPINNsEikonal-dev/src/hcpinnseikonal/utils.py", line 128, in __iter__
    r = torch.randperm(self.dataset_len)
KeyboardInterrupt