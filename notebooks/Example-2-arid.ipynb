{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f8388e1-4ca9-4436-b332-95ad07955303",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Example 2\n",
    "\n",
    "## 3D case\n",
    "\n",
    "**Content**\n",
    "\n",
    "This notebook reproduces the first example of the paper. It consists of four main subheadings;\n",
    "\n",
    "- Importing the *hcpinnseikonal* package functions\n",
    "- Define the arguments for the input parameters\n",
    "- Setup the medium and compute the data\n",
    "- Training and inference\n",
    "\n",
    "**Saving directory**\n",
    "\n",
    "The notebook utilized [*wandb*](https://wandb.ai) for keeping track of the parameters and experiments. You can uncomment the *wandb* call to turn this feature off. Accordingly you need to specify the folder to save your experiment by changing the related *wandb* line inside the main function below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca407d2d-fee6-4703-b01b-b89f579645ca",
   "metadata": {},
   "source": [
    "## 3D Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecf9efe-2c2e-41ec-87be-3d14c1ace174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvista as pv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyvista import examples\n",
    "\n",
    "mesh = examples.download_bunny()\n",
    "mesh.flip_normals()\n",
    "\n",
    "pl = pv.Plotter()\n",
    "pl.add_mesh(mesh, color='lightgrey')\n",
    "pl.background_color = 'white'\n",
    "pl.camera_position = 'xy'\n",
    "\n",
    "pl.show(jupyter_backend='pythreejs')\n",
    "\n",
    "# widget = pl.show(jupyter_backend='pythreejs', return_viewer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba37586c-bde3-4f54-a815-5d4a4863d903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvista as pv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyvista import examples\n",
    "\n",
    "values = np.linspace(0, 10, 1000).reshape((20, 5, 10))\n",
    "\n",
    "# Create the spatial reference\n",
    "grid = pv.UniformGrid()\n",
    "\n",
    "# Set the grid dimensions: shape + 1 because we want to inject our values on\n",
    "#   the CELL data\n",
    "grid.dimensions = np.array(values.shape) + 1\n",
    "\n",
    "# Edit the spatial reference\n",
    "grid.origin = (100, 33, 55.6)  # The bottom left corner of the data set\n",
    "grid.spacing = (1, 5, 2)  # These are the cell sizes along each axis\n",
    "\n",
    "# Add the data values to the cell data\n",
    "grid.cell_data[\"values\"] = values.flatten(order=\"F\")  # Flatten the array!\n",
    "\n",
    "cmap = plt.cm.get_cmap(\"viridis\", 4)\n",
    "\n",
    "# Now plot the grid!\n",
    "grid.plot(show_edges=True, cmap=cmap, jupyter_backend='pythreejs', background='white', show_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6ebbc4-5b21-4be6-aac0-4bb1f5f2bee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvista as pv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyvista import examples\n",
    "\n",
    "mesh = examples.load_channels()\n",
    "cmap = plt.cm.get_cmap(\"viridis\", 4)\n",
    "\n",
    "mesh.plot(cmap=cmap, jupyter_backend='pythreejs', background='white', show_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1842afae-695a-4542-b968-aaeba08dfdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvista as pv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyvista import examples\n",
    "\n",
    "mesh = examples.load_channels()\n",
    "cmap = plt.cm.get_cmap(\"viridis\", 4)\n",
    "\n",
    "slices = mesh.slice_orthogonal(x=20, y=20, z=30)\n",
    "slices.plot(cmap=cmap, jupyter_backend='pythreejs', background='white', show_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e5304d-f97a-4688-88a1-011a946aca5d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import *hcpinnseikonal* package functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae510489-e60e-494e-ade6-470fc69977d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lateral_spacing': 0.01, 'vertical_spacing': 0.01, 'max_offset': 5.0, 'max_depth': 1.0, 'rec_spacing': 10, 'sou_spacing': 10, 'num_epochs': 250, 'num_neurons': 20, 'num_layers': 10, 'learning_rate': 0.001, 'model_type': 'seam', 'data_type': 'full', 'middle_shot': 'n', 'until_cmb': 'n', 'earth_scale': 'n', 'scale_factor': 10, 'reduce_after': 15, 'seed': 123, 'initialization': 'varianceScaling', 'plotting_factor': 1, 'rescale_plot': 'n', 'depth_shift': 'n', 'tau_multiplier': 3.0, 'initial_velocity': 4, 'zid_source': 5, 'zid_receiver': 0, 'explode_reflector': 'n', 'field_synthetic': 'n', 'v_multiplier': 3, 'activation': 'elu', 'num_points': 1.0, 'irregular_grid': 'n', 'xid_well': 5, 'last_vmultiplier': 5, 'nu_units': 'unitless', 'well_depth': None, 'exp_function': 'n', 'exp_factor': 1.0, 'exclude_topo': 'n', 'exclude_well': 'n', 'exclude_source': 'n', 'loss_function': 'mse', 'station_factor': 1.0, 'event_factor': 1.0, 'checker_size': 5.0, 'tau_act': 'None', 'empty_middle': 'n', 'factorization_type': 'multiplicative', 'causality_factor': 1.0, 'causality_weight': 'type_0', 'residual_network': 'n', 'velocity_loss': 'n', 'regular_station': 'n', 'data_neurons': 16, 'data_layers': 8, 'append_shot': 'n', 'use_wandb': 'n', 'save_folder': './', 'project_name': 'GFATT_PINNs-11-pytorch-surface-inversion', 'regularization_type': 'None', 'regularization_weight': 0.0}\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from argparse import ArgumentParser   \n",
    "from scipy import interpolate\n",
    "\n",
    "from hcpinnseikonal.utils import *\n",
    "from hcpinnseikonal.model import *\n",
    "from hcpinnseikonal.train3d import *\n",
    "from hcpinnseikonal.plot import *\n",
    "from hcpinnseikonal.arguments import *\n",
    "\n",
    "args = parser.parse_args([])\n",
    "\n",
    "args.use_wandb='n'\n",
    "\n",
    "dict_args = vars(args)\n",
    "print(dict_args)\n",
    "\n",
    "# Change these lines for the wandb setup\n",
    "if args.use_wandb=='y':\n",
    "    wandb.init(project=args.project_name)\n",
    "    wandb.run.log_code(\".\")\n",
    "    wandb_dir = wandb.run.dir\n",
    "else:\n",
    "    args.save_folder='../saves/saves_aridDeep3d'\n",
    "    from pathlib import Path\n",
    "    Path(args.save_folder).mkdir(parents=True, exist_ok=True)\n",
    "    wandb_dir = args.save_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36894ab-ca26-436c-9e0f-3e12b9a34d33",
   "metadata": {},
   "source": [
    "## Define arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2c6fc60-cac1-4243-a81e-da4d4d5f23d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lateral_spacing': 0.0375, 'vertical_spacing': 0.009375, 'max_offset': 4.9875, 'max_depth': 1.865625, 'rec_spacing': 20, 'sou_spacing': 20, 'num_epochs': 1000, 'num_neurons': 48, 'num_layers': 48, 'learning_rate': 0.0005, 'model_type': 'arid', 'data_type': 'full', 'middle_shot': 'n', 'until_cmb': 'y', 'earth_scale': 'n', 'scale_factor': 2, 'reduce_after': 50, 'seed': 1234, 'initialization': 'varianceScaling', 'plotting_factor': 1, 'rescale_plot': 'n', 'depth_shift': 'n', 'tau_multiplier': 1, 'initial_velocity': 3, 'zid_source': 1, 'zid_receiver': 0, 'explode_reflector': 'n', 'field_synthetic': 'n', 'v_multiplier': 3, 'activation': 'elu', 'num_points': 1.0, 'irregular_grid': 'y', 'xid_well': 5, 'last_vmultiplier': 5, 'nu_units': 'unitless', 'well_depth': None, 'exp_function': 'n', 'exp_factor': 1.0, 'exclude_topo': 'n', 'exclude_well': 'n', 'exclude_source': 'n', 'loss_function': 'mse', 'station_factor': 0.2, 'event_factor': 0.9, 'checker_size': 5.0, 'tau_act': 'tanh', 'empty_middle': 'n', 'factorization_type': 'additive', 'causality_factor': 0.5, 'causality_weight': 'type_0', 'residual_network': 'y', 'velocity_loss': 'n', 'regular_station': 'y', 'data_neurons': 16, 'data_layers': 8, 'append_shot': 'n', 'use_wandb': 'n', 'save_folder': '../saves/saves_aridDeep3d', 'project_name': 'GFATT_PINNs-11-pytorch-surface-inversion', 'regularization_type': 'None', 'regularization_weight': 0.0}\n"
     ]
    }
   ],
   "source": [
    "args.scale_factor=2 \n",
    "args.until_cmb='y' \n",
    "args.num_epochs=1000\n",
    "args.seed=1234 \n",
    "args.learning_rate=5e-4\n",
    "args.rescale_plot='n' \n",
    "args.initial_velocity=3 \n",
    "args.zid_source=1 \n",
    "args.zid_receiver=0 \n",
    "args.data_type='full' \n",
    "args.irregular_grid='y' \n",
    "args.num_layers=12 \n",
    "args.model_type='arid' \n",
    "args.v_multiplier=3 \n",
    "args.factorization_type='additive' \n",
    "args.tau_act='tanh' \n",
    "args.tau_multiplier=1 \n",
    "args.max_offset=4.9875\n",
    "args.max_depth=1.865625 \n",
    "args.vertical_spacing=0.009375 \n",
    "args.lateral_spacing=0.0375\n",
    "args.num_neurons=24 \n",
    "args.causality_factor=.5 \n",
    "\n",
    "args.causality_weight='type_0' \n",
    "args.reduce_after=50 \n",
    "args.field_synthetic='n' \n",
    "args.event_factor=0.9 \n",
    "args.station_factor=0.2 \n",
    "args.residual_network='y' \n",
    "args.empty_middle='n' \n",
    "args.regular_station='y' \n",
    "args.rec_spacing=20 \n",
    "args.sou_spacing=20\n",
    "\n",
    "args.num_layers=48\n",
    "args.num_neurons=48\n",
    "\n",
    "dict_args=vars(args)\n",
    "print(dict_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcd207d-1ca0-492d-87ad-ee85b2f2ef13",
   "metadata": {},
   "source": [
    "## Medium setup and data interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a49dd75-33c0-4616-a997-72e405a3f7cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda installed! Running on GPU!\n",
      "Device: cuda:0 Quadro RTX 8000\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "if args.use_wandb=='y':\n",
    "    wandb.config.update(args)\n",
    "\n",
    "seed = args.seed\n",
    "\n",
    "set_seed(seed)\n",
    "device = set_device()\n",
    "\n",
    "# Medium\n",
    "data_type = args.data_type\n",
    "deltar = args.rec_spacing\n",
    "deltas = args.sou_spacing\n",
    "\n",
    "# Computational model parameters\n",
    "zmin = -0.1 if args.field_synthetic=='y' else 0; zmax = args.max_depth; deltaz = args.vertical_spacing;\n",
    "ymin = 0.; ymax = args.max_offset; deltay = args.lateral_spacing;\n",
    "xmin = 0.; xmax = args.max_offset; deltax = args.lateral_spacing;\n",
    "\n",
    "if args.earth_scale=='y':\n",
    "    earth_radi = 6371/args.scale_factor # Average in km\n",
    "    xmin, xmax, deltax = earth_radi*xmin, earth_radi*xmax, earth_radi*deltax\n",
    "    ymin, ymax, deltay = earth_radi*ymin, earth_radi*ymax, earth_radi*deltay\n",
    "    zmin, zmax, deltaz = earth_radi*zmin, earth_radi*zmax, earth_radi*deltaz\n",
    "\n",
    "# Creating grid, extending the velocity model, and prepare list of grid points for training (X_star)\n",
    "z = np.arange(zmin,zmax+deltaz,deltaz)\n",
    "nz = z.size\n",
    "\n",
    "y = np.arange(ymin,ymax+deltay,deltay)\n",
    "ny = y.size\n",
    "\n",
    "x = np.arange(xmin,xmax+deltax,deltax)\n",
    "nx = x.size\n",
    "\n",
    "Z,Y,X = np.meshgrid(z,y,x,indexing='ij')\n",
    "\n",
    "# Number of training points\n",
    "num_tr_pts = 4000 #int(args.num_points * nz * nx)\n",
    "\n",
    "if args.field_synthetic=='y':\n",
    "    import pandas as pd\n",
    "    import pygmt\n",
    "    import numpy as np\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    # Earthquake events location\n",
    "    location = pd.read_csv('/home/taufikmh/KAUST/fall_2022/GFATT_PINNs/data/fang_etal_2020/sjfzcatlog.csv')\n",
    "\n",
    "    # Recorded traveltime data\n",
    "    traveltime = pd.read_table('/home/taufikmh/KAUST/fall_2022/GFATT_PINNs/data/fang_etal_2020/sjfz_traveltime.dat', delim_whitespace='y')\n",
    "\n",
    "    # Rounding to make the coordinates rounding the same\n",
    "    location, traveltime = location.round(3), traveltime.round(3)\n",
    "\n",
    "    # Merge\n",
    "    data = pd.merge(traveltime, location,  how='left', left_on=['evlat','evlon','evdep'], right_on = ['evlat','evlon','evdep'])\n",
    "\n",
    "    # Create earthquake group\n",
    "    data['event_id'] = data.groupby(['evlat', 'evlon', 'evdep']).cumcount() + 1\n",
    "    data['station_id'] = data.groupby(['stlat', 'stlon', 'stele']).cumcount() + 1\n",
    "\n",
    "    # Station only\n",
    "    sta_only = data.drop_duplicates(subset=['stlat', 'stlon'], keep='last')\n",
    "\n",
    "    # Event only\n",
    "    eve_only = data.drop_duplicates(subset=['evlat', 'evlon'], keep='last')\n",
    "\n",
    "    region = [-118, -115, 32.5, 34.50]\n",
    "    x0,x1,y0,y1 = -117.45, -115.55, 34.15, 32.76\n",
    "\n",
    "    # eve_only['dist_to_line'] = \n",
    "    p1=np.array([(360+x0)*np.ones_like(eve_only.event_id.values), y0*np.ones_like(eve_only.event_id.values)])\n",
    "    p2=np.array([(360+x1)*np.ones_like(eve_only.event_id.values), y1*np.ones_like(eve_only.event_id.values)])\n",
    "    p3=np.array([eve_only.evlon, eve_only.evlat])\n",
    "\n",
    "    d = pd.DataFrame(np.cross((p2-p1).T,(p3-p1).T)/np.linalg.norm((p2-p1).T))\n",
    "    eve_only.loc[:, 'closest_event'] = np.copy(d[0].values)\n",
    "\n",
    "    # sta_only['dist_to_line'] = \n",
    "    p1=np.array([(360+x0)*np.ones_like(sta_only.station_id.values), y0*np.ones_like(sta_only.station_id.values)])\n",
    "    p2=np.array([(360+x1)*np.ones_like(sta_only.station_id.values), y1*np.ones_like(sta_only.station_id.values)])\n",
    "    p3=np.array([sta_only.stlon, sta_only.stlat])\n",
    "\n",
    "    d = pd.DataFrame(np.cross((p2-p1).T,(p3-p1).T)/np.linalg.norm((p2-p1).T))\n",
    "    sta_only.loc[:, 'closest_station'] = np.copy(d[0].values)\n",
    "\n",
    "    closest_sta = sta_only[np.abs(sta_only['closest_station'])<0.003]\n",
    "    closest_eve = eve_only[np.abs(eve_only['closest_event'])<0.00003]\n",
    "\n",
    "    grid = pygmt.datasets.load_earth_relief(resolution=\"03m\", region=region)\n",
    "\n",
    "    points = pd.DataFrame(\n",
    "        data=np.linspace(start=(x0, y0), stop=(x1, y1), num=len(x)),\n",
    "        columns=[\"x\", \"y\"],\n",
    "    )\n",
    "\n",
    "    track = pygmt.grdtrack(points=points, grid=grid, newcolname=\"elevation\")\n",
    "    xtop = track.x.values + 360\n",
    "    ztop = track.elevation.values*1e-3\n",
    "\n",
    "    xsta = closest_sta.stlon.values\n",
    "    zsta = closest_sta.stele.values\n",
    "\n",
    "    xeve = closest_eve.evlon.values\n",
    "    zeve = closest_eve.evdep.values\n",
    "\n",
    "    xtop,xsta,xeve = xtop-xtop.min(),xsta-xsta.min(),xeve-xeve.min()\n",
    "    xtop,xsta,xeve = xtop/xtop.max()*xmax,xsta/xsta.max()*xmax,xeve/xeve.max()*xmax\n",
    "    \n",
    "    ytop,ysta,yeve = ytop-ytop.min(),ysta-ysta.min(),yeve-yeve.min()\n",
    "    ytop,ysta,yeve = ytop/ytop.max()*ymax,ysta/ysta.max()*ymax,yeve/yeve.max()*ymax\n",
    "\n",
    "    ztop,zsta,zeve = ztop-ztop.min(),zsta-zsta.min(),zeve-zeve.min()\n",
    "    ztop,zsta,zeve = args.station_factor*ztop/ztop.max()+zmin,args.station_factor*zsta/zsta.max()+zmin,zmax-args.event_factor*zeve/zeve.max()\n",
    "\n",
    "    xsta,xeve = xsta[(xsta>xtop.min()) & (xsta<xtop.max())], xeve[(xeve>xtop.min()) & (xeve<xtop.max())]\n",
    "    ysta,yeve = ysta[(ysta>ytop.min()) & (ysta<ytop.max())], yeve[(yeve>ytop.min()) & (yeve<ytop.max())]\n",
    "    zsta,zeve = zsta[(xsta>xtop.min()) & (xsta<xtop.max())],zeve[(xeve>xtop.min()) & (xeve<xtop.max())]\n",
    "\n",
    "    if args.exclude_topo=='y':\n",
    "        ztop, zsta = zmin*np.ones_like(ztop), zmin*np.ones_like(zsta)\n",
    "\n",
    "    ztop, zsta = zmin-ztop, zmin-zsta\n",
    "\n",
    "    id_sou_z = np.array([]).astype(int)\n",
    "\n",
    "    for szi in zeve.round(2):\n",
    "        sid = np.where(np.abs(z.round(3)-szi)<1e-6)\n",
    "        id_sou_z = np.append(id_sou_z,sid)\n",
    "\n",
    "    id_rec_z = np.array([]).astype(int)\n",
    "\n",
    "    for rzi in zsta.round(2):\n",
    "        sid = np.where(np.abs(z.round(3)-rzi)<1e-6)\n",
    "        id_rec_z = np.append(id_rec_z,sid)\n",
    "\n",
    "    id_sou_y = np.array([]).astype(int)\n",
    "\n",
    "    for syi in yeve.round(2):\n",
    "        sid = np.where(np.abs(y.round(3)-syi)<1.5e-2)\n",
    "        id_sou_y = np.append(id_sou_y,sid)\n",
    "\n",
    "    id_rec_y = np.array([]).astype(int)\n",
    "\n",
    "    for ryi in ysta.round(2):\n",
    "        sid = np.where(np.abs(y.round(3)-ryi)<1.5e-2)\n",
    "        id_rec_y = np.append(id_rec_y,sid)\n",
    "        \n",
    "    id_sou_x = np.array([]).astype(int)\n",
    "\n",
    "    for sxi in xeve.round(2):\n",
    "        sid = np.where(np.abs(x.round(3)-sxi)<1.5e-2)\n",
    "        id_sou_x = np.append(id_sou_x,sid)\n",
    "\n",
    "    id_rec_x = np.array([]).astype(int)\n",
    "\n",
    "    for rxi in xsta.round(2):\n",
    "        sid = np.where(np.abs(x.round(3)-rxi)<1.5e-2)\n",
    "        id_rec_x = np.append(id_rec_x,sid)\n",
    "\n",
    "    id_top_x = []\n",
    "    id_top_y = []\n",
    "    id_top_z = []\n",
    "\n",
    "    for h in range(len(xtop)):\n",
    "\n",
    "        for i in range(len(x)):\n",
    "            if np.abs(xtop[h]-x[i])<1e-2:\n",
    "                id_top_x.append(i)\n",
    "                \n",
    "        for i in range(len(y)):\n",
    "            if np.abs(ytop[h]-y[i])<1e-2:\n",
    "                id_top_y.append(i)\n",
    "\n",
    "        for j in range(len(z)):    \n",
    "            if np.abs(ztop[h]-z[j])<5e-3:\n",
    "                id_top_z.append(j)\n",
    "\n",
    "    if args.regular_station=='y':\n",
    "        id_rec_x = id_top_x[::args.rec_spacing]\n",
    "        id_rec_y = id_top_y[::args.rec_spacing]\n",
    "        id_rec_z = id_top_z[::args.rec_spacing]\n",
    "\n",
    "    if args.append_shot=='y':\n",
    "        for i in range(8):\n",
    "            id_sou_x = np.append(id_sou_x, len(x)-1-2*i)\n",
    "            id_sou_y = np.append(id_sou_y, len(y)-1-2*i)\n",
    "            id_sou_z = np.append(id_sou_z, len(z)-1-int(0.5*i))\n",
    "            \n",
    "    # plt.plot(args.plotting_factor*(xtop-xtop.min()), args.plotting_factor*ztop)\n",
    "    # plt.scatter(args.plotting_factor*(xeve-xtop.min()), args.plotting_factor*zeve)\n",
    "    # plt.scatter(x[id_rec_x], z[id_rec_z], c='y', marker='v')\n",
    "    # plt.title('Cross-section')\n",
    "    # plt.xlabel('X (km)')\n",
    "    # plt.ylabel('Z (km)')\n",
    "    # plt.gca().invert_yaxis()\n",
    "    # plt.axis('tight')\n",
    "    # plt.savefig(os.path.join(wandb_dir, 'cross_section.png'), format='png', bbox_inches=\"tight\")\n",
    "else:\n",
    "    zeve, yeve, xeve = z[args.zid_source]*np.ones_like(x[::deltas]), y[::deltas], x[::deltas]\n",
    "    zsta, ysta, xsta = z[args.zid_receiver]*np.ones_like(x[::deltar]), y[::deltar], x[::deltar]\n",
    "    ztop, ytop, xtop = zmin*np.ones_like(x), np.copy(y), np.copy(x)\n",
    "\n",
    "    idx_all = np.arange(X.size).reshape(X.shape)\n",
    "    \n",
    "    # Sources indices\n",
    "    id_sou = idx_all[args.zid_source, ::deltas, ::deltas].reshape(-1)\n",
    "    \n",
    "    # Receivers indices\n",
    "    id_rec = idx_all[args.zid_receiver, ::deltar, ::deltar].reshape(-1)\n",
    "\n",
    "# Keeping the number of shots fixed while centering the shots location\n",
    "if args.middle_shot=='y':\n",
    "    id_sou_left = x.shape[0]//2-len(id_sou_x)//2\n",
    "    id_sou_x = np.array(range(id_sou_left, id_sou_left+len(id_sou_x)))\n",
    "    id_sou_y = np.array(range(id_sou_left, id_sou_left+len(id_sou_y)))\n",
    "\n",
    "if args.explode_reflector=='y':\n",
    "    id_sou_x = np.arange(0, len(x), args.sou_spacing)\n",
    "    id_sou_y = np.arange(0, len(y), args.sou_spacing)\n",
    "    id_sou_z = np.ones_like(id_sou_x)*(len(z)-1)\n",
    "\n",
    "if args.empty_middle=='y':\n",
    "    id_sou, id_rec = (np.array(id_sou_x)<=(len(x)//2-50))|(np.array(id_sou_x)>=(len(x)//2+50)), (np.array(id_rec_x)<=(len(x)//2-50))|(np.array(id_rec_x)>=(len(x)//2+50))\n",
    "    if args.field_synthetic=='n':\n",
    "        id_sou_x = np.array(id_sou_x)[id_sou]\n",
    "        id_sou_y = np.array(id_sou_y)[id_sou]\n",
    "        id_sou_z = np.array(id_sou_z)[id_sou]\n",
    "    id_rec_x = np.array(id_rec_x)[id_rec]\n",
    "    id_rec_y = np.array(id_rec_y)[id_rec]\n",
    "    id_rec_z = np.array(id_rec_z)[id_rec]\n",
    "\n",
    "sz = Z.reshape(-1)[id_sou]\n",
    "sy = Y.reshape(-1)[id_sou]\n",
    "sx = X.reshape(-1)[id_sou]\n",
    "\n",
    "Z,Y,X,SX = np.meshgrid(z,y,x,sx,indexing='ij')\n",
    "_,_,_,SY = np.meshgrid(z,y,x,sy,indexing='ij')\n",
    "_,_,_,SZ = np.meshgrid(z,y,x,sz,indexing='ij')\n",
    "_,_,_,ID = np.meshgrid(z,y,x,np.arange(sx.size),indexing='ij')\n",
    "\n",
    "## Sources location checkpointing\n",
    "# for i in range(len(id_sou)):\n",
    "#     print(np.unique(SX[:,:,:,i]), np.unique(SY[:,:,:,i]), np.unique(SZ[:,:,:,i]))\n",
    "\n",
    "if args.model_type=='marmousi':\n",
    "    vel = np.fromfile('../data/marmousi.bin', np.float32).reshape(221, 601)\n",
    "    x1 = np.linspace(0, 5, 601)\n",
    "    z1 = np.linspace(0, 1, 221) \n",
    "    x2 = np.linspace(0.25, 5, len(x))\n",
    "    z2 = np.linspace(0.09, 0.55, len(z)) \n",
    "    f = interpolate.interp2d(x1, z1, vel, kind='cubic')\n",
    "    vel = f(x2, z2)\n",
    "    # Augment a 3D velocity volume from 2D data\n",
    "    vel3d = np.repeat(vel[:, np.newaxis, :], len(y), axis=1)\n",
    "elif args.model_type=='seam':\n",
    "    vel = np.load('/home/taufikmh/KAUST/spring_2022/constrained_eikonal/notebooks/PINNtomo/inputs/seam_model/vel_seam.npy')\n",
    "    x1 = np.arange(0,1+0.01,0.01)\n",
    "    z1 = np.arange(0,1+0.01,0.01)\n",
    "    from scipy import interpolate\n",
    "    f = interpolate.interp2d(x1, z1, vel, kind='cubic')\n",
    "    vel = f(x, z)\n",
    "    # Augment a 3D velocity volume from 2D data\n",
    "    vel3d = np.repeat(vel[:, np.newaxis, :], len(y), axis=1)\n",
    "elif args.model_type=='constant':\n",
    "    vel = 4*np.ones((nz,nx))\n",
    "elif args.model_type=='gradient':\n",
    "    vel = 1 + 7*np.meshgrid(x,z)[1]\n",
    "elif args.model_type=='arid':\n",
    "    vel = np.fromfile('../data/seam_arid', np.float32).reshape(400,400,600)/1000\n",
    "    vel3d = np.moveaxis(vel[::3,::3,::3], -1, 0)\n",
    "    \n",
    "# Extending the velocity model in thirs dimension byy repeatin the array\n",
    "velmodel = np.repeat(vel3d[...,np.newaxis], sx.size,axis=2)\n",
    "\n",
    "if args.depth_shift=='y':\n",
    "    zmin, zmax, z, sz, Z, SZ = zmin+5, zmax+5, z+5, sz+5, Z+5, SZ+5\n",
    "\n",
    "X_star = [Z.reshape(-1,1), Y.reshape(-1,1), X.reshape(-1,1), SY.reshape(-1,1), SX.reshape(-1,1)] # Grid points for prediction \n",
    "\n",
    "# Numerical traveltime\n",
    "T_data3d = numerical_traveltime3d(vel3d, len(x), len(y), len(z), len(id_sou), \n",
    "                                  xmin, ymin, zmin, deltax, deltay, deltaz, \n",
    "                                  [np.where(x==X[:,:,:,0].reshape(-1)[id_sou[i]])[0][0] for i in range(len(id_sou))], \n",
    "                                  [np.where(y==Y[:,:,:,0].reshape(-1)[id_sou[i]])[0][0] for i in range(len(id_sou))], \n",
    "                                  [np.where(z==Z[:,:,:,0].reshape(-1)[id_sou[i]])[0][0] for i in range(len(id_sou))])\n",
    "\n",
    "# ZX plane after\n",
    "plot_section(vel3d[:,10,:], 'v_true_zx.png', vmin=np.nanmin(velmodel)+0.1, \n",
    "             vmax=np.nanmax(velmodel)-0.5, save_dir=wandb_dir, aspect='equal',\n",
    "             xmin=xmin, xmax=xmax, zmin=zmin, zmax=zmax, \n",
    "             sx=X[:,:,:,0].reshape(-1)[id_sou],sz=Z[:,:,:,0].reshape(-1)[id_sou],rx=X[:,:,:,0].reshape(-1)[id_rec],rz=Z[:,:,:,0].reshape(-1)[id_rec])\n",
    "\n",
    "# XY plane\n",
    "plot_section(vel3d[5,:,:], 'v_true_xy.png', vmin=np.nanmin(velmodel)+0.1, \n",
    "             vmax=np.nanmax(velmodel)-0.5, save_dir=wandb_dir, aspect='equal',\n",
    "             xmin=xmin, xmax=xmax, zmin=xmin, zmax=xmax, \n",
    "             sx=X[:,:,:,0].reshape(-1)[id_sou],sz=Y[:,:,:,0].reshape(-1)[id_sou],rx=X[:,:,:,0].reshape(-1)[id_rec],rz=Y[:,:,:,0].reshape(-1)[id_rec])\n",
    "\n",
    "# ZY plane\n",
    "plot_section(vel3d[:,:,10], 'v_true_zy.png', vmin=np.nanmin(velmodel)+0.1, \n",
    "             vmax=np.nanmax(velmodel)-0.5, save_dir=wandb_dir, aspect='equal',\n",
    "             xmin=xmin, xmax=xmax, zmin=zmin, zmax=zmax, \n",
    "             sx=Y[:,:,:,0].reshape(-1)[id_sou],sz=Z[:,:,:,0].reshape(-1)[id_sou],rx=Y[:,:,:,0].reshape(-1)[id_rec],rz=Z[:,:,:,0].reshape(-1)[id_rec])\n",
    "\n",
    "# Plots\n",
    "if args.model_type=='checkerboard':\n",
    "    plot_section((6 + 6.5217391304347826*Z[:,:,0])/args.scale_factor, \"v_back.png\", \n",
    "                 save_dir=wandb_dir, aspect='equal',\n",
    "                 xmin=xmin, xmax=xmax, zmin=zmin, zmax=zmax, \n",
    "                 sx=x[id_sou_x],sz=z[id_sou_z],rx=x[id_rec_x],rz=z[id_rec_z])\n",
    "    plot_section(velpert[:,:,0]/args.scale_factor, \"v_pert.png\", \n",
    "                 save_dir=wandb_dir, aspect='equal',\n",
    "                 xmin=xmin, xmax=xmax, zmin=zmin, zmax=zmax, \n",
    "                 sx=x[id_sou_x],sz=z[id_sou_z],rx=x[id_rec_x],rz=z[id_rec_z])\n",
    "\n",
    "# Interpolation\n",
    "Td_nn = np.zeros_like(T_data3d)\n",
    "taudx_nn = np.zeros_like(T_data3d)\n",
    "\n",
    "Ti_data = np.zeros((len(id_rec)*len(id_sou)))\n",
    "xri = np.tile(X.reshape(-1)[id_rec], len(id_sou))\n",
    "yri = np.tile(Y.reshape(-1)[id_rec], len(id_sou))\n",
    "zri = np.tile(Z.reshape(-1)[id_rec], len(id_sou))\n",
    "\n",
    "xsi = np.repeat(X.reshape(-1)[id_sou], len(id_rec))\n",
    "ysi = np.repeat(Y.reshape(-1)[id_sou], len(id_rec))\n",
    "zsi = np.repeat(Z.reshape(-1)[id_sou], len(id_rec))\n",
    "\n",
    "for i in range(len(id_sou)):\n",
    "    Ti_data[i*len(id_rec):(i+1)*len(id_rec)] = T_data3d[:,:,:,i].reshape(-1)[id_rec]\n",
    "    \n",
    "rand_idx = np.random.permutation(np.arange(len(Ti_data)))\n",
    "\n",
    "X_ori = np.vstack((xri, yri, zri, xsi, ysi, zsi)).T\n",
    "y_ori = Ti_data\n",
    "\n",
    "X_all = X_ori[rand_idx,:]\n",
    "y_all = y_ori[rand_idx]\n",
    "\n",
    "X_all = torch.from_numpy(X_all).float()\n",
    "y_all = torch.from_numpy(y_all).float()\n",
    "\n",
    "X_ori = torch.from_numpy(X_ori).float()\n",
    "\n",
    "all_dataset = torch.utils.data.TensorDataset(X_all, y_all)\n",
    "\n",
    "# Use Pytorch's functionality to load data in batches. Here we use full-batch training again.\n",
    "all_loader = torch.utils.data.DataLoader(all_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "if data_type=='nn':\n",
    "    torch.manual_seed(8888)\n",
    "    model = FullyConnectedNetwork(6, 1, n_hidden=[args.data_neurons]*args.data_layers, act='elu')\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=5e-4) # best\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=2e-3)\n",
    "    # scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=int(5*args.reduce_after), verbose=True)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=int(4*args.reduce_after), verbose=True)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    model.train()\n",
    "    loss_data = []\n",
    "    for epoch in range(int(5e3)):\n",
    "        total_loss = 0.\n",
    "        model.train()\n",
    "        loss = 0\n",
    "        for x_i, y_i in all_loader:\n",
    "            optimizer.zero_grad()\n",
    "            yest = model(x_i).view(-1)\n",
    "            loss = criterion(yest, y_i)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        if epoch % 50 == 0 and epoch > 0:\n",
    "            print(f'Epoch: {epoch}, Loss: {total_loss / X_all[:,0].detach().numpy().size}')\n",
    "        mean_loss = total_loss / X_all[:,0].detach().numpy().size\n",
    "        # wandb.log({\"data_loss\": mean_loss})\n",
    "        scheduler.step(mean_loss)\n",
    "        loss_data.append(mean_loss)\n",
    "\n",
    "    Td_nn = np.zeros_like(T_data3d)\n",
    "    \n",
    "    if args.field_synthetic=='y':\n",
    "        X_all = [np.tile(x[id_top_x], len(sx)), np.tile(z[id_top_z], len(sz)), \n",
    "                 np.repeat(x[id_sou_x], len(x[id_top_x])), \n",
    "                 np.repeat(z[id_sou_z], len(z[id_top_z]))]\n",
    "    else:\n",
    "        X_all = [np.tile(x, len(sx)), \n",
    "                 np.tile(y, len(sy)),\n",
    "                 np.tile(z[args.zid_receiver]*np.ones_like(x), len(sz)), \n",
    "                 np.repeat(x[id_sou_x], len(x)), \n",
    "                 np.repeat(y[id_sou_y], len(y)), \n",
    "                 np.repeat(z[id_sou_z], len(z))]\n",
    "\n",
    "    model.eval()\n",
    "    Td_pred = model(torch.FloatTensor(X_all).T)\n",
    "\n",
    "    for i in range(len(id_sou)):\n",
    "        Td_nn[:,:,:,i] = Td_pred[i*len(x):(i+1)*len(x)].detach().numpy().reshape(-1)\n",
    "\n",
    "    # Convergence history plot for verification\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes()\n",
    "    ax.semilogy(loss_data)\n",
    "\n",
    "    ax.set_xlabel('Epochs',fontsize=14)\n",
    "\n",
    "    plt.xticks(fontsize=11)\n",
    "\n",
    "    ax.set_ylabel('Loss',fontsize=14)\n",
    "    plt.yticks(fontsize=11);\n",
    "    plt.grid()\n",
    "    plt.savefig(os.path.join(wandb_dir, \"data_loss.png\"), format='png', bbox_inches=\"tight\")\n",
    "\n",
    "    # Save model\n",
    "    torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss_data\n",
    "    }, wandb_dir+'/saved_data_model')\n",
    "\n",
    "# Analytical solution for the known traveltime part\n",
    "if args.depth_shift=='y':\n",
    "    vs = args.initial_velocity #velmodel[np.round((SZ-5)/deltaz).astype(int),np.round(SX/deltax).astype(int),0]\n",
    "else:\n",
    "    vs = vel3d[np.round(SZ/deltaz).astype(int),np.round(SY/deltay).astype(int),np.round(SX/deltax).astype(int)]\n",
    "\n",
    "T0 = np.sqrt((Z-SZ)**2 + (Y-SY)**2 + (X-SX)**2)/vs;\n",
    "px0 = np.divide(X-SX, T0*vs**2, out=np.zeros_like(T0), where=T0!=0)\n",
    "py0 = np.divide(Y-SY, T0*vs**2, out=np.zeros_like(T0), where=T0!=0)\n",
    "pz0 = np.divide(Z-SZ, T0*vs**2, out=np.zeros_like(T0), where=T0!=0)\n",
    "\n",
    "if args.field_synthetic=='y':\n",
    "    xf = np.arange(xmin,xmax+0.1*deltax,0.1*deltax)\n",
    "    zf = np.arange(zmin,zmax+0.1*deltaz,0.1*deltaz)\n",
    "    T_topo = np.zeros((len(zf), len(xf), len(id_sou_x)))\n",
    "    for i in range(len(id_sou_x)):\n",
    "        f = interpolate.interp2d(x, z, T_data3d[:,:,i], kind='cubic')\n",
    "        T_topo[:,:,i] = f(xf, zf)\n",
    "    id_top_x = []\n",
    "    id_top_z = []\n",
    "\n",
    "    for h in range(len(xtop)):\n",
    "\n",
    "        for i in range(len(x)):\n",
    "            if np.abs(xtop[h]-x[i])<1e-2:\n",
    "                id_top_x.append(i)\n",
    "\n",
    "        for j in range(len(z)):    \n",
    "            if np.abs(ztop[h]-z[j])<5e-3:\n",
    "                id_top_z.append(j)\n",
    "\n",
    "    taud_topo = np.divide(T_data3d, T0, where=T0!=0)[id_top_z, id_top_x, :]\n",
    "    T_topo = T_data[id_top_z, id_top_x, :]\n",
    "\n",
    "    taud_topo = np.repeat(taud_topo, nz).reshape(nx,len(id_sou_x),nz).swapaxes(1,2).swapaxes(0,1)\n",
    "    T_topo = np.repeat(T_topo, nz).reshape(nx,len(id_sou_x),nz).swapaxes(1,2).swapaxes(0,1)\n",
    "\n",
    "Td_hc = np.zeros_like(T0)\n",
    "T0_hc = np.zeros_like(T0)\n",
    "taud_hc = np.zeros_like(T0)\n",
    "taudx_hc = np.zeros_like(T0)\n",
    "taudy_hc = np.zeros_like(T0)\n",
    "\n",
    "for i in range(len(id_sou)):\n",
    "    T0_hc[:,:,:,i] = np.moveaxis(np.tile(T0.reshape(X.shape)[args.zid_receiver,:,:,i], nz).reshape(ny,nz,nx), 1, 0)\n",
    "    # np.tile(T0[args.zid_receiver,:,:,i], nz).reshape(nz,ny,nx)\n",
    "\n",
    "    # Numerical\n",
    "    if data_type=='full':\n",
    "        Td_hc[:,:,:,i] = np.moveaxis(np.tile(T_data3d[args.zid_receiver,:,:,i], nz).reshape(ny,nz,nx), 1, 0)\n",
    "    # np.tile(T_data3d[args.zid_receiver,:,:,i], nz).reshape(nz,ny,nx)\n",
    "\n",
    "    # NN-based interpolation\n",
    "    elif data_type=='nn':\n",
    "        Td_hc[:,:,:,i] = Td_nn[:,:,:,i].reshape(nz,ny,nx)\n",
    "\n",
    "    if args.factorization_type=='multiplicative':   \n",
    "        taud_hc[:,:,:,i] = np.divide(Td_hc[:,:,:,i], T0_hc[:,:,:,i], out=np.ones_like(T0_hc[:,:,:,i]),\n",
    "                                   where=T0_hc[:,:,:,i]!=0)\n",
    "    else:\n",
    "        taud_hc[:,:,:,i] = Td_hc[:,:,:,i] - T0_hc[:,:,:,i]\n",
    "\n",
    "    # Numerical\n",
    "    if data_type=='full':\n",
    "        taudy_hc[:,:,:,i] = np.gradient(taud_hc.reshape(X.shape)[:,:,:,i], deltay, axis=1)\n",
    "        taudx_hc[:,:,:,i] = np.gradient(taud_hc.reshape(X.shape)[:,:,:,i], deltax, axis=2)\n",
    "\n",
    "    # NN-based interpolation\n",
    "    elif data_type=='nn':\n",
    "        taudy_hc[:,:,:,i] = np.gradient(taud_hc.reshape(X.shape)[:,:,:,i], deltay, axis=1)        \n",
    "        taudx_hc[:,:,:,i] = np.gradient(taud_hc.reshape(X.shape)[:,:,:,i], deltax, axis=2)\n",
    "\n",
    "if args.field_synthetic=='y':\n",
    "\n",
    "    NAN = np.ones_like(X)\n",
    "    for i in range(z.shape[0]):\n",
    "        for j in range(x.shape[0]):\n",
    "            if z[i] < Z[id_top_z, id_top_x, 0][j]:\n",
    "                NAN[i,j,:] = float(\"Nan\")\n",
    "                \n",
    "# # Interpolation check            \n",
    "# plot_int = len(id_sou)//5 if len(id_sou)>=5 else 1\n",
    "# for i in range(0, len(id_sou), plot_int):\n",
    "#     plt.figure()\n",
    "#     plt.scatter(x[id_rec_x], T_data[id_rec_z,id_rec_x,i], label='Data')\n",
    "#     plt.scatter(x[id_rec_x], Td_hc[id_rec_z,id_rec_x,i], label='Interpolated')\n",
    "#     plt.legend(['Data', 'Interpolation'])\n",
    "#     plt.savefig(os.path.join(wandb_dir, \"interpolation.png\"), format='png', bbox_inches=\"tight\")\n",
    "\n",
    "# Locate source boolean\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "sids = id_sou\n",
    "\n",
    "# Locate source boolean\n",
    "isource = np.ones_like(X_star[0]).reshape(-1,).astype(bool)\n",
    "isource[sids] = False\n",
    "\n",
    "velmodel = vel3d.reshape(-1,1)\n",
    "px0 = px0.reshape(-1,1)\n",
    "py0 = py0.reshape(-1,1)\n",
    "pz0 = pz0.reshape(-1,1)\n",
    "T0 = T0.reshape(-1,1)\n",
    "T_data = T_data3d.reshape(-1,1)\n",
    "taud = taud_hc.reshape(-1,1)\n",
    "\n",
    "if args.factorization_type=='multiplicative':\n",
    "    taud[~isource] = 1.    \n",
    "taudx = taudx_hc.reshape(-1,1)\n",
    "taudy = taudy_hc.reshape(-1,1)\n",
    "index = ID.reshape(-1,1)\n",
    "\n",
    "perm_id = np.random.permutation(X.size-sx.size)\n",
    "\n",
    "input_wsrc = [X, Y, Z, SX, SY, SZ, taud, taudx, taudy, T0, px0, py0, pz0, index]\n",
    "input_wosrc = [i.ravel()[isource.reshape(-1)][perm_id] for i in input_wsrc]\n",
    "if args.field_synthetic=='y':\n",
    "    input_wonan = [i.ravel()[~np.isnan(NAN.ravel()[isource.reshape(-1)][perm_id])] for i in input_wosrc]\n",
    "\n",
    "# Network\n",
    "lay = 'linear'\n",
    "ini = args.initialization\n",
    "bias = 0.2\n",
    "mean = 0.01\n",
    "std = 0.05\n",
    "opttype = 'adam'\n",
    "lr = args.learning_rate\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "if args.residual_network=='n':\n",
    "    tau_model = FullyConnectedNetwork(4, 1, [args.num_neurons]*args.num_layers, last_act=args.tau_act, act=args.activation, lay=lay, last_multiplier=args.tau_multiplier)\n",
    "else:\n",
    "    tau_model = ResidualNetwork(4, 1, num_neurons=args.num_neurons, num_layers=args.num_layers, act=args.activation, lay=lay, last_multiplier=args.tau_multiplier)\n",
    "tau_model.to(device)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "if args.residual_network=='n':\n",
    "    v_model = FullyConnectedNetwork(3, 1, [args.num_neurons//2]*args.num_layers, act=args.activation, lay=lay, last_act='relu', last_multiplier=args.v_multiplier)\n",
    "else:\n",
    "    v_model = ResidualNetwork(3, 1, num_neurons=args.num_neurons//2, act='relu', last_act='relu', num_layers=args.num_layers, lay=lay, last_multiplier=args.v_multiplier)\n",
    "v_model.to(device)\n",
    "v_model.apply(lambda m: init_weights(m, init_type=ini, bias=bias, mean=mean, std=std))\n",
    "\n",
    "perc = args.num_points\n",
    "\n",
    "if args.irregular_grid=='y':\n",
    "    npoints = int(X.size * perc)\n",
    "    ipermute = np.random.permutation(np.arange(X.size))[:npoints]\n",
    "else:\n",
    "    ipermute = None\n",
    "\n",
    "# Compute traveltime with randomly initialized network\n",
    "pde_loader, ic = create_dataloader3d([i.ravel() for i in input_wsrc], sx, sy, sz,\n",
    "                                   shuffle=False, batch_size=2048, fast_loader=True, perm_id=ipermute)\n",
    "\n",
    "if args.exp_function=='y':\n",
    "    rec_op = (1-np.exp((Z.reshape(X.shape)-z[args.zid_receiver])**args.exp_factor))\n",
    "else:\n",
    "    rec_op = Z.reshape(X.shape)\n",
    "\n",
    "# if args.factorization_type=='multiplicative':\n",
    "#     tau_true = np.divide(\n",
    "#         T_data.reshape(X.shape), \n",
    "#         T0.reshape(X.shape), \n",
    "#         out=np.ones_like(T0.reshape(X.shape)), \n",
    "#         where=T0.reshape(X.shape)!=0\n",
    "#     )\n",
    "#     tau_true = tau_true - taud.reshape(X.shape)\n",
    "#     tau_true = np.divide(\n",
    "#         tau_true, \n",
    "#         rec_op, \n",
    "#         out=np.ones_like(Z.reshape(X.shape)), \n",
    "#         where=Z.reshape(X.shape)!=0\n",
    "#     )\n",
    "#     T_true = (rec_op*tau_true + taud.reshape(X.shape))*T0.reshape(X.shape)\n",
    "# else:\n",
    "#     tau_true = T_data.reshape(X.shape) - T0.reshape(X.shape) - taud.reshape(X.shape)\n",
    "#     tau_true = np.divide(\n",
    "#         tau_true, \n",
    "#         rec_op, \n",
    "#         out=np.ones_like(Z.reshape(X.shape)), \n",
    "#         where=Z.reshape(X.shape)!=0\n",
    "#     )\n",
    "#     T_true = rec_op*tau_true + taud.reshape(X.shape) + T0.reshape(X.shape)\n",
    "\n",
    "v_init = evaluate_velocity3d(v_model, pde_loader, X.size, batch_size=2048, device=device)\n",
    "tau_init = evaluate_tau3d(tau_model, pde_loader, X.size, batch_size=2048, device=device)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print('Loading time: %.2f minutes' %(elapsed/60.))\n",
    "\n",
    "# Optimizer\n",
    "if opttype == 'adam':\n",
    "    optimizer = torch.optim.Adam(list(tau_model.parameters()) + list(v_model.parameters()), lr=lr, betas=(0.9, 0.999), eps=1e-5)\n",
    "elif opttype == 'lbfgs':\n",
    "    optimizer = torch.optim.LBFGS(list(tau_model.parameters()) + list(v_model.parameters()), line_search_fn=\"strong_wolfe\")\n",
    "\n",
    "# Scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=args.reduce_after, verbose=True)\n",
    "\n",
    "v_init = v_init.detach().cpu().numpy().reshape(X.shape)\n",
    "\n",
    "# ZX plane after\n",
    "plot_section(v_init.reshape(X.shape)[:,10,:,0], 'v_init_zx.png', vmin=np.nanmin(velmodel)+0.1, \n",
    "             vmax=np.nanmax(velmodel)-0.5, save_dir=wandb_dir, aspect='equal',\n",
    "             xmin=xmin, xmax=xmax, zmin=zmin, zmax=zmax, \n",
    "             sx=X[:,:,:,0].reshape(-1)[id_sou],sz=Z[:,:,:,0].reshape(-1)[id_sou],rx=X[:,:,:,0].reshape(-1)[id_rec],rz=Z[:,:,:,0].reshape(-1)[id_rec])\n",
    "\n",
    "# XY plane\n",
    "plot_section(v_init.reshape(X.shape)[5,:,:,0], 'v_init_xy.png', vmin=np.nanmin(velmodel)+0.1, \n",
    "             vmax=np.nanmax(velmodel)-0.5, save_dir=wandb_dir, aspect='equal',\n",
    "             xmin=xmin, xmax=xmax, zmin=xmin, zmax=xmax, \n",
    "             sx=X[:,:,:,0].reshape(-1)[id_sou],sz=Y[:,:,:,0].reshape(-1)[id_sou],rx=X[:,:,:,0].reshape(-1)[id_rec],rz=Y[:,:,:,0].reshape(-1)[id_rec])\n",
    "\n",
    "# ZY plane\n",
    "plot_section(v_init.reshape(X.shape)[:,:,10,0], 'v_init_zy.png', vmin=np.nanmin(velmodel)+0.1, \n",
    "             vmax=np.nanmax(velmodel)-0.5, save_dir=wandb_dir, aspect='equal',\n",
    "             xmin=xmin, xmax=xmax, zmin=zmin, zmax=zmax, \n",
    "             sx=Y[:,:,:,0].reshape(-1)[id_sou],sz=Z[:,:,:,0].reshape(-1)[id_sou],rx=Y[:,:,:,0].reshape(-1)[id_rec],rz=Z[:,:,:,0].reshape(-1)[id_rec])\n",
    "\n",
    "# if data_type!='full':\n",
    "#     if args.field_synthetic=='y':\n",
    "#         for i in range(0, len(id_sou_x), plot_int):\n",
    "#             plot_horizontal(T_data.reshape(X.shape)[id_top_z,id_top_x,i], Td_nn[id_top_z,id_top_x,i], \n",
    "#                             x*args.plotting_factor,'Interpolation Comparison','T (s)',\n",
    "#                             'T_comp_'+str(i)+'.png','True','NN', \n",
    "#                             save_dir=wandb_dir, id_rec_x=id_rec_x, id_rec_z=id_rec_z)\n",
    "#             plot_horizontal(smooth(np.gradient(T_data.reshape(X.shape)[id_top_z,id_top_x,i]),2,'blackman'), \n",
    "#                             np.gradient(Td_nn[id_top_z,id_top_x,i]), \n",
    "#                             x*args.plotting_factor,'Interpolation Comparison','dT/dx (s/km)',\n",
    "#                             'Tdx_comp_'+str(i)+'.png','True','NN', \n",
    "#                             save_dir=wandb_dir, id_rec_x=id_rec_x, id_rec_z=id_rec_z)\n",
    "#     else:\n",
    "#         for i in range(0, len(id_sou_x), plot_int):\n",
    "#             plot_horizontal(T_data.reshape(X.shape)[0,:,i], Td_nn[0,:,i], \n",
    "#                             x*args.plotting_factor,'Interpolation Comparison','T (s)',\n",
    "#                             'T_comp_'+str(i)+'.png','True','NN', \n",
    "#                             save_dir=wandb_dir, id_rec_x=id_rec_x, id_rec_z=id_rec_z)\n",
    "#             plot_horizontal(smooth(np.gradient(T_data.reshape(X.shape)[0,:,i]),2,'blackman'), np.gradient(Td_nn[0,:,i]), \n",
    "#                             x*args.plotting_factor,'Interpolation Comparison','dT/dx (s/km)',\n",
    "#                             'Tdx_comp_'+str(i)+'.png','True','NN', \n",
    "#                             save_dir=wandb_dir, id_rec_x=id_rec_x, id_rec_z=id_rec_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd5b2b9-7c51-4e04-b797-24596d4a0d78",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Visualize 3D model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f5b14d-6a78-4542-acba-a4fa87936d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_idsx = [np.where(x==X[:,:,:,0].reshape(-1)[id_sou[i]])[0][0] for i in range(len(id_sou))]\n",
    "tmp_idsy = [np.where(y==Y[:,:,:,0].reshape(-1)[id_sou[i]])[0][0] for i in range(len(id_sou))]\n",
    "tmp_idsz = [np.where(z==Z[:,:,:,0].reshape(-1)[id_sou[i]])[0][0] for i in range(len(id_sou))]\n",
    "\n",
    "print([np.unique(np.isnan(i)) for i in input_wosrc])\n",
    "\n",
    "for i in range(len(id_sou)):\n",
    "    input_item = taudy.reshape(X.shape)\n",
    "    print(\"Shot number #\"+str(i+1)+\" \"+str(np.unique(input_item[0,:,:,i]==input_item[-1,:,:,i])))\n",
    " \n",
    "for i in range(len(id_sou)):\n",
    "    temp = np.copy(T_data3d.reshape(X.shape))\n",
    "    print(temp[tmp_idsz[i], tmp_idsy[i], tmp_idsx[i], i])\n",
    "\n",
    "for i in range(0,len(id_sou),len(id_sou)//3):\n",
    "    # ZX plane after\n",
    "    print(i,x[np.where(x==X[:,:,:,0].reshape(-1)[id_sou[i]])[0][0]])\n",
    "    plot_section(Td_hc.reshape(X.shape)[:,np.where(y==Y[:,:,:,i].reshape(-1)[id_sou[i]])[0][0],:,i], 'T_data3d_zx.png', save_dir=wandb_dir, aspect='equal',\n",
    "                 xmin=xmin, xmax=xmax, zmin=zmin, zmax=zmax, \n",
    "                 sx=X[:,:,:,i].reshape(-1)[id_sou],sz=Z[:,:,:,i].reshape(-1)[id_sou],rx=X[:,:,:,i].reshape(-1)[id_rec],rz=Z[:,:,:,i].reshape(-1)[id_rec])\n",
    "\n",
    "    # XY plane\n",
    "    plot_section(Td_hc.reshape(X.shape)[args.zid_source,:,:,i], 'T_data3d_xy.png', save_dir=wandb_dir, aspect='equal',\n",
    "                 xmin=xmin, xmax=xmax, zmin=xmin, zmax=xmax, \n",
    "                 sx=X[:,:,:,i].reshape(-1)[id_sou],sz=Y[:,:,:,i].reshape(-1)[id_sou],rx=X[:,:,:,i].reshape(-1)[id_rec],rz=Y[:,:,:,i].reshape(-1)[id_rec])\n",
    "\n",
    "    # ZY plane\n",
    "    plot_section(Td_hc.reshape(X.shape)[:,:,np.where(x==X[:,:,:,i].reshape(-1)[id_sou[i]])[0][0],i], 'T_data3d_zy.png', save_dir=wandb_dir, aspect='equal',\n",
    "                 xmin=xmin, xmax=xmax, zmin=zmin, zmax=zmax, \n",
    "                 sx=Y[:,:,:,i].reshape(-1)[id_sou],sz=Z[:,:,:,i].reshape(-1)[id_sou],rx=Y[:,:,:,i].reshape(-1)[id_rec],rz=Z[:,:,:,i].reshape(-1)[id_rec])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b18fc7-3f7d-401c-8693-62b09a3e0480",
   "metadata": {},
   "source": [
    "## Training and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2c9f90-0fae-478b-bfde-947299ad8e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "import time\n",
    "start_time = time.time()\n",
    "if args.field_synthetic=='y':\n",
    "    loss_history = \\\n",
    "        training_loop3d(\n",
    "            input_wonan, sx, sy, sz,\n",
    "            tau_model, v_model, optimizer, args.num_epochs, \n",
    "            batch_size=Z.size//2000, device=device, scheduler=scheduler,\n",
    "            fast_loader=True, args=dict_args\n",
    "    ) \n",
    "else:\n",
    "    loss_history = \\\n",
    "        training_loop3d(\n",
    "            input_wosrc, sx, sy, sz,\n",
    "            tau_model, v_model, optimizer, args.num_epochs, \n",
    "            batch_size=Z.size//2000, device=device, scheduler=scheduler, \n",
    "            fast_loader=True, args=dict_args\n",
    "    )\n",
    "elapsed = time.time() - start_time\n",
    "print('Training time: %.2f minutes' %(elapsed/60.))\n",
    "\n",
    "# Convergence history plot for verification\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "ax.semilogy(loss_history)\n",
    "ax.set_xlabel('Epochs',fontsize=14)\n",
    "plt.xticks(fontsize=11)\n",
    "ax.set_ylabel('Loss',fontsize=14)\n",
    "plt.yticks(fontsize=11);\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join(wandb_dir, \"loss.png\"), format='png', bbox_inches=\"tight\")\n",
    "\n",
    "# Prediction\n",
    "pde_loader, ic = create_dataloader3d([i.ravel() for i in input_wsrc], sx, sy, sz,\n",
    "                                   shuffle=False, batch_size=2048, fast_loader=True, perm_id=ipermute)\n",
    "v_pred = evaluate_velocity3d(v_model, pde_loader)\n",
    "\n",
    "tau_pred = evaluate_tau3d(tau_model, pde_loader)\n",
    "\n",
    "v_pred = v_pred.detach().cpu().numpy()\n",
    "tau_pred = tau_pred.detach().cpu().numpy()\n",
    "\n",
    "# if args.factorization_type=='multiplicative':\n",
    "#     t_pred = (taud.reshape(X.shape) + rec_op*tau_pred)*T0.reshape(X.shape)\n",
    "# else:\n",
    "#     t_pred = taud.reshape(X.shape) + rec_op*tau_pred.reshape(X.shape) + T0.reshape(X.shape)\n",
    "\n",
    "# v_pred = v_pred.detach().cpu().numpy().reshape(X.shape)[:,:,0]\n",
    "# v_true = velmodel.reshape(Z.shape)[::1,:,0]\n",
    "\n",
    "# if args.rescale_plot=='y':\n",
    "#     earth_radi = args.plotting_factor # Average in km\n",
    "#     xmin, xmax, deltax = earth_radi*xmin, earth_radi*xmax, earth_radi*deltax\n",
    "\n",
    "#     if args.depth_shift=='y':\n",
    "#         zmin, zmax, deltaz = earth_radi*(zmin-5), earth_radi*(zmax-5), earth_radi*deltaz\n",
    "#     else:\n",
    "#         zmin, zmax, deltaz = earth_radi*(zmin-0), earth_radi*(zmax-0), earth_radi*deltaz\n",
    "\n",
    "#     # Creating grid, extending the velocity model, and prepare list of grid points for training (X_star)\n",
    "#     z = np.arange(zmin,zmax+deltaz,deltaz)\n",
    "#     x = np.arange(xmin,xmax+deltax,deltax)\n",
    "\n",
    "#     # Point-source locations\n",
    "#     sz = z[id_sou_z]\n",
    "#     sx = x[id_sou_x]\n",
    "\n",
    "#     Z,X,SX = np.meshgrid(z,x,sx,indexing='ij')\n",
    "\n",
    "#     SZ = np.ones(SX.shape)*sz # Creating an array of sources along z with same size as SX\n",
    "\n",
    "#     t_pred, T_data, T0 = t_pred*args.plotting_factor, T_data*args.plotting_factor, T0*args.plotting_factor\n",
    "\n",
    "# plot_section(v_pred, \"v_pred.png\", vmin=np.nanmin(velmodel)+0.1, vmax=np.nanmax(velmodel)-0.5, \n",
    "#              save_dir=wandb_dir, aspect='equal',\n",
    "#              xmin=xmin, xmax=xmax, zmin=zmin, zmax=zmax, \n",
    "#              sx=x[id_sou_x],sz=z[id_sou_z],rx=x[id_rec_x],rz=z[id_rec_z]) \n",
    "# plot_section(velmodel.reshape(Z.shape)[:,:,0], 'v_true.png', \n",
    "#              vmin=np.nanmin(velmodel)+0.1, vmax=np.nanmax(velmodel)-0.5, \n",
    "#              save_dir=wandb_dir, aspect='equal',\n",
    "#              xmin=xmin, xmax=xmax, zmin=zmin, zmax=zmax, \n",
    "#              sx=x[id_sou_x],sz=z[id_sou_z],rx=x[id_rec_x],rz=z[id_rec_z]) \n",
    "\n",
    "# for i in range(0, len(id_sou_x), plot_int):\n",
    "#     plot_section(tau_true[:,:,i], \"tau_true_\"+str(i)+\".png\", 's/km', \n",
    "#                  vmin=tau_true.min(), vmax=tau_true.max(), \n",
    "#                  save_dir=wandb_dir, aspect='equal',\n",
    "#                  xmin=xmin, xmax=xmax, zmin=zmin, zmax=zmax, \n",
    "#                  sx=x[id_sou_x],sz=z[id_sou_z],rx=x[id_rec_x],rz=z[id_rec_z]) \n",
    "#     plot_section(-tau_pred.reshape(Z.shape)[:,:,i], \"tau_pred_\"+str(i)+\".png\", 's/km', \n",
    "#                  vmin=tau_true.min(), vmax=tau_true.max(), \n",
    "#                  save_dir=wandb_dir, aspect='equal',\n",
    "#                  xmin=xmin, xmax=xmax, zmin=zmin, zmax=zmax, \n",
    "#                  sx=x[id_sou_x],sz=z[id_sou_z],rx=x[id_rec_x],rz=z[id_rec_z]) \n",
    "#     plot_contour(t_pred.reshape(X.shape), T_true.reshape(X.shape),\n",
    "#                  T0.reshape(X.shape), i, nx, nz, len(id_sou_x), sx, sz, x, z,\n",
    "#                  'contour_'+str(i)+'.png', save_dir=wandb_dir)\n",
    "#     plot_section(t_pred.reshape(X.shape)[:,:,i], 't_pred_'+str(i)+'.png', 's', \n",
    "#                  save_dir=wandb_dir, aspect='equal') \n",
    "\n",
    "# v_pred[0,:] = np.copy(v_pred[1,:])\n",
    "\n",
    "# for i in range(0, len(x), len(x)//5):\n",
    "#     plot_trace(v_init, v_true, v_pred, i, x, z, \"v_trace_\"+str(i)+\".png\", save_dir=wandb_dir)\n",
    "\n",
    "# ZX plane after\n",
    "plot_section(v_pred.reshape(X.shape)[:,0,:,i], 'v_pred_zx.png', vmin=np.nanmin(velmodel)+0.1, \n",
    "             vmax=np.nanmax(velmodel)-0.5, save_dir=wandb_dir, aspect='equal',\n",
    "             xmin=xmin, xmax=xmax, zmin=zmin, zmax=zmax, \n",
    "             sx=X[:,:,:,i].reshape(-1)[id_sou],sz=Z[:,:,:,i].reshape(-1)[id_sou],rx=X[:,:,:,i].reshape(-1)[id_rec],rz=Z[:,:,:,i].reshape(-1)[id_rec])\n",
    "\n",
    "# XY plane\n",
    "plot_section(v_pred.reshape(X.shape)[args.zid_source,:,:,i], 'v_pred_xy.png', vmin=np.nanmin(velmodel)+0.1, \n",
    "             vmax=np.nanmax(velmodel)-0.5, save_dir=wandb_dir, aspect='equal',\n",
    "             xmin=xmin, xmax=xmax, zmin=xmin, zmax=xmax, \n",
    "             sx=X[:,:,:,i].reshape(-1)[id_sou],sz=Y[:,:,:,i].reshape(-1)[id_sou],rx=X[:,:,:,i].reshape(-1)[id_rec],rz=Y[:,:,:,i].reshape(-1)[id_rec])\n",
    "\n",
    "# ZY plane\n",
    "plot_section(v_pred.reshape(X.shape)[:,:,0,i], 'v_pred_zy.png', vmin=np.nanmin(velmodel)+0.1, \n",
    "             vmax=np.nanmax(velmodel)-0.5, save_dir=wandb_dir, aspect='equal',\n",
    "             xmin=xmin, xmax=xmax, zmin=zmin, zmax=zmax, \n",
    "             sx=Y[:,:,:,i].reshape(-1)[id_sou],sz=Z[:,:,:,i].reshape(-1)[id_sou],rx=Y[:,:,:,i].reshape(-1)[id_rec],rz=Z[:,:,:,i].reshape(-1)[id_rec])\n",
    "\n",
    "# Save model\n",
    "torch.save({\n",
    "        'tau_model_state_dict': tau_model.state_dict(),\n",
    "        'v_model_state_dict': v_model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss_history\n",
    "}, wandb_dir+'/saved_model')\n",
    "\n",
    "# To load\n",
    "checkpoint = torch.load(wandb_dir+'/saved_model')\n",
    "tau_model.load_state_dict(checkpoint['tau_model_state_dict'])\n",
    "v_model.load_state_dict(checkpoint['v_model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298edf8e-e9ca-4bdc-b09c-d5ee84695daa",
   "metadata": {},
   "source": [
    "## Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b172037-c1ca-419d-bbf2-ee57125751d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save({\n",
    "        'tau_model_state_dict': tau_model.state_dict(),\n",
    "        'v_model_state_dict': v_model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "}, wandb_dir+'/saved_model')\n",
    "\n",
    "# To load\n",
    "checkpoint = torch.load(wandb_dir+'/saved_model')\n",
    "tau_model.load_state_dict(checkpoint['tau_model_state_dict'])\n",
    "v_model.load_state_dict(checkpoint['v_model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664f9a4d-dcad-4a0e-b19e-34b883768f0d",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0627b22-b7f4-47ea-b575-fa0512af5ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load\n",
    "checkpoint = torch.load(wandb_dir+'/saved_model')\n",
    "tau_model.load_state_dict(checkpoint['tau_model_state_dict'])\n",
    "v_model.load_state_dict(checkpoint['v_model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# Prediction\n",
    "pde_loader, ic = create_dataloader3d([i.ravel() for i in input_wsrc], sx, sy, sz,\n",
    "                                   shuffle=False, batch_size=2048, fast_loader=True, perm_id=ipermute)\n",
    "v_pred = evaluate_velocity3d(v_model, pde_loader, X.size, batch_size=2048, device=device)\n",
    "\n",
    "tau_pred = evaluate_tau3d(tau_model, pde_loader, X.size, batch_size=2048, device=device)\n",
    "\n",
    "v_pred = v_pred.detach().cpu().numpy()\n",
    "tau_pred = tau_pred.detach().cpu().numpy()\n",
    "\n",
    "# ZX plane after\n",
    "plot_section(v_pred.reshape(X.shape)[:,10,:,i], 'v_pred_zx.png', vmin=np.nanmin(velmodel)+0.1, \n",
    "             vmax=np.nanmax(velmodel)-0.5, save_dir=wandb_dir, aspect='equal',\n",
    "             xmin=xmin, xmax=xmax, zmin=zmin, zmax=zmax, \n",
    "             sx=X[:,:,:,i].reshape(-1)[id_sou],sz=Z[:,:,:,i].reshape(-1)[id_sou],rx=X[:,:,:,i].reshape(-1)[id_rec],rz=Z[:,:,:,i].reshape(-1)[id_rec])\n",
    "\n",
    "# XY plane\n",
    "plot_section(v_pred.reshape(X.shape)[5,:,:,i], 'v_pred_xy.png', vmin=np.nanmin(velmodel)+0.1, \n",
    "             vmax=np.nanmax(velmodel)-0.5, save_dir=wandb_dir, aspect='equal',\n",
    "             xmin=xmin, xmax=xmax, zmin=xmin, zmax=xmax, \n",
    "             sx=X[:,:,:,i].reshape(-1)[id_sou],sz=Y[:,:,:,i].reshape(-1)[id_sou],rx=X[:,:,:,i].reshape(-1)[id_rec],rz=Y[:,:,:,i].reshape(-1)[id_rec])\n",
    "\n",
    "# ZY plane\n",
    "plot_section(v_pred.reshape(X.shape)[:,:,10,i], 'v_pred_zy.png', vmin=np.nanmin(velmodel)+0.1, \n",
    "             vmax=np.nanmax(velmodel)-0.5, save_dir=wandb_dir, aspect='equal',\n",
    "             xmin=xmin, xmax=xmax, zmin=zmin, zmax=zmax, \n",
    "             sx=Y[:,:,:,i].reshape(-1)[id_sou],sz=Z[:,:,:,i].reshape(-1)[id_sou],rx=Y[:,:,:,i].reshape(-1)[id_rec],rz=Z[:,:,:,i].reshape(-1)[id_rec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e231c72f-347e-40d6-9a9e-6cc70ec6c523",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "def plot_slice(x, y, z, data, xslice, yslice, zslice, ax=None, vmin=None, vmax=None, fig_name=None, save_dir='./'):\n",
    "    \n",
    "    if ax is None:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "    else:\n",
    "        fig = plt.figure()\n",
    "\n",
    "    data_z = data[zslice,:,:]\n",
    "    data_x = data[:,:,xslice]\n",
    "    data_y = data[:,yslice,:]\n",
    "    \n",
    "    norm = matplotlib.colors.Normalize(vmin=data.min(), vmax=data.max())\n",
    "    cmap = plt.cm.get_cmap('terrain')#plt.cm.\n",
    "    m = plt.cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "    m.set_array([])\n",
    "    # fcolors = m.to_rgba(data.reshape(-1,1))\n",
    "    \n",
    "    # Plot X slice\n",
    "    xs, ys, zs = data.shape\n",
    "    \n",
    "    xplot = ax.plot_surface(np.atleast_2d(x[xslice]), y[:, np.newaxis], z[np.newaxis, :],\n",
    "                            facecolors=m.to_rgba(data_x.T), cmap=cmap) #, vmin=1.5, vmax=8.85)\n",
    "    # Plot Y slice\n",
    "    yplot = ax.plot_surface(x[:, np.newaxis], np.atleast_2d(y[yslice]), z[np.newaxis, :],\n",
    "                            facecolors=m.to_rgba(data_y.T), cmap=cmap) #, vmin=1.5, vmax=8.85)\n",
    "    # Plot Z slice\n",
    "    zplot = ax.plot_surface(x[:, np.newaxis], y[np.newaxis, :], np.atleast_2d(z[zslice]),\n",
    "                            facecolors=m.to_rgba(data_z.T), cmap=cmap) #, vmin=1.5, vmax=8.85)\n",
    "    # zplot.\n",
    "    cbar = plt.colorbar(m, shrink=0.15, aspect=5, location='bottom')\n",
    "    cbar.set_label('km/s')\n",
    "    \n",
    "    ax.invert_zaxis()\n",
    "    ax.set_xlabel('X (km)')\n",
    "    ax.set_ylabel('Y (km)')\n",
    "    ax.set_zlabel('Z (km)')\n",
    "    \n",
    "    if fig_name is not None:\n",
    "        plt.savefig(os.path.join(save_dir, fig_name), \n",
    "                    format='png', bbox_inches=\"tight\")\n",
    "\n",
    "# Computational model parameters\n",
    "zmin = -0.1 if args.field_synthetic=='y' else 0; zmax = args.max_depth; deltaz = args.vertical_spacing;\n",
    "ymin = 0.; ymax = args.max_offset; deltay = args.lateral_spacing;\n",
    "xmin = 0.; xmax = args.max_offset; deltax = args.lateral_spacing;\n",
    "\n",
    "if args.earth_scale=='y':\n",
    "    earth_radi = 6371/args.scale_factor # Average in km\n",
    "    xmin, xmax, deltax = earth_radi*xmin, earth_radi*xmax, earth_radi*deltax\n",
    "    ymin, ymax, deltay = earth_radi*ymin, earth_radi*ymax, earth_radi*deltay\n",
    "    zmin, zmax, deltaz = earth_radi*zmin, earth_radi*zmax, earth_radi*deltaz\n",
    "\n",
    "# Creating grid, extending the velocity model, and prepare list of grid points for training (X_star)\n",
    "z = np.arange(zmin,zmax+deltaz,deltaz)\n",
    "nz = z.size\n",
    "\n",
    "y = np.arange(ymin,ymax+deltay,deltay)\n",
    "ny = y.size\n",
    "\n",
    "x = np.arange(xmin,xmax+deltax,deltax)\n",
    "nx = x.size\n",
    "\n",
    "plot_slice(x, y, z, vel3d.reshape(X[:,:,:,0].shape), 0, y.size-1, z.size-1, fig_name='v_trueCube.png', save_dir=wandb_dir)\n",
    "plot_slice(x, y, z, v_pred.reshape(X.shape)[:,:,:,0], 0, y.size-1, z.size-1, fig_name='v_predCube.png', save_dir=wandb_dir)\n",
    "plot_slice(x, y, z, v_init.reshape(X.shape)[:,:,:,0], 0, y.size-1, z.size-1, fig_name='v_initCube.png', save_dir=wandb_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0f6e30-8007-4028-a75b-68f14c81aff5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
