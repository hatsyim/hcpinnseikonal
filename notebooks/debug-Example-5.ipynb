{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f8388e1-4ca9-4436-b332-95ad07955303",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Example 2\n",
    "\n",
    "## 3D case\n",
    "\n",
    "**Content**\n",
    "\n",
    "This notebook reproduces the first example of the paper. It consists of four main subheadings;\n",
    "\n",
    "- Importing the *hcpinnseikonal* package functions\n",
    "- Define the arguments for the input parameters\n",
    "- Setup the medium and compute the data\n",
    "- Training and inference\n",
    "\n",
    "**Saving directory**\n",
    "\n",
    "The notebook utilized [*wandb*](https://wandb.ai) for keeping track of the parameters and experiments. You can uncomment the *wandb* call to turn this feature off. Accordingly you need to specify the folder to save your experiment by changing the related *wandb* line inside the main function below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc390277-d8a1-4da4-85ab-048780da5d5f",
   "metadata": {},
   "source": [
    "## Distributed.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0dfaa79-f68e-40ff-be63-54c82b10e4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io \n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# Load style @hatsyim\n",
    "# plt.style.use(\"~/science.mplstyle\")\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['xtick.bottom'] = plt.rcParams['xtick.labelbottom'] = False\n",
    "plt.rcParams['xtick.top'] = plt.rcParams['xtick.labeltop'] = True\n",
    "plt.rcParams['figure.figsize'] =  [6.4, 4.8]\n",
    "\n",
    "from hcpinnseikonal.train3d import *\n",
    "from hcpinnseikonal.model import *\n",
    "from hcpinnseikonal.utils import *\n",
    "from hcpinnseikonal.plot import *\n",
    "\n",
    "def setup_medium(args):  \n",
    "    \n",
    "    import numpy as np\n",
    "    from scipy import interpolate\n",
    "    import pykonal\n",
    "    \n",
    "    # Medium\n",
    "    data_type = args['data_type']\n",
    "    deltar = args['rec_spacing']\n",
    "    deltas = args['sou_spacing']\n",
    "    \n",
    "    # Computational model parameters\n",
    "    zmin = -0.1 if args['field_synthetic']=='y' else 0; zmax = args['max_depth'] #; deltaz = args['vertical_spacing'];\n",
    "    ymin = 0.; ymax = args['max_offset'] #; deltay = args['lateral_spacing'];\n",
    "    xmin = 0.; xmax = args['max_offset'] #; deltax = args['lateral_spacing'];\n",
    "    if args['model_type']!='arid':\n",
    "        deltax, deltay, deltaz = args['lateral_spacing'], args['lateral_spacing'], args['vertical_spacing']\n",
    "    else:\n",
    "        deltax, deltay, deltaz = args['sampling_rate']*0.00625/2*4, args['sampling_rate']*0.00625/2*4, args['sampling_rate']*0.00625/2\n",
    "\n",
    "    if args['earth_scale']=='y':\n",
    "        earth_radi = 6371/args['scale_factor'] # Average in km\n",
    "        xmin, xmax, deltax = earth_radi*xmin, earth_radi*xmax, earth_radi*deltax\n",
    "        ymin, ymax, deltay = earth_radi*ymin, earth_radi*ymax, earth_radi*deltay\n",
    "        zmin, zmax, deltaz = earth_radi*zmin, earth_radi*zmax, earth_radi*deltaz\n",
    "\n",
    "    # Creating grid, extending the velocity model, and prepare list of grid points for training (X_star)\n",
    "    z = np.arange(zmin,zmax,deltaz)\n",
    "    nz = z.size\n",
    "\n",
    "    y = np.arange(ymin,ymax,deltay)\n",
    "    ny = y.size\n",
    "\n",
    "    x = np.arange(xmin,xmax,deltax)\n",
    "    nx = x.size\n",
    "\n",
    "    Z,Y,X = np.meshgrid(z,y,x,indexing='ij')\n",
    "\n",
    "    # Number of training points\n",
    "    num_tr_pts = 4000\n",
    "\n",
    "    if args['field_synthetic']=='y':\n",
    "        import pandas as pd\n",
    "        import pygmt\n",
    "        import numpy as np\n",
    "\n",
    "        import pandas as pd\n",
    "\n",
    "        # Earthquake events location\n",
    "        location = pd.read_csv('/home/taufikmh/KAUST/fall_2022/GFATT_PINNs/data/fang_etal_2020/sjfzcatlog.csv')\n",
    "\n",
    "        # Recorded traveltime data\n",
    "        traveltime = pd.read_table('/home/taufikmh/KAUST/fall_2022/GFATT_PINNs/data/fang_etal_2020/sjfz_traveltime.dat', delim_whitespace='y')\n",
    "\n",
    "        # Rounding to make the coordinates rounding the same\n",
    "        location, traveltime = location.round(3), traveltime.round(3)\n",
    "\n",
    "        # Merge\n",
    "        data = pd.merge(traveltime, location,  how='left', left_on=['evlat','evlon','evdep'], right_on = ['evlat','evlon','evdep'])\n",
    "\n",
    "        # Create earthquake group\n",
    "        data['event_id'] = data.groupby(['evlat', 'evlon', 'evdep']).cumcount() + 1\n",
    "        data['station_id'] = data.groupby(['stlat', 'stlon', 'stele']).cumcount() + 1\n",
    "\n",
    "        # Station only\n",
    "        sta_only = data.drop_duplicates(subset=['stlat', 'stlon'], keep='last')\n",
    "\n",
    "        # Event only\n",
    "        eve_only = data.drop_duplicates(subset=['evlat', 'evlon'], keep='last')\n",
    "\n",
    "        region = [-118, -115, 32.5, 34.50]\n",
    "        x0,x1,y0,y1 = -117.45, -115.55, 34.15, 32.76\n",
    "\n",
    "        # eve_only['dist_to_line'] = \n",
    "        p1=np.array([(360+x0)*np.ones_like(eve_only.event_id.values), y0*np.ones_like(eve_only.event_id.values)])\n",
    "        p2=np.array([(360+x1)*np.ones_like(eve_only.event_id.values), y1*np.ones_like(eve_only.event_id.values)])\n",
    "        p3=np.array([eve_only.evlon, eve_only.evlat])\n",
    "\n",
    "        d = pd.DataFrame(np.cross((p2-p1).T,(p3-p1).T)/np.linalg.norm((p2-p1).T))\n",
    "        eve_only.loc[:, 'closest_event'] = np.copy(d[0].values)\n",
    "\n",
    "        # sta_only['dist_to_line'] = \n",
    "        p1=np.array([(360+x0)*np.ones_like(sta_only.station_id.values), y0*np.ones_like(sta_only.station_id.values)])\n",
    "        p2=np.array([(360+x1)*np.ones_like(sta_only.station_id.values), y1*np.ones_like(sta_only.station_id.values)])\n",
    "        p3=np.array([sta_only.stlon, sta_only.stlat])\n",
    "\n",
    "        d = pd.DataFrame(np.cross((p2-p1).T,(p3-p1).T)/np.linalg.norm((p2-p1).T))\n",
    "        sta_only.loc[:, 'closest_station'] = np.copy(d[0].values)\n",
    "\n",
    "        closest_sta = sta_only[np.abs(sta_only['closest_station'])<0.003]\n",
    "        closest_eve = eve_only[np.abs(eve_only['closest_event'])<0.00003]\n",
    "\n",
    "        grid = pygmt.datasets.load_earth_relief(resolution=\"03m\", region=region)\n",
    "\n",
    "        points = pd.DataFrame(\n",
    "            data=np.linspace(start=(x0, y0), stop=(x1, y1), num=len(x)),\n",
    "            columns=[\"x\", \"y\"],\n",
    "        )\n",
    "\n",
    "        track = pygmt.grdtrack(points=points, grid=grid, newcolname=\"elevation\")\n",
    "        xtop = track.x.values + 360\n",
    "        ztop = track.elevation.values*1e-3\n",
    "\n",
    "        xsta = closest_sta.stlon.values\n",
    "        zsta = closest_sta.stele.values\n",
    "\n",
    "        xeve = closest_eve.evlon.values\n",
    "        zeve = closest_eve.evdep.values\n",
    "\n",
    "        xtop,xsta,xeve = xtop-xtop.min(),xsta-xsta.min(),xeve-xeve.min()\n",
    "        xtop,xsta,xeve = xtop/xtop.max()*xmax,xsta/xsta.max()*xmax,xeve/xeve.max()*xmax\n",
    "\n",
    "        ytop,ysta,yeve = ytop-ytop.min(),ysta-ysta.min(),yeve-yeve.min()\n",
    "        ytop,ysta,yeve = ytop/ytop.max()*ymax,ysta/ysta.max()*ymax,yeve/yeve.max()*ymax\n",
    "\n",
    "        ztop,zsta,zeve = ztop-ztop.min(),zsta-zsta.min(),zeve-zeve.min()\n",
    "        ztop,zsta,zeve = args['station_factor']*ztop/ztop.max()+zmin,args['station_factor']*zsta/zsta.max()+zmin,zmax-args['event_factor']*zeve/zeve.max()\n",
    "\n",
    "        xsta,xeve = xsta[(xsta>xtop.min()) & (xsta<xtop.max())], xeve[(xeve>xtop.min()) & (xeve<xtop.max())]\n",
    "        ysta,yeve = ysta[(ysta>ytop.min()) & (ysta<ytop.max())], yeve[(yeve>ytop.min()) & (yeve<ytop.max())]\n",
    "        zsta,zeve = zsta[(xsta>xtop.min()) & (xsta<xtop.max())],zeve[(xeve>xtop.min()) & (xeve<xtop.max())]\n",
    "\n",
    "        if args['exclude_topo']=='y':\n",
    "            ztop, zsta = zmin*np.ones_like(ztop), zmin*np.ones_like(zsta)\n",
    "\n",
    "        ztop, zsta = zmin-ztop, zmin-zsta\n",
    "\n",
    "        id_sou_z = np.array([]).astype(int)\n",
    "\n",
    "        for szi in zeve.round(2):\n",
    "            sid = np.where(np.abs(z.round(3)-szi)<1e-6)\n",
    "            id_sou_z = np.append(id_sou_z,sid)\n",
    "\n",
    "        id_rec_z = np.array([]).astype(int)\n",
    "\n",
    "        for rzi in zsta.round(2):\n",
    "            sid = np.where(np.abs(z.round(3)-rzi)<1e-6)\n",
    "            id_rec_z = np.append(id_rec_z,sid)\n",
    "\n",
    "        id_sou_y = np.array([]).astype(int)\n",
    "\n",
    "        for syi in yeve.round(2):\n",
    "            sid = np.where(np.abs(y.round(3)-syi)<1.5e-2)\n",
    "            id_sou_y = np.append(id_sou_y,sid)\n",
    "\n",
    "        id_rec_y = np.array([]).astype(int)\n",
    "\n",
    "        for ryi in ysta.round(2):\n",
    "            sid = np.where(np.abs(y.round(3)-ryi)<1.5e-2)\n",
    "            id_rec_y = np.append(id_rec_y,sid)\n",
    "\n",
    "        id_sou_x = np.array([]).astype(int)\n",
    "\n",
    "        for sxi in xeve.round(2):\n",
    "            sid = np.where(np.abs(x.round(3)-sxi)<1.5e-2)\n",
    "            id_sou_x = np.append(id_sou_x,sid)\n",
    "\n",
    "        id_rec_x = np.array([]).astype(int)\n",
    "\n",
    "        for rxi in xsta.round(2):\n",
    "            sid = np.where(np.abs(x.round(3)-rxi)<1.5e-2)\n",
    "            id_rec_x = np.append(id_rec_x,sid)\n",
    "\n",
    "        id_top_x = []\n",
    "        id_top_y = []\n",
    "        id_top_z = []\n",
    "\n",
    "        for h in range(len(xtop)):\n",
    "\n",
    "            for i in range(len(x)):\n",
    "                if np.abs(xtop[h]-x[i])<1e-2:\n",
    "                    id_top_x.append(i)\n",
    "\n",
    "            for i in range(len(y)):\n",
    "                if np.abs(ytop[h]-y[i])<1e-2:\n",
    "                    id_top_y.append(i)\n",
    "\n",
    "            for j in range(len(z)):    \n",
    "                if np.abs(ztop[h]-z[j])<5e-3:\n",
    "                    id_top_z.append(j)\n",
    "\n",
    "        if args['regular_station']=='y':\n",
    "            id_rec_x = id_top_x[::args['rec_spacing']]\n",
    "            id_rec_y = id_top_y[::args['rec_spacing']]\n",
    "            id_rec_z = id_top_z[::args['rec_spacing']]\n",
    "\n",
    "        if args['append_shot']=='y':\n",
    "            for i in range(8):\n",
    "                id_sou_x = np.append(id_sou_x, len(x)-1-2*i)\n",
    "                id_sou_y = np.append(id_sou_y, len(y)-1-2*i)\n",
    "                id_sou_z = np.append(id_sou_z, len(z)-1-int(0.5*i))\n",
    "\n",
    "    else:\n",
    "        zeve, yeve, xeve = z[args['zid_source']]*np.ones_like(x[::deltas]), y[::deltas], x[::deltas]\n",
    "        zsta, ysta, xsta = z[args['zid_receiver']]*np.ones_like(x[::deltar]), y[::deltar], x[::deltar]\n",
    "        ztop, ytop, xtop = zmin*np.ones_like(x), np.copy(y), np.copy(x)\n",
    "\n",
    "        idx_all = np.arange(X.size).reshape(X.shape)\n",
    "\n",
    "        # Sources indices\n",
    "        id_sou = idx_all[args['zid_source'], ::deltas, ::deltas].reshape(-1)\n",
    "\n",
    "        # Receivers indices\n",
    "        id_rec = idx_all[args['zid_receiver'], ::deltar, ::deltar].reshape(-1)\n",
    "\n",
    "    # Keeping the number of shots fixed while centering the shots location\n",
    "    if args['middle_shot']=='y':\n",
    "        id_sou_left = x.shape[0]//2-len(id_sou_x)//2\n",
    "        id_sou_x = np.array(range(id_sou_left, id_sou_left+len(id_sou_x)))\n",
    "        id_sou_y = np.array(range(id_sou_left, id_sou_left+len(id_sou_y)))\n",
    "\n",
    "    if args['explode_reflector']=='y':\n",
    "        id_sou_x = np.arange(0, len(x), args['sou_spacing'])\n",
    "        id_sou_y = np.arange(0, len(y), args['sou_spacing'])\n",
    "        id_sou_z = np.ones_like(id_sou_x)*(len(z)-1)\n",
    "\n",
    "    if args['empty_middle']=='y':\n",
    "        id_sou, id_rec = (np.array(id_sou_x)<=(len(x)//2-50))|(np.array(id_sou_x)>=(len(x)//2+50)), (np.array(id_rec_x)<=(len(x)//2-50))|(np.array(id_rec_x)>=(len(x)//2+50))\n",
    "        if args['field_synthetic']=='n':\n",
    "            id_sou_x = np.array(id_sou_x)[id_sou]\n",
    "            id_sou_y = np.array(id_sou_y)[id_sou]\n",
    "            id_sou_z = np.array(id_sou_z)[id_sou]\n",
    "        id_rec_x = np.array(id_rec_x)[id_rec]\n",
    "        id_rec_y = np.array(id_rec_y)[id_rec]\n",
    "        id_rec_z = np.array(id_rec_z)[id_rec]\n",
    "\n",
    "    sz = Z.reshape(-1)[id_sou]\n",
    "    sy = Y.reshape(-1)[id_sou]\n",
    "    sx = X.reshape(-1)[id_sou]\n",
    "\n",
    "    Z,Y,X,SX = np.meshgrid(z,y,x,sx,indexing='ij')\n",
    "    _,_,_,SY = np.meshgrid(z,y,x,sy,indexing='ij')\n",
    "    _,_,_,SZ = np.meshgrid(z,y,x,sz,indexing='ij')\n",
    "    _,_,_,ID = np.meshgrid(z,y,x,np.arange(sx.size),indexing='ij')\n",
    "\n",
    "    ## Sources location checkpointing\n",
    "    # for i in range(len(id_sou)):\n",
    "    #     print(np.unique(SX[:,:,:,i]), np.unique(SY[:,:,:,i]), np.unique(SZ[:,:,:,i]))\n",
    "\n",
    "    if args['model_type']=='marmousi':\n",
    "        vel = np.fromfile('../data/marmousi.bin', np.float32).reshape(221, 601)\n",
    "        x1 = np.linspace(0, 5, 601)\n",
    "        z1 = np.linspace(0, 1, 221) \n",
    "        x2 = np.linspace(0.25, 5, len(x))\n",
    "        z2 = np.linspace(0.09, 0.55, len(z)) \n",
    "        f = interpolate.interp2d(x1, z1, vel, kind='cubic')\n",
    "        vel = f(x2, z2)\n",
    "        # Augment a 3D velocity volume frdeom 2D data\n",
    "        vel3d = np.repeat(vel[:, np.newaxis, :], len(y), axis=1)\n",
    "    elif args['model_type']=='seam':\n",
    "        vel = np.load('/home/taufikmh/KAUST/spring_2022/constrained_eikonal/notebooks/PINNtomo/inputs/seam_model/vel_seam.npy')\n",
    "        x1 = np.arange(0,1+0.01,0.01)\n",
    "        z1 = np.arange(0,1+0.01,0.01)\n",
    "        from scipy import interpolate\n",
    "        f = interpolate.interp2d(x1, z1, vel, kind='cubic')\n",
    "        vel = f(x, z)\n",
    "        # Augment a 3D velocity volume from 2D data\n",
    "        vel3d = np.repeat(vel[:, np.newaxis, :], len(y), axis=1)\n",
    "    elif args['model_type']=='constant':\n",
    "        vel = 4*np.ones((nz,nx))\n",
    "    elif args['model_type']=='gradient':\n",
    "        vel = 1 + 7*np.meshgrid(x,z)[1]\n",
    "    elif args['model_type']=='arid':\n",
    "        vel = np.fromfile('../data/seam_arid', np.float32).reshape(400,400,600)/1000\n",
    "        vel3d = np.moveaxis(vel[::args['sampling_rate'],::args['sampling_rate'],::args['sampling_rate']], -1, 0)\n",
    "\n",
    "    # Extending the velocity model in thirs dimension byy repeatin the array\n",
    "    velmodel = np.repeat(vel3d[...,np.newaxis], sx.size,axis=2)\n",
    "\n",
    "    if args['depth_shift']=='y':\n",
    "        zmin, zmax, z, sz, Z, SZ = zmin+5, zmax+5, z+5, sz+5, Z+5, SZ+5\n",
    "\n",
    "    X_star = [Z.reshape(-1,1), Y.reshape(-1,1), X.reshape(-1,1), SY.reshape(-1,1), SX.reshape(-1,1)] # Grid points for prediction \n",
    "\n",
    "    # Numerical traveltime\n",
    "    T_data3d = numerical_traveltime3d(vel3d, len(x), len(y), len(z), len(id_sou), \n",
    "                                      xmin, ymin, zmin, deltax, deltay, deltaz, \n",
    "                                      [np.where(x==X[:,:,:,0].reshape(-1)[id_sou[i]])[0][0] for i in range(len(id_sou))], \n",
    "                                      [np.where(y==Y[:,:,:,0].reshape(-1)[id_sou[i]])[0][0] for i in range(len(id_sou))], \n",
    "                                      [np.where(z==Z[:,:,:,0].reshape(-1)[id_sou[i]])[0][0] for i in range(len(id_sou))])\n",
    "\n",
    "    # ZX plane after\n",
    "    plot_section(vel3d[:,10,:], 'v_true_zx.png', vmin=np.nanmin(velmodel)+0.1, \n",
    "                 vmax=np.nanmax(velmodel)-0.5, save_dir=args['save_folder'], aspect='equal',\n",
    "                 xmin=xmin, xmax=xmax, zmin=zmin, zmax=zmax, \n",
    "                 sx=X[:,:,:,0].reshape(-1)[id_sou],sz=Z[:,:,:,0].reshape(-1)[id_sou],rx=X[:,:,:,0].reshape(-1)[id_rec],rz=Z[:,:,:,0].reshape(-1)[id_rec])\n",
    "\n",
    "    # XY plane\n",
    "    plot_section(vel3d[5,:,:], 'v_true_xy.png', vmin=np.nanmin(velmodel)+0.1, \n",
    "                 vmax=np.nanmax(velmodel)-0.5, save_dir=args['save_folder'], aspect='equal',\n",
    "                 xmin=xmin, xmax=xmax, zmin=xmin, zmax=xmax, \n",
    "                 sx=X[:,:,:,0].reshape(-1)[id_sou],sz=Y[:,:,:,0].reshape(-1)[id_sou],rx=X[:,:,:,0].reshape(-1)[id_rec],rz=Y[:,:,:,0].reshape(-1)[id_rec])\n",
    "\n",
    "    # ZY plane\n",
    "    plot_section(vel3d[:,:,10], 'v_true_zy.png', vmin=np.nanmin(velmodel)+0.1, \n",
    "                 vmax=np.nanmax(velmodel)-0.5, save_dir=args['save_folder'], aspect='equal',\n",
    "                 xmin=xmin, xmax=xmax, zmin=zmin, zmax=zmax, \n",
    "                 sx=Y[:,:,:,0].reshape(-1)[id_sou],sz=Z[:,:,:,0].reshape(-1)[id_sou],rx=Y[:,:,:,0].reshape(-1)[id_rec],rz=Z[:,:,:,0].reshape(-1)[id_rec])\n",
    "\n",
    "    # Plots\n",
    "    if args['model_type']=='checkerboard':\n",
    "        plot_section((6 + 6.5217391304347826*Z[:,:,0])/args['scale_factor'], \"v_back.png\", \n",
    "                     save_dir=args['save_folder'], aspect='equal',\n",
    "                     xmin=xmin, xmax=xmax, zmin=zmin, zmax=zmax, \n",
    "                     sx=x[id_sou_x],sz=z[id_sou_z],rx=x[id_rec_x],rz=z[id_rec_z])\n",
    "        plot_section(velpert[:,:,0]/args['scale_factor'], \"v_pert.png\", \n",
    "                     save_dir=args['save_folder'], aspect='equal',\n",
    "                     xmin=xmin, xmax=xmax, zmin=zmin, zmax=zmax, \n",
    "                     sx=x[id_sou_x],sz=z[id_sou_z],rx=x[id_rec_x],rz=z[id_rec_z])\n",
    "\n",
    "    # Interpolation\n",
    "    Td_nn = np.zeros_like(T_data3d)\n",
    "    taudx_nn = np.zeros_like(T_data3d)\n",
    "\n",
    "    Ti_data = np.zeros((len(id_rec)*len(id_sou)))\n",
    "    xri = np.tile(X.reshape(-1)[id_rec], len(id_sou))\n",
    "    yri = np.tile(Y.reshape(-1)[id_rec], len(id_sou))\n",
    "    zri = np.tile(Z.reshape(-1)[id_rec], len(id_sou))\n",
    "\n",
    "    xsi = np.repeat(X.reshape(-1)[id_sou], len(id_rec))\n",
    "    ysi = np.repeat(Y.reshape(-1)[id_sou], len(id_rec))\n",
    "    zsi = np.repeat(Z.reshape(-1)[id_sou], len(id_rec))\n",
    "\n",
    "    for i in range(len(id_sou)):\n",
    "        Ti_data[i*len(id_rec):(i+1)*len(id_rec)] = T_data3d[:,:,:,i].reshape(-1)[id_rec]\n",
    "\n",
    "    rand_idx = np.random.permutation(np.arange(len(Ti_data)))\n",
    "\n",
    "    X_ori = np.vstack((xri, yri, zri, xsi, ysi, zsi)).T\n",
    "    y_ori = Ti_data\n",
    "\n",
    "    X_all = X_ori[rand_idx,:]\n",
    "    y_all = y_ori[rand_idx]\n",
    "\n",
    "    X_all = torch.from_numpy(X_all).float()\n",
    "    y_all = torch.from_numpy(y_all).float()\n",
    "\n",
    "    X_ori = torch.from_numpy(X_ori).float()\n",
    "\n",
    "    all_dataset = torch.utils.data.TensorDataset(X_all, y_all)\n",
    "\n",
    "    # Use Pytorch's functionality to load data in batches. Here we use full-batch training again.\n",
    "    all_loader = torch.utils.data.DataLoader(all_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "    if data_type=='nn':\n",
    "        torch.manual_seed(8888)\n",
    "        model = FullyConnectedNetwork(6, 1, n_hidden=[args['data_neurons']]*args['data_layers'], act='elu')\n",
    "        # optimizer = torch.optim.Adam(model.parameters(), lr=5e-4) # best\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=2e-3)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=int(4*args['reduce_after']), verbose=True)\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        model.train()\n",
    "        loss_data = []\n",
    "        for epoch in range(int(5e3)):\n",
    "            total_loss = 0.\n",
    "            model.train()\n",
    "            loss = 0\n",
    "            for x_i, y_i in all_loader:\n",
    "                optimizer.zero_grad()\n",
    "                yest = model(x_i).view(-1)\n",
    "                loss = criterion(yest, y_i)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            if epoch % 50 == 0 and epoch > 0:\n",
    "                print(f'Epoch: {epoch}, Loss: {total_loss / X_all[:,0].detach().numpy().size}')\n",
    "            mean_loss = total_loss / X_all[:,0].detach().numpy().size\n",
    "            # wandb.log({\"data_loss\": mean_loss})\n",
    "            scheduler.step(mean_loss)\n",
    "            loss_data.append(mean_loss)\n",
    "\n",
    "        Td_nn = np.zeros_like(T_data3d)\n",
    "\n",
    "        if args['field_synthetic']=='y':\n",
    "            X_all = [np.tile(x[id_top_x], len(sx)), np.tile(z[id_top_z], len(sz)), \n",
    "                     np.repeat(x[id_sou_x], len(x[id_top_x])), \n",
    "                     np.repeat(z[id_sou_z], len(z[id_top_z]))]\n",
    "        else:\n",
    "            X_all = [np.tile(x, len(sx)), \n",
    "                     np.tile(y, len(sy)),\n",
    "                     np.tile(z[args['zid_receiver']]*np.ones_like(x), len(sz)), \n",
    "                     np.repeat(x[id_sou_x], len(x)), \n",
    "                     np.repeat(y[id_sou_y], len(y)), \n",
    "                     np.repeat(z[id_sou_z], len(z))]\n",
    "\n",
    "        model.eval()\n",
    "        Td_pred = model(torch.FloatTensor(X_all).T)\n",
    "\n",
    "        for i in range(len(id_sou)):\n",
    "            Td_nn[:,:,:,i] = Td_pred[i*len(x):(i+1)*len(x)].detach().numpy().reshape(-1)\n",
    "\n",
    "        # Convergence history plot for verification\n",
    "        fig = plt.figure()\n",
    "        ax = plt.axes()\n",
    "        ax.semilogy(loss_data)\n",
    "\n",
    "        ax.set_xlabel('Epochs',fontsize=14)\n",
    "\n",
    "        plt.xticks(fontsize=11)\n",
    "\n",
    "        ax.set_ylabel('Loss',fontsize=14)\n",
    "        plt.yticks(fontsize=11);\n",
    "        plt.grid()\n",
    "        plt.savefig(os.path.join(wandb_dir, \"data_loss.png\"), format='png', bbox_inches=\"tight\")\n",
    "\n",
    "        # Save model\n",
    "        torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss_data\n",
    "        }, wandb_dir+'/saved_data_model')\n",
    "\n",
    "    # Analytical solution for the known traveltime part\n",
    "    if args['depth_shift']=='y':\n",
    "        vs = args['initial_velocity'] #velmodel[np.round((SZ-5)/deltaz).astype(int),np.round(SX/deltax).astype(int),0]\n",
    "    else:\n",
    "        vs = vel3d[np.round(SZ/deltaz).astype(int),np.round(SY/deltay).astype(int),np.round(SX/deltax).astype(int)]\n",
    "\n",
    "    T0 = np.sqrt((Z-SZ)**2 + (Y-SY)**2 + (X-SX)**2)/vs;\n",
    "    px0 = np.divide(X-SX, T0*vs**2, out=np.zeros_like(T0), where=T0!=0)\n",
    "    py0 = np.divide(Y-SY, T0*vs**2, out=np.zeros_like(T0), where=T0!=0)\n",
    "    pz0 = np.divide(Z-SZ, T0*vs**2, out=np.zeros_like(T0), where=T0!=0)\n",
    "\n",
    "    if args['field_synthetic']=='y':\n",
    "        xf = np.arange(xmin,xmax+0.1*deltax,0.1*deltax)\n",
    "        zf = np.arange(zmin,zmax+0.1*deltaz,0.1*deltaz)\n",
    "        T_topo = np.zeros((len(zf), len(xf), len(id_sou_x)))\n",
    "        for i in range(len(id_sou_x)):\n",
    "            f = interpolate.interp2d(x, z, T_data3d[:,:,i], kind='cubic')\n",
    "            T_topo[:,:,i] = f(xf, zf)\n",
    "        id_top_x = []\n",
    "        id_top_z = []\n",
    "\n",
    "        for h in range(len(xtop)):\n",
    "\n",
    "            for i in range(len(x)):\n",
    "                if np.abs(xtop[h]-x[i])<1e-2:\n",
    "                    id_top_x.append(i)\n",
    "\n",
    "            for j in range(len(z)):    \n",
    "                if np.abs(ztop[h]-z[j])<5e-3:\n",
    "                    id_top_z.append(j)\n",
    "\n",
    "        taud_topo = np.divide(T_data3d, T0, where=T0!=0)[id_top_z, id_top_x, :]\n",
    "        T_topo = T_data[id_top_z, id_top_x, :]\n",
    "\n",
    "        taud_topo = np.repeat(taud_topo, nz).reshape(nx,len(id_sou_x),nz).swapaxes(1,2).swapaxes(0,1)\n",
    "        T_topo = np.repeat(T_topo, nz).reshape(nx,len(id_sou_x),nz).swapaxes(1,2).swapaxes(0,1)\n",
    "\n",
    "    Td_hc = np.zeros_like(T0)\n",
    "    T0_hc = np.zeros_like(T0)\n",
    "    taud_hc = np.zeros_like(T0)\n",
    "    taudx_hc = np.zeros_like(T0)\n",
    "    taudy_hc = np.zeros_like(T0)\n",
    "\n",
    "    for i in range(len(id_sou)):\n",
    "        T0_hc[:,:,:,i] = np.moveaxis(np.tile(T0.reshape(X.shape)[args['zid_receiver'],:,:,i], nz).reshape(ny,nz,nx), 1, 0)\n",
    "        # np.tile(T0[args['zid_receiver'],:,:,i], nz).reshape(nz,ny,nx)\n",
    "\n",
    "        # Numerical\n",
    "        if data_type=='full':\n",
    "            Td_hc[:,:,:,i] = np.moveaxis(np.tile(T_data3d[args['zid_receiver'],:,:,i], nz).reshape(ny,nz,nx), 1, 0)\n",
    "        # np.tile(T_data3d[args['zid_receiver'],:,:,i], nz).reshape(nz,ny,nx)\n",
    "\n",
    "        # NN-based interpolation\n",
    "        elif data_type=='nn':\n",
    "            Td_hc[:,:,:,i] = Td_nn[:,:,:,i].reshape(nz,ny,nx)\n",
    "\n",
    "        if args['factorization_type']=='multiplicative':   \n",
    "            taud_hc[:,:,:,i] = np.divide(Td_hc[:,:,:,i], T0_hc[:,:,:,i], out=np.ones_like(T0_hc[:,:,:,i]),\n",
    "                                       where=T0_hc[:,:,:,i]!=0)\n",
    "        else:\n",
    "            taud_hc[:,:,:,i] = Td_hc[:,:,:,i] - T0_hc[:,:,:,i]\n",
    "\n",
    "        # Numerical\n",
    "        if data_type=='full':\n",
    "            taudy_hc[:,:,:,i] = np.gradient(taud_hc.reshape(X.shape)[:,:,:,i], deltay, axis=1)\n",
    "            taudx_hc[:,:,:,i] = np.gradient(taud_hc.reshape(X.shape)[:,:,:,i], deltax, axis=2)\n",
    "\n",
    "        # NN-based interpolation\n",
    "        elif data_type=='nn':\n",
    "            taudy_hc[:,:,:,i] = np.gradient(taud_hc.reshape(X.shape)[:,:,:,i], deltay, axis=1)        \n",
    "            taudx_hc[:,:,:,i] = np.gradient(taud_hc.reshape(X.shape)[:,:,:,i], deltax, axis=2)\n",
    "\n",
    "    if args['field_synthetic']=='y':\n",
    "\n",
    "        NAN = np.ones_like(X)\n",
    "        for i in range(z.shape[0]):\n",
    "            for j in range(x.shape[0]):\n",
    "                if z[i] < Z[id_top_z, id_top_x, 0][j]:\n",
    "                    NAN[i,j,:] = float(\"Nan\")\n",
    "\n",
    "    # Locate source boolean\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "\n",
    "    sids = id_sou\n",
    "\n",
    "    # Locate source boolean\n",
    "    isource = np.ones_like(X_star[0]).reshape(-1,).astype(bool)\n",
    "    isource[sids] = False\n",
    "\n",
    "    velmodel = vel3d.reshape(-1,1)\n",
    "    px0 = px0.reshape(-1,1)\n",
    "    py0 = py0.reshape(-1,1)\n",
    "    pz0 = pz0.reshape(-1,1)\n",
    "    T0 = T0.reshape(-1,1)\n",
    "    T_data = T_data3d.reshape(-1,1)\n",
    "    taud = taud_hc.reshape(-1,1)\n",
    "\n",
    "    if args['factorization_type']=='multiplicative':\n",
    "        taud[~isource] = 1.    \n",
    "    taudx = taudx_hc.reshape(-1,1)\n",
    "    taudy = taudy_hc.reshape(-1,1)\n",
    "    index = ID.reshape(-1,1)\n",
    "\n",
    "    perm_id = np.random.permutation(X.size-sx.size)\n",
    "    \n",
    "    rx=X[:,:,:,0].reshape(-1)[id_rec]\n",
    "    ry=Y[:,:,:,0].reshape(-1)[id_rec]\n",
    "    rz=Z[:,:,:,0].reshape(-1)[id_rec]\n",
    "    \n",
    "    return X, Y, Z, SX, SY, SZ, taud, taudx, taudy, T0, px0, py0, pz0, index, sx, sy, sz, rx, ry, rz, vel3d, id_rec, id_sou\n",
    "\n",
    "# Lightning Dataset\n",
    "import copy\n",
    "class HCEikonalPINNsData(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        args,\n",
    "        batch_size: int = 2**26 ,\n",
    "        num_workers: int = int(os.cpu_count() / 2),\n",
    "        permute=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.fast_loader = args['fast_loader']\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        X, Y, Z, SX, SY, SZ, taud, taudx, taudy, T0, px0, py0, pz0, index, self.sx, self.sy, self.sz, rx, ry, rz, self.vel3d, self.id_rec, self.id_sou = setup_medium(args)\n",
    "        self.input_list = [X, Y, Z, SX, SY, SZ, taud, taudx, taudy, T0, px0, py0, pz0, index]\n",
    "        if permute:\n",
    "            perm_id = np.random.permutation(X.size-self.sx.size)\n",
    "            self.input_list = [i[perm_id] for i in self.input_list]\n",
    "        self.input_dataset = self.create_dataset([i.ravel() for i in self.input_list])\n",
    "\n",
    "    def prepare_data(self):\n",
    "        dummy_data = None\n",
    "        \n",
    "    def create_dataset(self, input_list):\n",
    "        \n",
    "        XYZ = torch.from_numpy(np.vstack((input_list[0].ravel(), input_list[1].ravel(), input_list[2].ravel())).T).float()\n",
    "        SX = torch.from_numpy(input_list[3]).ravel().float()\n",
    "        SY = torch.from_numpy(input_list[4]).ravel().float()\n",
    "        SZ = torch.from_numpy(input_list[5]).ravel().float()\n",
    "\n",
    "        taud = torch.from_numpy(input_list[6]).ravel().float()\n",
    "        taud_dx = torch.from_numpy(input_list[7]).ravel().float()\n",
    "        taud_dy = torch.from_numpy(input_list[8]).ravel().float()\n",
    "\n",
    "        tana = torch.from_numpy(input_list[9]).ravel().float()\n",
    "        tana_dx = torch.from_numpy(input_list[10]).ravel().float()\n",
    "        tana_dy = torch.from_numpy(input_list[11]).ravel().float()\n",
    "        tana_dz = torch.from_numpy(input_list[12]).ravel().float()\n",
    "\n",
    "        index = torch.from_numpy(input_list[13]).ravel().float()\n",
    "    \n",
    "        return TensorDataset(XYZ, SX, SY, SZ, taud, taud_dx, taud_dy, tana, tana_dx, tana_dy, tana_dz, index)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            self.train_input = self.input_dataset\n",
    "            \n",
    "    def train_dataloader(self):\n",
    "        \n",
    "        if self.fast_loader=='y':\n",
    "            return FastTensorDataLoader(\n",
    "            torch.from_numpy(np.vstack((self.input_list[0].ravel(), self.input_list[1].ravel(), self.input_list[2].ravel())).T).float(),\n",
    "            torch.from_numpy(self.input_list[3]).ravel().float(),\n",
    "            torch.from_numpy(self.input_list[4]).ravel().float(),\n",
    "            torch.from_numpy(self.input_list[5]).ravel().float(),\n",
    "            torch.from_numpy(self.input_list[6]).ravel().float(),\n",
    "            torch.from_numpy(self.input_list[7]).ravel().float(),\n",
    "            torch.from_numpy(self.input_list[8]).ravel().float(),\n",
    "            torch.from_numpy(self.input_list[9]).ravel().float(),\n",
    "            torch.from_numpy(self.input_list[10]).ravel().float(),\n",
    "            torch.from_numpy(self.input_list[11]).ravel().float(),\n",
    "            torch.from_numpy(self.input_list[12]).ravel().float(),\n",
    "            batch_size=self.batch_size, \n",
    "            shuffle=True\n",
    "    )\n",
    "        \n",
    "        else:\n",
    "            return DataLoader(\n",
    "                self.train_input,\n",
    "                batch_size=self.batch_size,\n",
    "                pin_memory=True,\n",
    "                persistent_workers=True,\n",
    "                shuffle=True,\n",
    "                num_workers=self.num_workers,\n",
    "            )\n",
    "\n",
    "# Lightning Model\n",
    "from pytorch_lightning import LightningDataModule, LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "class HCEikonalPINNsModel(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "                                                               \n",
    "        self.optim_type = kwargs['optimizer']\n",
    "        \n",
    "        self.register_buffer(\"seed\", torch.tensor(kwargs['seed']))\n",
    "        self.register_buffer(\"learning_rate\", torch.tensor(kwargs['learning_rate']))    \n",
    "\n",
    "        # Computational model parameters    \n",
    "        deltar = kwargs['rec_spacing']\n",
    "        deltas = kwargs['sou_spacing']\n",
    "        zmin = 0.; zmax = kwargs['max_depth']\n",
    "        ymin = 0.; ymax = kwargs['max_offset']\n",
    "        xmin = 0.; xmax = kwargs['max_offset']\n",
    "        if kwargs['model_type']!='arid':\n",
    "            deltax, deltay, deltaz = kwargs['lateral_spacing'], kwargs['lateral_spacing'], kwargs['vertical_spacing']\n",
    "        else:\n",
    "            deltax, deltay, deltaz = kwargs['sampling_rate']*0.00625/2*4, kwargs['sampling_rate']*0.00625/2*4, kwargs['sampling_rate']*0.00625/2\n",
    "        \n",
    "        z = torch.arange(zmin,zmax,deltaz)\n",
    "        y = torch.arange(ymin,ymax,deltay)\n",
    "        x = torch.arange(xmin,xmax,deltax)\n",
    "        \n",
    "        Z,Y,X = np.meshgrid(z,y,x,indexing='ij')\n",
    "        \n",
    "        idx_all = np.arange(X.size).reshape(X.shape)\n",
    "        id_sou = idx_all[kwargs['zid_source'], ::deltas, ::deltas].reshape(-1)\n",
    "        id_rec = idx_all[kwargs['zid_receiver'], ::deltar, ::deltar].reshape(-1)\n",
    "        \n",
    "        self.register_buffer(\"x\", x)\n",
    "        self.register_buffer(\"y\", y)\n",
    "        self.register_buffer(\"z\", z)\n",
    "\n",
    "        self.register_buffer(\"sx\", torch.tensor(X.reshape(-1)[id_sou]))\n",
    "        self.register_buffer(\"sy\", torch.tensor(Y.reshape(-1)[id_sou]))\n",
    "        self.register_buffer(\"sz\", torch.tensor(Z.reshape(-1)[id_sou]))\n",
    "        \n",
    "        self.register_buffer(\"rx\", torch.tensor(X.reshape(-1)[id_rec]))\n",
    "        self.register_buffer(\"ry\", torch.tensor(Y.reshape(-1)[id_rec]))\n",
    "        self.register_buffer(\"rz\", torch.tensor(Z.reshape(-1)[id_rec]))\n",
    "        self.register_buffer(\"sid\", torch.arange(torch.tensor(self.sx.shape[0])))\n",
    "        \n",
    "        self.register_buffer(\"bias\", torch.tensor(0.2))\n",
    "        self.register_buffer(\"mean\", torch.tensor(0.01))\n",
    "        self.register_buffer(\"std\", torch.tensor(0.05))\n",
    "        self.register_buffer(\"v_scaler\", torch.tensor(1.))\n",
    "        self.register_buffer(\"num_epochs\", torch.tensor(kwargs['num_epochs']))\n",
    "        self.register_buffer(\"reduce_after\", torch.tensor(kwargs['reduce_after']))\n",
    "        \n",
    "        # network\n",
    "        if kwargs['residual_network']=='n':\n",
    "            self.tau_model = FullyConnectedNetwork(4, 1, [kwargs['num_neurons']]*kwargs['num_layers'], last_act=kwargs['tau_act'], act=kwargs['activation'], lay='linear', last_multiplier=kwargs['tau_multiplier'])\n",
    "            \n",
    "            self.v_model = FullyConnectedNetwork(3, 1, [kwargs['num_neurons']//2]*kwargs['num_layers'], act='relu', lay='linear', last_act='relu', last_multiplier=kwargs['v_multiplier'])\n",
    "        else:\n",
    "            self.tau_model = ResidualNetwork(4, 1, num_neurons=kwargs['num_neurons'], num_layers=kwargs['num_layers'], act=kwargs['activation'], lay='linear', last_multiplier=kwargs['tau_multiplier'])\n",
    "            \n",
    "            self.v_model = ResidualNetwork(3, 1, num_neurons=kwargs['num_neurons']//2, act='relu', last_act='relu', num_layers=kwargs['num_layers'], lay='linear', last_multiplier=kwargs['v_multiplier'])\n",
    "        \n",
    "        self.v_model = self.v_model.apply(lambda m: init_weights(m, init_type=kwargs['initialization'], bias=self.bias, mean=self.mean, std=self.std))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if self.optim_type == 'adam':\n",
    "            self.optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate, betas=(0.9, 0.999), eps=1e-5)\n",
    "        elif self.optim_type == 'lbfgs':\n",
    "            self.optimizer = torch.optim.LBFGS(list(self.tau_model.parameters()) + list(self.v_model.parameters()), line_search_fn=\"strong_wolfe\")\n",
    "        scheduler = {'scheduler': ReduceLROnPlateau(self.optimizer, mode='min', factor=0.5, patience=self.num_epochs//self.reduce_after, verbose=True), \n",
    "                     'interval': 'epoch', \n",
    "                     'monitor':'train_pde_loss'}\n",
    "        return {\"optimizer\": self.optimizer, \"lr_scheduler\": scheduler}\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self.forward(batch[0],batch[1],batch[2],batch[3],batch[4],batch[5],batch[6],batch[7],batch[8],batch[9],batch[10],batch[11])\n",
    "        self.log(\"train_pde_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def evaluate_velocity(self, data_loader, batch_size, num_pts):\n",
    "        self.v_model.eval()\n",
    "\n",
    "        # Prepare input\n",
    "        with torch.no_grad():\n",
    "            V = torch.empty(num_pts)\n",
    "            for i, X in enumerate(data_loader):\n",
    "\n",
    "                # Compute v\n",
    "                batch_end = (i+1)*batch_size if (i+1)*batch_size<num_pts else i*batch_size + X[0].shape[0]\n",
    "                V[i*batch_size:batch_end] = self.v_model(X[0]).view(-1)\n",
    "\n",
    "        return V\n",
    "    \n",
    "    def evaluate_tau(self, data_loader, batch_size, num_pts):\n",
    "        self.tau_model.eval()\n",
    "\n",
    "        # Prepare input\n",
    "        with torch.no_grad():\n",
    "            T = torch.empty(num_pts)\n",
    "            for i, X in enumerate(data_loader, 0):\n",
    "\n",
    "                xyzs = torch.hstack((X[0], X[-1].view(-1,1)))\n",
    "                batch_end = (i+1)*batch_size if (i+1)*batch_size<num_pts else i*batch_size + X[0].shape[0]\n",
    "                T[i*batch_size:batch_end] = self.tau_model(xyzs).view(-1)\n",
    "\n",
    "        return T\n",
    "    \n",
    "    def forward(self, xyz, sx, sy, sz, taud, taud_dx, taud_dy, t0, t0_dx, t0_dy, t0_dz, idx):\n",
    "        \n",
    "        ic = torch.hstack((self.sx.view(-1,1), self.sy.view(-1,1), self.sz.view(-1,1))).float()\n",
    "        \n",
    "        # Number of source\n",
    "        num_sou = len(ic[:,0])\n",
    "\n",
    "        xyz.requires_grad = True\n",
    "\n",
    "        # Input for the velocity network\n",
    "        xyzic = torch.cat([xyz, ic])\n",
    "\n",
    "        # Source location\n",
    "        sxic = torch.cat([sx, ic[:,0]])\n",
    "        syic = torch.cat([sy, ic[:,1]])\n",
    "        szic = torch.cat([sz, ic[:,2]])\n",
    "        sidx = torch.cat([idx, self.sid])\n",
    "\n",
    "        # Input for the data network\n",
    "        # xyzsic = torch.hstack((xyzic, sxic.view(-1,1), syic.view(-1,1), szic.view(-1,1)))\n",
    "        xyzsic = torch.hstack((xyzic, sidx.view(-1,1)))\n",
    "\n",
    "        # Compute T\n",
    "        tau = self.tau_model(xyzsic).view(-1)\n",
    "\n",
    "        # Compute v\n",
    "        v = self.v_model(xyzsic[:, :3]).view(-1)\n",
    "\n",
    "        # Gradients\n",
    "        gradient = torch.autograd.grad(tau, xyzsic, torch.ones_like(tau), create_graph=True)[0]\n",
    "        \n",
    "        tau_dx = gradient[:, 0]\n",
    "        tau_dy = gradient[:, 1]\n",
    "        tau_dz = gradient[:, 2]\n",
    "        \n",
    "        rec_op = xyz[:,2]\n",
    "        rec_op_dz = 1\n",
    "                \n",
    "        T_dx = rec_op*tau_dx[:-num_sou] + taud_dx + t0_dx\n",
    "        T_dy = rec_op*tau_dy[:-num_sou] + taud_dy + t0_dy\n",
    "        T_dz = rec_op*tau_dz[:-num_sou] + rec_op_dz*tau[:-num_sou] + t0_dz\n",
    "        \n",
    "        vscaler = 1\n",
    "        \n",
    "        pde_lhs = (T_dx**2 + T_dy**2 + T_dz**2) * vscaler\n",
    "\n",
    "        pde = pde_lhs - vscaler / (v[:-num_sou] ** 2)\n",
    "        \n",
    "        wl2=1        \n",
    "        \n",
    "        return torch.mean(wl2*pde**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e5304d-f97a-4688-88a1-011a946aca5d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import *hcpinnseikonal* package functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae510489-e60e-494e-ade6-470fc69977d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lateral_spacing': 0.01, 'vertical_spacing': 0.01, 'max_offset': 5.0, 'max_depth': 1.0, 'rec_spacing': 10, 'sou_spacing': 10, 'num_epochs': 250, 'num_neurons': 20, 'num_layers': 10, 'learning_rate': 0.001, 'model_type': 'seam', 'data_type': 'full', 'middle_shot': 'n', 'until_cmb': 'n', 'earth_scale': 'n', 'scale_factor': 10, 'reduce_after': 15, 'seed': 123, 'initialization': 'varianceScaling', 'plotting_factor': 1, 'rescale_plot': 'n', 'depth_shift': 'n', 'tau_multiplier': 3.0, 'initial_velocity': 4, 'zid_source': 5, 'zid_receiver': 0, 'explode_reflector': 'n', 'field_synthetic': 'n', 'v_multiplier': 3, 'activation': 'elu', 'num_points': 1.0, 'irregular_grid': 'n', 'xid_well': 5, 'last_vmultiplier': 5, 'nu_units': 'unitless', 'well_depth': None, 'exp_function': 'n', 'exp_factor': 1.0, 'exclude_topo': 'n', 'exclude_well': 'n', 'exclude_source': 'n', 'loss_function': 'mse', 'station_factor': 1.0, 'event_factor': 1.0, 'checker_size': 5.0, 'tau_act': 'None', 'empty_middle': 'n', 'factorization_type': 'multiplicative', 'causality_factor': 1.0, 'causality_weight': 'type_0', 'residual_network': 'n', 'velocity_loss': 'n', 'regular_station': 'n', 'data_neurons': 16, 'data_layers': 8, 'append_shot': 'n', 'use_wandb': 'n', 'save_folder': './', 'project_name': 'GFATT_PINNs-20-3d-lightning-inversion', 'regularization_type': 'None', 'regularization_weight': 0.0, 'optimizer': 'adam', 'mixed_precision': 'n', 'fast_loader': 'n', 'sampling_rate': 4}\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from argparse import ArgumentParser   \n",
    "from scipy import interpolate\n",
    "\n",
    "from hcpinnseikonal.utils import *\n",
    "from hcpinnseikonal.model import *\n",
    "from hcpinnseikonal.plot import *\n",
    "from hcpinnseikonal.arguments import *\n",
    "# from hcpinnseikonal.distributed import *\n",
    "\n",
    "args = parser.parse_args([])\n",
    "\n",
    "args.use_wandb='n'\n",
    "\n",
    "dict_args = vars(args)\n",
    "print(dict_args)\n",
    "\n",
    "# Change these lines for the wandb setup\n",
    "if args.use_wandb=='y':\n",
    "    wandb.init(project=args.project_name)\n",
    "    wandb.run.log_code(\".\")\n",
    "    wandb_dir = wandb.run.dir\n",
    "else:\n",
    "    args.save_folder='../saves/saves_lightningTry3d'\n",
    "    from pathlib import Path\n",
    "    Path(args.save_folder).mkdir(parents=True, exist_ok=True)\n",
    "    wandb_dir = args.save_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36894ab-ca26-436c-9e0f-3e12b9a34d33",
   "metadata": {},
   "source": [
    "## Define arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2c6fc60-cac1-4243-a81e-da4d4d5f23d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lateral_spacing': 0.375, 'vertical_spacing': 0.09375, 'max_offset': 4.9875, 'max_depth': 1.865625, 'rec_spacing': 20, 'sou_spacing': 20, 'num_epochs': 1000, 'num_neurons': 24, 'num_layers': 12, 'learning_rate': 0.0005, 'model_type': 'arid', 'data_type': 'full', 'middle_shot': 'n', 'until_cmb': 'y', 'earth_scale': 'n', 'scale_factor': 2, 'reduce_after': 50, 'seed': 1234, 'initialization': 'varianceScaling', 'plotting_factor': 1, 'rescale_plot': 'n', 'depth_shift': 'n', 'tau_multiplier': 1, 'initial_velocity': 3, 'zid_source': 1, 'zid_receiver': 0, 'explode_reflector': 'n', 'field_synthetic': 'n', 'v_multiplier': 3, 'activation': 'elu', 'num_points': 1.0, 'irregular_grid': 'y', 'xid_well': 5, 'last_vmultiplier': 5, 'nu_units': 'unitless', 'well_depth': None, 'exp_function': 'n', 'exp_factor': 1.0, 'exclude_topo': 'n', 'exclude_well': 'n', 'exclude_source': 'n', 'loss_function': 'mse', 'station_factor': 0.2, 'event_factor': 0.9, 'checker_size': 5.0, 'tau_act': 'tanh', 'empty_middle': 'n', 'factorization_type': 'additive', 'causality_factor': 0.5, 'causality_weight': 'type_0', 'residual_network': 'y', 'velocity_loss': 'n', 'regular_station': 'y', 'data_neurons': 16, 'data_layers': 8, 'append_shot': 'n', 'use_wandb': 'n', 'save_folder': '../saves/saves_lightningTry3d', 'project_name': 'GFATT_PINNs-20-3d-lightning-inversion', 'regularization_type': 'None', 'regularization_weight': 0.0, 'optimizer': 'adam', 'mixed_precision': 'y', 'fast_loader': 'n', 'sampling_rate': 4}\n"
     ]
    }
   ],
   "source": [
    "args.scale_factor=2 \n",
    "args.until_cmb='y' \n",
    "args.num_epochs=1000\n",
    "args.seed=1234 \n",
    "args.learning_rate=5e-4\n",
    "args.rescale_plot='n' \n",
    "args.initial_velocity=3 \n",
    "args.zid_source=1 \n",
    "args.zid_receiver=0 \n",
    "args.data_type='full' \n",
    "args.irregular_grid='y' \n",
    "args.num_layers=12 \n",
    "args.model_type='arid' \n",
    "args.v_multiplier=3 \n",
    "args.factorization_type='additive' \n",
    "args.tau_act='tanh' \n",
    "args.tau_multiplier=1 \n",
    "args.max_offset=4.9875\n",
    "args.max_depth=1.865625 \n",
    "args.vertical_spacing=0.09375 \n",
    "args.lateral_spacing=0.375\n",
    "args.num_neurons=24 \n",
    "args.causality_factor=.5 \n",
    "args.causality_weight='type_0' \n",
    "args.reduce_after=50 \n",
    "args.field_synthetic='n' \n",
    "args.event_factor=0.9 \n",
    "args.station_factor=0.2 \n",
    "args.residual_network='y' \n",
    "args.empty_middle='n' \n",
    "args.regular_station='y' \n",
    "args.rec_spacing=20\n",
    "args.sou_spacing=20\n",
    "args.mixed_precision='y'\n",
    "args.fast_loader='n'\n",
    "args.sampling_rate=4\n",
    "\n",
    "dict_args=vars(args)\n",
    "print(dict_args)\n",
    "\n",
    "def total_fruits(**kwargs):\n",
    "    print(kwargs, type(kwargs))\n",
    "    print(kwargs['banana'])\n",
    "\n",
    "input_dict = {'banana': 5, 'mango': 7, 'apple': 8}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b18fc7-3f7d-401c-8693-62b09a3e0480",
   "metadata": {},
   "source": [
    "## Training anddict_argse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2c9f90-0fae-478b-bfde-947299ad8e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1234\n",
      "/home/taufikmh/miniconda3/envs/my_env_plot/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py:352: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/taufikmh/miniconda3/envs/my_env_plot/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:616: UserWarning: Checkpoint directory /home/taufikmh/KAUST/fall_2022/external_repos/copy-HCPINNsEikonal-dev/saves/saves_lightningTry3d exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/tmp/ipykernel_33750/481518410.py:704: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  scheduler = {'scheduler': ReduceLROnPlateau(self.optimizer, mode='min', factor=0.5, patience=self.num_epochs//self.reduce_after, verbose=True),\n",
      "\n",
      "  | Name      | Type            | Params\n",
      "----------------------------------------------\n",
      "0 | tau_model | ResidualNetwork | 22.9 K\n",
      "1 | v_model   | ResidualNetwork | 6.0 K \n",
      "----------------------------------------------\n",
      "28.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "28.9 K    Total params\n",
      "0.058     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "386519b1bceb4db5ab56bee82aff0230",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setup\n",
    "pl.seed_everything(dict_args['seed'])\n",
    "\n",
    "model = HCEikonalPINNsModel(**dict_args)\n",
    "nx, nz, ns = model.x.shape[0], model.z.shape[0], model.sx.shape[0]    \n",
    "data = HCEikonalPINNsData(dict_args, batch_size=int(nx*nz*ns)//200)\n",
    "X, Y, Z, SX, SY, SZ, taud, taudx, taudy, T0, px0, py0, pz0, index = data.input_list\n",
    "xmin, ymin, zmin = 0, 0, 0\n",
    "xmax, ymax, zmax = dict_args['max_offset'], dict_args['max_offset'], dict_args['max_depth']\n",
    "\n",
    "data.id_sou_z = np.array(dict_args['zid_source'])\n",
    "data.id_rec_z = np.array(dict_args['zid_receiver'])\n",
    "data.id_sou_x = np.arange(0,len(X[0,:,0]),dict_args['sou_spacing'])\n",
    "data.id_rec_x = np.arange(0,len(X[0,:,0]),dict_args['rec_spacing'])\n",
    "\n",
    "BATCH_SIZE = Z.size//200 if torch.cuda.is_available() else 64\n",
    "NUM_WORKERS = int(os.cpu_count() / 2)\n",
    "\n",
    "if dict_args['fast_loader']=='y':\n",
    "    data_loader = FastTensorDataLoader(\n",
    "        torch.from_numpy(np.vstack((data.input_list[0], data.input_list[1], data.input_list[2])).T).ravel().float(),\n",
    "        torch.from_numpy(data.input_list[3]).ravel().float(),\n",
    "        torch.from_numpy(data.input_list[4]).ravel().float(),\n",
    "        torch.from_numpy(data.input_list[5]).ravel().float(),\n",
    "        torch.from_numpy(data.input_list[6]).ravel().float(),\n",
    "        torch.from_numpy(data.input_list[7]).ravel().float(),\n",
    "        torch.from_numpy(data.input_list[8]).ravel().float(),\n",
    "        torch.from_numpy(data.input_list[9]).ravel().float(),\n",
    "        torch.from_numpy(data.input_list[10]).ravel().float(),\n",
    "        torch.from_numpy(data.input_list[11]).ravel().float(),\n",
    "        torch.from_numpy(data.input_list[12]).ravel().float(),\n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True\n",
    ")\n",
    "else:\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        data.input_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "init_loader = FastTensorDataLoader(\n",
    "    torch.from_numpy(np.vstack((data.input_list[0].ravel(), data.input_list[1].ravel(), data.input_list[2].ravel())).T).float(),\n",
    "    torch.from_numpy(data.input_list[3].ravel()).float(),\n",
    "    torch.from_numpy(data.input_list[4].ravel()).float(),\n",
    "    torch.from_numpy(data.input_list[5].ravel()).float(),\n",
    "    torch.from_numpy(data.input_list[6].ravel()).float(),\n",
    "    torch.from_numpy(data.input_list[7].ravel()).float(),\n",
    "    torch.from_numpy(data.input_list[8].ravel()).float(),\n",
    "    torch.from_numpy(data.input_list[9].ravel()).float(),\n",
    "    torch.from_numpy(data.input_list[10].ravel()).float(),\n",
    "    torch.from_numpy(data.input_list[11].ravel()).float(),\n",
    "    torch.from_numpy(data.input_list[12].ravel()).float(),\n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# ZX plane after\n",
    "plot_section(data.vel3d[:,10,:], 'v_true_zx.png', vmin=np.nanmin(data.vel3d)+0.1, \n",
    "             vmax=np.nanmax(data.vel3d)-0.5, save_dir=wandb_dir, aspect='equal',\n",
    "             xmin=xmin, xmax=xmax, zmin=zmin, zmax=zmax, \n",
    "             sx=X[:,:,:,0].reshape(-1)[data.id_sou],sz=Z[:,:,:,0].reshape(-1)[data.id_sou],rx=X[:,:,:,0].reshape(-1)[data.id_rec],rz=Z[:,:,:,0].reshape(-1)[data.id_rec])\n",
    "\n",
    "# XY plane\n",
    "plot_section(data.vel3d[5,:,:], 'v_true_xy.png', vmin=np.nanmin(data.vel3d)+0.1, \n",
    "             vmax=np.nanmax(data.vel3d)-0.5, save_dir=wandb_dir, aspect='equal',\n",
    "             xmin=xmin, xmax=xmax, zmin=xmin, zmax=xmax, \n",
    "             sx=X[:,:,:,0].reshape(-1)[data.id_sou],sz=Y[:,:,:,0].reshape(-1)[data.id_sou],rx=X[:,:,:,0].reshape(-1)[data.id_rec],rz=Y[:,:,:,0].reshape(-1)[data.id_rec])\n",
    "\n",
    "# ZY plane\n",
    "plot_section(data.vel3d[:,:,10], 'v_true_zy.png', vmin=np.nanmin(data.vel3d)+0.1, \n",
    "             vmax=np.nanmax(data.vel3d)-0.5, save_dir=wandb_dir, aspect='equal',\n",
    "             xmin=xmin, xmax=xmax, zmin=zmin, zmax=zmax, \n",
    "             sx=Y[:,:,:,0].reshape(-1)[data.id_sou],sz=Z[:,:,:,0].reshape(-1)[data.id_sou],rx=Y[:,:,:,0].reshape(-1)[data.id_rec],rz=Z[:,:,:,0].reshape(-1)[data.id_rec])\n",
    "\n",
    "v_init = model.evaluate_velocity(init_loader,batch_size=BATCH_SIZE,num_pts=X.size)\n",
    "\n",
    "# Training\n",
    "wandb_logger = WandbLogger(log_model=\"all\")\n",
    "\n",
    "if dict_args['mixed_precision']=='y':\n",
    "    trainer = Trainer(\n",
    "        accelerator=\"gpu\",\n",
    "        strategy=\"dp\",#pl.strategies.DDPStrategy(find_unused_parameters=False), #\"ddp\", #\"ddp_find_unused_parameters_false\",\n",
    "        devices=\"auto\",  # limiting got iPython runs\n",
    "        max_epochs=dict_args['num_epochs'],\n",
    "        precision=16,\n",
    "        callbacks=[\n",
    "            TQDMProgressBar(refresh_rate=20), \n",
    "            ModelCheckpoint(dirpath=wandb_dir,filename='{epoch}-{train_pde_loss:.2f}',save_last=True,monitor=\"train_pde_loss\", mode=\"min\")],\n",
    "        logger=wandb_logger,\n",
    "        default_root_dir=wandb_dir+'checkpoint.ckpt'\n",
    "    )\n",
    "\n",
    "else:\n",
    "    trainer = Trainer(\n",
    "        accelerator=\"gpu\",\n",
    "        strategy=\"dp\", #pl.strategies.DDPStrategy(find_unused_parameters=False),\n",
    "        devices=\"auto\",  # limiting got iPython runs\n",
    "        max_epochs=dict_args['num_epochs'],\n",
    "        callbacks=[\n",
    "            TQDMProgressBar(refresh_rate=20), \n",
    "            ModelCheckpoint(dirpath=wandb_dir,filename='{epoch}-{train_pde_loss:.2f}',save_last=True,monitor=\"train_pde_loss\", mode=\"min\")],\n",
    "        logger=wandb_logger,\n",
    "        default_root_dir=wandb_dir+'checkpoint.ckpt'\n",
    "    )\n",
    "trainer.fit(model, datamodule=data)\n",
    "\n",
    "# Inference\n",
    "v_pred = model.evaluate_velocity(init_loader,batch_size=BATCH_SIZE,num_pts=X.size)\n",
    "tau_pred = model.evaluate_tau(init_loader,batch_size=BATCH_SIZE,num_pts=X.size)\n",
    "\n",
    "# ZX plane after\n",
    "plot_section(v_pred.reshape(X.shape)[:,0,:,i], 'v_pred_zx.png', vmin=np.nanmin(data.vel3d)+0.1, \n",
    "             vmax=np.nanmax(data.vel3d)-0.5, save_dir=wandb_dir, aspect='equal',\n",
    "             xmin=xmin, xmax=xmax, zmin=zmin, zmax=zmax, \n",
    "             sx=X[:,:,:,i].reshape(-1)[data.id_sou],sz=Z[:,:,:,i].reshape(-1)[data.id_sou],rx=X[:,:,:,i].reshape(-1)[data.id_rec],rz=Z[:,:,:,i].reshape(-1)[data.id_rec])\n",
    "\n",
    "# XY plane\n",
    "plot_section(v_pred.reshape(X.shape)[args.zid_source,:,:,i], 'v_pred_xy.png', vmin=np.nanmin(data.vel3d)+0.1, \n",
    "             vmax=np.nanmax(data.vel3d)-0.5, save_dir=wandb_dir, aspect='equal',\n",
    "             xmin=xmin, xmax=xmax, zmin=xmin, zmax=xmax, \n",
    "             sx=X[:,:,:,i].reshape(-1)[data.id_sou],sz=Y[:,:,:,i].reshape(-1)[data.id_sou],rx=X[:,:,:,i].reshape(-1)[data.id_rec],rz=Y[:,:,:,i].reshape(-1)[data.id_rec])\n",
    "\n",
    "# ZY plane\n",
    "plot_section(v_pred.reshape(X.shape)[:,:,0,i], 'v_pred_zy.png', vmin=np.nanmin(data.vel3d)+0.1, \n",
    "             vmax=np.nanmax(data.vel3d)-0.5, save_dir=wandb_dir, aspect='equal',\n",
    "             xmin=xmin, xmax=xmax, zmin=zmin, zmax=zmax, \n",
    "             sx=Y[:,:,:,i].reshape(-1)[data.id_sou],sz=Z[:,:,:,i].reshape(-1)[data.id_sou],rx=Y[:,:,:,i].reshape(-1)[data.id_rec],rz=Z[:,:,:,i].reshape(-1)[data.id_rec])\n",
    "\n",
    "# Save model\n",
    "torch.save({\n",
    "        'tau_model_state_dict': tau_model.state_dict(),\n",
    "        'v_model_state_dict': v_model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss_history\n",
    "}, wandb_dir+'/saved_model')\n",
    "\n",
    "# Save model\n",
    "torch.save({\n",
    "        'tau_model_state_dict': model.tau_model.state_dict(),\n",
    "        'v_model_state_dict': model.v_model.state_dict(),\n",
    "        'optimizer_state_dict': model.optimizer.state_dict()\n",
    "}, wandb.run.dir + '/checkpoint')\n",
    "\n",
    "# # To load\n",
    "# checkpoint = torch.load( wandb.run.dir + '/checkpoint')\n",
    "# tau_model.load_state_dict(checkpoint['tau_model_state_dict'])\n",
    "# v_model.load_state_dict(checkpoint['v_model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd5b2b9-7c51-4e04-b797-24596d4a0d78",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Visualize 3D model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f5b14d-6a78-4542-acba-a4fa87936d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_idsx = [np.where(x==X[:,:,:,0].reshape(-1)[id_sou[i]])[0][0] for i in range(len(id_sou))]\n",
    "tmp_idsy = [np.where(y==Y[:,:,:,0].reshape(-1)[id_sou[i]])[0][0] for i in range(len(id_sou))]\n",
    "tmp_idsz = [np.where(z==Z[:,:,:,0].reshape(-1)[id_sou[i]])[0][0] for i in range(len(id_sou))]\n",
    "\n",
    "print([np.unique(np.isnan(i)) for i in input_wosrc])\n",
    "\n",
    "for i in range(len(id_sou)):\n",
    "    input_item = taudy.reshape(X.shape)\n",
    "    print(\"Shot number #\"+str(i+1)+\" \"+str(np.unique(input_item[0,:,:,i]==input_item[-1,:,:,i])))\n",
    " \n",
    "for i in range(len(id_sou)):\n",
    "    temp = np.copy(T_data3d.reshape(X.shape))\n",
    "    print(temp[tmp_idsz[i], tmp_idsy[i], tmp_idsx[i], i])\n",
    "\n",
    "for i in range(0,len(id_sou),len(id_sou)//3):\n",
    "    # ZX plane after\n",
    "    print(i,x[np.where(x==X[:,:,:,0].reshape(-1)[id_sou[i]])[0][0]])\n",
    "    plot_section(Td_hc.reshape(X.shape)[:,np.where(y==Y[:,:,:,i].reshape(-1)[id_sou[i]])[0][0],:,i], 'T_data3d_zx.png', save_dir=wandb_dir, aspect='equal',\n",
    "                 xmin=xmin, xmax=xmax, zmin=zmin, zmax=zmax, \n",
    "                 sx=X[:,:,:,i].reshape(-1)[id_sou],sz=Z[:,:,:,i].reshape(-1)[id_sou],rx=X[:,:,:,i].reshape(-1)[id_rec],rz=Z[:,:,:,i].reshape(-1)[id_rec])\n",
    "\n",
    "    # XY plane\n",
    "    plot_section(Td_hc.reshape(X.shape)[args.zid_source,:,:,i], 'T_data3d_xy.png', save_dir=wandb_dir, aspect='equal',\n",
    "                 xmin=xmin, xmax=xmax, zmin=xmin, zmax=xmax, \n",
    "                 sx=X[:,:,:,i].reshape(-1)[id_sou],sz=Y[:,:,:,i].reshape(-1)[id_sou],rx=X[:,:,:,i].reshape(-1)[id_rec],rz=Y[:,:,:,i].reshape(-1)[id_rec])\n",
    "\n",
    "    # ZY plane\n",
    "    plot_section(Td_hc.reshape(X.shape)[:,:,np.where(x==X[:,:,:,i].reshape(-1)[id_sou[i]])[0][0],i], 'T_data3d_zy.png', save_dir=wandb_dir, aspect='equal',\n",
    "                 xmin=xmin, xmax=xmax, zmin=zmin, zmax=zmax, \n",
    "                 sx=Y[:,:,:,i].reshape(-1)[id_sou],sz=Z[:,:,:,i].reshape(-1)[id_sou],rx=Y[:,:,:,i].reshape(-1)[id_rec],rz=Z[:,:,:,i].reshape(-1)[id_rec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3256f565-d110-4bd6-bb5c-a379ad413b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save model\n",
    "# torch.save({\n",
    "#         'tau_model_state_dict': tau_model.state_dict(),\n",
    "#         'v_model_state_dict': v_model.state_dict(),\n",
    "#         'optimizer_state_dict': optimizer.state_dict()\n",
    "# }, wandb_dir+'/saved_model')\n",
    "\n",
    "# To load\n",
    "checkpoint = torch.load(wandb_dir+'/saved_model')\n",
    "tau_model.load_state_dict(checkpoint['tau_model_state_dict'])\n",
    "v_model.load_state_dict(checkpoint['v_model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664f9a4d-dcad-4a0e-b19e-34b883768f0d",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0627b22-b7f4-47ea-b575-fa0512af5ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load\n",
    "checkpoint = torch.load(wandb_dir+'/saved_model')\n",
    "tau_model.load_state_dict(checkpoint['tau_model_state_dict'])\n",
    "v_model.load_state_dict(checkpoint['v_model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# Prediction\n",
    "pde_loader, ic = create_dataloader3d([i.ravel() for i in input_wsrc], sx, sy, sz,\n",
    "                                   shuffle=False, batch_size=512, fast_loader=True, perm_id=ipermute)\n",
    "v_pred = evaluate_velocity3d(v_model, pde_loader, X.size, batch_size=512, device=device)\n",
    "\n",
    "tau_pred = evaluate_tau3d(tau_model, pde_loader, X.size, batch_size=512, device=device)\n",
    "\n",
    "v_pred = v_pred.detach().cpu().numpy()\n",
    "tau_pred = tau_pred.detach().cpu().numpy()\n",
    "\n",
    "# ZX plane after\n",
    "plot_section(v_pred.reshape(X.shape)[:,10,:,i], 'v_pred_zx.png', vmin=np.nanmin(velmodel)+0.1, \n",
    "             vmax=np.nanmax(velmodel)-0.5, save_dir=wandb_dir, aspect='equal',\n",
    "             xmin=xmin, xmax=xmax, zmin=zmin, zmax=zmax, \n",
    "             sx=X[:,:,:,i].reshape(-1)[id_sou],sz=Z[:,:,:,i].reshape(-1)[id_sou],rx=X[:,:,:,i].reshape(-1)[id_rec],rz=Z[:,:,:,i].reshape(-1)[id_rec])\n",
    "\n",
    "# XY plane\n",
    "plot_section(v_pred.reshape(X.shape)[5,:,:,i], 'v_pred_xy.png', vmin=np.nanmin(velmodel)+0.1, \n",
    "             vmax=np.nanmax(velmodel)-0.5, save_dir=wandb_dir, aspect='equal',\n",
    "             xmin=xmin, xmax=xmax, zmin=xmin, zmax=xmax, \n",
    "             sx=X[:,:,:,i].reshape(-1)[id_sou],sz=Y[:,:,:,i].reshape(-1)[id_sou],rx=X[:,:,:,i].reshape(-1)[id_rec],rz=Y[:,:,:,i].reshape(-1)[id_rec])\n",
    "\n",
    "# ZY plane\n",
    "plot_section(v_pred.reshape(X.shape)[:,:,10,i], 'v_pred_zy.png', vmin=np.nanmin(velmodel)+0.1, \n",
    "             vmax=np.nanmax(velmodel)-0.5, save_dir=wandb_dir, aspect='equal',\n",
    "             xmin=xmin, xmax=xmax, zmin=zmin, zmax=zmax, \n",
    "             sx=Y[:,:,:,i].reshape(-1)[id_sou],sz=Z[:,:,:,i].reshape(-1)[id_sou],rx=Y[:,:,:,i].reshape(-1)[id_rec],rz=Z[:,:,:,i].reshape(-1)[id_rec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e231c72f-347e-40d6-9a9e-6cc70ec6c523",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "def plot_slice(x, y, z, data, xslice, yslice, zslice, ax=None, vmin=None, vmax=None, fig_name=None, save_dir='./'):\n",
    "    \n",
    "    if ax is None:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "    else:\n",
    "        fig = plt.figure()\n",
    "\n",
    "    data_z = data[zslice,:,:]\n",
    "    data_x = data[:,:,xslice]\n",
    "    data_y = data[:,yslice,:]\n",
    "    \n",
    "    norm = matplotlib.colors.Normalize(vmin=data.min(), vmax=data.max())\n",
    "    cmap = plt.cm.get_cmap('terrain')#plt.cm.\n",
    "    m = plt.cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "    m.set_array([])\n",
    "    # fcolors = m.to_rgba(data.reshape(-1,1))\n",
    "    \n",
    "    # Plot X slice\n",
    "    xs, ys, zs = data.shape\n",
    "    \n",
    "    xplot = ax.plot_surface(np.atleast_2d(x[xslice]), y[:, np.newaxis], z[np.newaxis, :],\n",
    "                            facecolors=m.to_rgba(data_x.T), cmap=cmap) #, vmin=1.5, vmax=8.85)\n",
    "    # Plot Y slice\n",
    "    yplot = ax.plot_surface(x[:, np.newaxis], np.atleast_2d(y[yslice]), z[np.newaxis, :],\n",
    "                            facecolors=m.to_rgba(data_y.T), cmap=cmap) #, vmin=1.5, vmax=8.85)\n",
    "    # Plot Z slice\n",
    "    zplot = ax.plot_surface(x[:, np.newaxis], y[np.newaxis, :], np.atleast_2d(z[zslice]),\n",
    "                            facecolors=m.to_rgba(data_z.T), cmap=cmap) #, vmin=1.5, vmax=8.85)\n",
    "    # zplot.\n",
    "    cbar = plt.colorbar(m, shrink=0.15, aspect=5, location='bottom')\n",
    "    cbar.set_label('km/s')\n",
    "    \n",
    "    ax.invert_zaxis()\n",
    "    ax.set_xlabel('X (km)')\n",
    "    ax.set_ylabel('Y (km)')\n",
    "    ax.set_zlabel('Z (km)')\n",
    "    \n",
    "    if fig_name is not None:\n",
    "        plt.savefig(os.path.join(save_dir, fig_name), \n",
    "                    format='png', bbox_inches=\"tight\")\n",
    "\n",
    "# Computational model parameters\n",
    "zmin = -0.1 if args.field_synthetic=='y' else 0; zmax = args.max_depth; deltaz = args.vertical_spacing;\n",
    "ymin = 0.; ymax = args.max_offset; deltay = args.lateral_spacing;\n",
    "xmin = 0.; xmax = args.max_offset; deltax = args.lateral_spacing;\n",
    "\n",
    "if args.earth_scale=='y':\n",
    "    earth_radi = 6371/args.scale_factor # Average in km\n",
    "    xmin, xmax, deltax = earth_radi*xmin, earth_radi*xmax, earth_radi*deltax\n",
    "    ymin, ymax, deltay = earth_radi*ymin, earth_radi*ymax, earth_radi*deltay\n",
    "    zmin, zmax, deltaz = earth_radi*zmin, earth_radi*zmax, earth_radi*deltaz\n",
    "\n",
    "# Creating grid, extending the velocity model, and prepare list of grid points for training (X_star)\n",
    "z = np.arange(zmin,zmax+deltaz,deltaz)\n",
    "nz = z.size\n",
    "\n",
    "y = np.arange(ymin,ymax+deltay,deltay)\n",
    "ny = y.size\n",
    "\n",
    "x = np.arange(xmin,xmax+deltax,deltax)\n",
    "nx = x.size\n",
    "\n",
    "plot_slice(x, y, z, vel3d.reshape(X[:,:,:,0].shape), 0, y.size-1, z.size-1, fig_name='v_trueCube.png', save_dir=wandb_dir)\n",
    "plot_slice(x, y, z, v_pred.reshape(X.shape)[:,:,:,0], 0, y.size-1, z.size-1, fig_name='v_predCube.png', save_dir=wandb_dir)\n",
    "plot_slice(x, y, z, v_init.reshape(X.shape)[:,:,:,0], 0, y.size-1, z.size-1, fig_name='v_initCube.png', save_dir=wandb_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0f6e30-8007-4028-a75b-68f14c81aff5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
