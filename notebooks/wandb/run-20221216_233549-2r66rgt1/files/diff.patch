diff --git a/environment.yml b/environment.yml
index 09c7d17..4e2ca25 100755
--- a/environment.yml
+++ b/environment.yml
@@ -1,4 +1,4 @@
-name: my_env
+name: my_env_plot
 channels:
   - conda-forge
   - defaults
@@ -311,7 +311,6 @@ dependencies:
       - pyrsistent==0.18.1
       - pytorch-lightning==1.7.2
       - pytz==2022.2.1
-      - pyvista==0.37.0
       - pyyaml==5.4.1
       - pyzmq==23.2.1
       - qtconsole==5.3.1
diff --git a/install_env.sh b/install_env.sh
index 33e06c7..14a199b 100755
--- a/install_env.sh
+++ b/install_env.sh
@@ -9,7 +9,7 @@ echo 'Creating Package environment'
 # Create conda env
 conda env create -f environment.yml
 source ~/miniconda3/etc/profile.d/conda.sh
-conda activate my_env
+conda activate my_env_plot
 
 # Install eikonal solver: fast-marching with Pykonal
 pip install -e git+https://github.com/malcolmw/pykonal@373a7d4#egg=pykonal
@@ -17,6 +17,9 @@ pip install -e git+https://github.com/malcolmw/pykonal@373a7d4#egg=pykonal
 # Install hcpinnseikonal package
 pip install -e .
 
+# Install packages for 3D plotting on Jupyterlab
+pip install pyvista[all]
+
 conda env list
 echo 'Created and activated environment:' $(which python)
 
diff --git a/notebooks/Example-1.ipynb b/notebooks/Example-1.ipynb
index 913c325..e58aef2 100644
--- a/notebooks/Example-1.ipynb
+++ b/notebooks/Example-1.ipynb
@@ -9,6 +9,8 @@
    "source": [
     "# Example 1\n",
     "\n",
+    "## 2D Case\n",
+    "\n",
     "**Content**\n",
     "\n",
     "This notebook reproduces the first example of the paper. It consists of four main subheadings;\n",
diff --git a/src/hcpinnseikonal.egg-info/PKG-INFO b/src/hcpinnseikonal.egg-info/PKG-INFO
index d2e025e..c293d2c 100644
--- a/src/hcpinnseikonal.egg-info/PKG-INFO
+++ b/src/hcpinnseikonal.egg-info/PKG-INFO
@@ -1,9 +1,9 @@
 Metadata-Version: 2.1
 Name: hcpinnseikonal
-Version: 0.1.dev9+g307789d.d20221210
+Version: 0.1.dev67+ge94359d.d20221213
 Summary: Repository for DWXXX: A robust seismic tomography framework via physics-informed machine learning with hard constrained data.
-Author: Mohammad Taufik, Tariq Alkhalifah, Umair bin Waheed
-Author-email: mohammad.taufik@kaust.edu.sa, tariq.alkhalifah@kaust.edu.sa, umair.waheed@kfupm.edu.sa
+Author: Mohammad Taufik, Tariq Alkhalifah
+Author-email: mohammad.taufik@kaust.edu.sa, tariq.alkhalifah@kaust.edu.sa
 Keywords: inverse problems,deep learning,tomography,pinns,pde,seismic
 Classifier: Intended Audience :: Science/Research
 Classifier: Natural Language :: English
@@ -13,26 +13,29 @@ Classifier: Topic :: Scientific/Engineering :: Mathematics
 
 ![LOGO](asset/logo.png)  
 
-Reproducible material for **A robust seismic tomography framework via physics-informed machine learning with hard constrained data - Taufik M., Alkhalifah T., and Waheed U.**
+Reproducible material for **A robust seismic tomography framework via physics-informed machine learning with hard constrained data - Taufik M. and Alkhalifah T.**
 
 
 # Project structure
 This repository is organized as follows:
 
-* :open_file_folder: **package**: python library containing routines for ....;
-* :open_file_folder: **asset**: folder containing logo;
-* :open_file_folder: **data**: folder containing data (or instructions on how to retrieve the data
-* :open_file_folder: **notebooks**: set of jupyter notebooks reproducing the experiments in the paper (see below for more details);
-* :open_file_folder: **scripts**: set of python scripts used to run multiple experiments ...
+* :open_file_folder: **asset**: folder containing logo.
+* :open_file_folder: **data**: folder containing the cropped [Marmousi](https://wiki.seg.org/wiki/Dictionary:Marmousi_model) model.
+* :open_file_folder: **notebooks**: set of jupyter notebooks reproducing the experiments in the paper (see below for more details).
+* :open_file_folder: **scripts**: set of (bash and python) scripts used to run multiple experiments.
+* :open_file_folder: **src**: folder containing materials for the *hcpinnseikonal* package.
 
 ## Notebooks
 The following notebooks are provided:
 
-- :orange_book: ``X1.ipynb``: notebook performing ...;
-- :orange_book: ``X2.ipynb``: notebook performing ...
+- :orange_book: ``Example-1.ipynb``: notebook performing a surface tomography acquisition with sparse source-receiver sampling.
 
+## Scripts
+The following scripts are provided:
 
-## Getting started :space_invader: :robot:
+- :page_with_curl: ``Example-1.sh``: script to perform different receiver sampling experiment for the acquisition perform in the ``Example-1.ipynb`` and ``Example-1.py``.
+
+## Getting started
 To ensure reproducibility of the results, we suggest using the `environment.yml` file when creating an environment.
 
 Simply run:
@@ -50,4 +53,4 @@ conda activate my_env
 configurations may be required for different combinations of workstation and GPU.
 
 ## Cite us 
-DWXXX - Taufik et al. (2022) Report title.
+DW0003 - Taufik and Alkhalifah (2022) A robust seismic tomography framework via physics-informed machine learning with hard constrained data.
diff --git a/src/hcpinnseikonal.egg-info/SOURCES.txt b/src/hcpinnseikonal.egg-info/SOURCES.txt
index 4fec461..604a4aa 100644
--- a/src/hcpinnseikonal.egg-info/SOURCES.txt
+++ b/src/hcpinnseikonal.egg-info/SOURCES.txt
@@ -2,21 +2,25 @@
 README.md
 environment.yml
 install_env.sh
-requirements.txt
 setup.py
 asset/logo.png
 asset/placeholder
+data/marmousi.bin
 data/placeholder
+notebooks/Example-1.ipynb
 notebooks/placeholder
-package/__init__.py
+scripts/Example-1.py
+scripts/Example-1.sh
 scripts/placeholder
 src/hcpinnseikonal/__init__.py
+src/hcpinnseikonal/arguments.py
+src/hcpinnseikonal/model.py
 src/hcpinnseikonal/plot.py
 src/hcpinnseikonal/train.py
+src/hcpinnseikonal/train3d.py
 src/hcpinnseikonal/utils.py
 src/hcpinnseikonal/version.py
 src/hcpinnseikonal.egg-info/PKG-INFO
 src/hcpinnseikonal.egg-info/SOURCES.txt
 src/hcpinnseikonal.egg-info/dependency_links.txt
-src/hcpinnseikonal.egg-info/requires.txt
 src/hcpinnseikonal.egg-info/top_level.txt
\ No newline at end of file
diff --git a/src/hcpinnseikonal.egg-info/requires.txt b/src/hcpinnseikonal.egg-info/requires.txt
deleted file mode 100644
index d678af1..0000000
--- a/src/hcpinnseikonal.egg-info/requires.txt
+++ /dev/null
@@ -1,2 +0,0 @@
-numpy>=1.15.0
-torch>=1.2.0
diff --git a/src/hcpinnseikonal/arguments.py b/src/hcpinnseikonal/arguments.py
index d4f57c0..c159f7e 100644
--- a/src/hcpinnseikonal/arguments.py
+++ b/src/hcpinnseikonal/arguments.py
@@ -355,4 +355,16 @@ parser.add_argument(
     type=str,
     default='GFATT_PINNs-11-pytorch-surface-inversion',
     help="The wandb project name when it is enabled.",
-)
\ No newline at end of file
+)
+parser.add_argument(
+    "--regularization_type",
+    type=str,
+    default='None',
+    help="Types of regularization scheme (e.g., isotropic-TV, 1st-Tikhonov, 2nd-Tikhonov)",
+)
+parser.add_argument(
+    "--regularization_weight",
+    type=float,
+    default=0.,
+    help="Regularization weighting coefficient",
+)
diff --git a/src/hcpinnseikonal/plot.py b/src/hcpinnseikonal/plot.py
index 96d2369..d64cabc 100644
--- a/src/hcpinnseikonal/plot.py
+++ b/src/hcpinnseikonal/plot.py
@@ -14,6 +14,40 @@ plt.rcParams['xtick.bottom'] = plt.rcParams['xtick.labelbottom'] = False
 plt.rcParams['xtick.top'] = plt.rcParams['xtick.labeltop'] = True
 plt.rcParams['figure.figsize'] =  [6.4, 4.8]
 
+import pyvista as pv
+
+from pyvista import examples
+
+# values = np.repeat(vel[:, np.newaxis, :], len(x), axis=1)
+
+def plot_cube(values, xmin, ymin, zmin, deltax, deltay, deltaz, fig_name=None, save_dir='./'):
+
+    # Create the spatial reference
+    grid = pv.UniformGrid()
+
+    # Set the grid dimensions: shape + 1 because we want to inject our values on
+    grid.dimensions = np.array(values.shape) + 1
+
+    # Edit the spatial reference
+    grid.spacing = (deltax, deltax, deltax)  # The bottom left corner of the data set
+    grid.origin = (xmin, ymin, zmin)  # These are the cell sizes along each axis
+
+    # Add the data values to the cell data
+    grid.cell_data["values"] = values.flatten(order="F")  # Flatten the array!
+
+    cmap = plt.cm.get_cmap("terrain", 4)
+
+    # Now plot the grid!
+    grid.plot(show_edges=True, cmap=cmap, jupyter_backend='pythreejs', background='white', show_axes=True)
+
+    # # Plot the slice
+    # slices = grid.slice_orthogonal(x=2, y=2, z=3)
+    # slices.plot(cmap=cmap, jupyter_backend='pythreejs', background='white', show_axes=True)
+    
+    if fig_name is not None:
+        plt.savefig(os.path.join(save_dir, fig_name), 
+                    format='png', bbox_inches="tight") 
+
 def plot_contour(pred, true, init, idx, nx, nz, ns, sx, sz, x, z, fig_name=None, save_dir='./'):
     plt.figure()
     c_p = plt.contour(pred.reshape(nz,nx,ns)[:,:,idx],20, 
diff --git a/src/hcpinnseikonal/train.py b/src/hcpinnseikonal/train.py
index 42cbb51..40c486f 100644
--- a/src/hcpinnseikonal/train.py
+++ b/src/hcpinnseikonal/train.py
@@ -140,7 +140,14 @@ def train(input_wosrc, sx, sz,
             delt = (1-0.01)/args['num_epochs']
             wl2 = torch.abs(torch.exp((-1+delt*epoch)*args['causality_factor']*torch.sqrt((xz[:,0]-s)**2+(xz[:,1]-z[args['zid_source']])**2))-1) + (1-torch.exp(torch.tensor(-5e-4*epoch)))
         
-        ls_pde = torch.mean(wl2*pde**2)
+        if args.regularization_type=='None':
+            ls_pde = torch.mean(wl2*pde**2)
+        elif args.regularization_type=='isotropic-TV':
+            dv_d1 = torch.autograd.grad(v, xzsic, torch.ones_like(v), create_graph=True)[0]
+            # dv_d2 = torch.autograd.grad(dv_d1, xzsic, torch.ones_like(v), create_graph=True)[0]
+            L1 = nn.L1Loss(reduction='sum')
+            ls_pde = torch.mean(wl2*pde**2) + args.regularization_weight * (L1(torch.dv_d1[:,0], 0) + L1(torch.dv_d1[:,1], 0))
+        
         ls = ls_pde
         loss.append(ls.item())
         ls.backward()
diff --git a/src/hcpinnseikonal/utils.py b/src/hcpinnseikonal/utils.py
index 94a8a58..99213ea 100644
--- a/src/hcpinnseikonal/utils.py
+++ b/src/hcpinnseikonal/utils.py
@@ -181,6 +181,94 @@ def create_dataloader(input_vec, sx, sz, batch_size=200**3, shuffle='y',
 
     return data_loader, ic.T
 
+def create_dataloader3d(input_vec, sx, sy, sz, batch_size=200**4, shuffle='y', 
+                      device='cuda', fast_loader='n', perm_id=None):
+    
+    XYZ = torch.from_numpy(np.vstack((input_vec[0], input_vec[1], input_vec[2])).T).float().to(device)
+    SX = torch.from_numpy(input_vec[3]).float().to(device)
+    SY = torch.from_numpy(input_vec[4]).float().to(device)
+    SZ = torch.from_numpy(input_vec[5]).float().to(device)
+    
+    taud = torch.from_numpy(input_vec[6]).float().to(device)
+    taud_dx = torch.from_numpy(input_vec[7]).float().to(device)
+    taud_dy = torch.from_numpy(input_vec[8]).float().to(device)
+
+    tana = torch.from_numpy(input_vec[9]).float().to(device)
+    tana_dx = torch.from_numpy(input_vec[10]).float().to(device)
+    tana_dy = torch.from_numpy(input_vec[11]).float().to(device)
+    tana_dz = torch.from_numpy(input_vec[12]).float().to(device)
+    
+    index = torch.arange(input_vec[0].size)
+    
+    if perm_id is not None:
+        dataset = TensorDataset(XYZ[perm_id], SX[perm_id], SY[perm_id], SZ[perm_id], taud[perm_id], 
+                                taud_dx[perm_id], taud_dy[perm_id], 
+                                tana[perm_id], tana_dx[perm_id], 
+                                tana_dy[perm_id], tana_dz[perm_id], index[perm_id])
+    else:
+        dataset = TensorDataset(XYZ, SX, SY, SZ, taud, taud_dx, taud_dy, tana, tana_dx, tana_dy, tana_dz, index)
+    
+    if fast_loader:
+        data_loader = FastTensorDataLoader(XYZ, SX, SY, SZ, taud, taud_dx, 
+                                           taud_dy, tana, tana_dx, tana_dy, tana_dz, index, 
+                                           batch_size=batch_size, shuffle=shuffle)
+    else:
+        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)
+
+    # initial condition
+    ic = torch.tensor(np.array([sx, sy, sz]), dtype=torch.float).to(device)
+
+    return data_loader, ic.T
+
+def create_dataloaderdd(input_vec, sx1, sz1, sx2, sz2, 
+                        batch_size=200**3, shuffle='y', 
+                        device='cuda', fast_loader='n', perm_id=None):
+    
+    XZ = torch.from_numpy(np.vstack((input_vec[0], input_vec[1])).T).float().to(device)
+    SX1 = torch.from_numpy(input_vec[2]).float().to(device)
+    SX2 = torch.from_numpy(input_vec[3]).float().to(device)
+    
+    taud1 = torch.from_numpy(input_vec[4]).float().to(device)
+    taud_dx1 = torch.from_numpy(input_vec[5]).float().to(device)
+    
+    taud2 = torch.from_numpy(input_vec[6]).float().to(device)
+    taud_dx2 = torch.from_numpy(input_vec[7]).float().to(device)
+
+    tana1 = torch.from_numpy(input_vec[8]).float().to(device)
+    tana_dx1 = torch.from_numpy(input_vec[9]).float().to(device)
+    tana_dz1 = torch.from_numpy(input_vec[10]).float().to(device)
+    
+    tana2 = torch.from_numpy(input_vec[11]).float().to(device)
+    tana_dx2 = torch.from_numpy(input_vec[12]).float().to(device)
+    tana_dz2 = torch.from_numpy(input_vec[13]).float().to(device)
+    
+    index = torch.arange(input_vec[0].size)
+    
+    if perm_id is not None:
+        dataset = TensorDataset(XZ[perm_id], SX1[perm_id], SX2[perm_id], 
+                                taud1[perm_id], taud_dx1[perm_id], 
+                                taud2[perm_id], taud_dx2[perm_id], 
+                                tana1[perm_id], tana_dx1[perm_id], tana_dz1[perm_id],
+                                tana2[perm_id], tana_dx2[perm_id], tana_dz2[perm_id], index[perm_id])
+    else:
+        dataset = TensorDataset(XZ, SX1, SX2, taud1, taud_dx1, taud2, taud_dx2, 
+                                tana1, tana_dx1, tana_dz1, 
+                                tana2, tana_dx2, tana_dz2, index)
+    
+    if fast_loader:
+        data_loader = FastTensorDataLoader(XZ, SX1, SX2, taud1, taud_dx1, taud2, taud_dx2, 
+                                           tana1, tana_dx1, tana_dz1, 
+                                           tana2, tana_dx2, tana_dz2, index, 
+                                           batch_size=batch_size, shuffle=shuffle)
+    else:
+        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)
+
+    # initial condition
+    ic1 = torch.tensor(np.array([sx1, sz1]), dtype=torch.float).to(device)
+    ic2 = torch.tensor(np.array([sx2, sz2]), dtype=torch.float).to(device)
+
+    return data_loader, ic1.T, ic2.T
+
 def set_seed(seed):
 
     random.seed(seed)
diff --git a/src/hcpinnseikonal/version.py b/src/hcpinnseikonal/version.py
index 92e38ba..d78f7bd 100644
--- a/src/hcpinnseikonal/version.py
+++ b/src/hcpinnseikonal/version.py
@@ -1,5 +1,5 @@
 # coding: utf-8
 # file generated by setuptools_scm
 # don't change, don't track in version control
-__version__ = version = '0.1.dev55+g8d9691f.d20221211'
-__version_tuple__ = version_tuple = (0, 1, 'dev55', 'g8d9691f.d20221211')
+__version__ = version = '0.1.dev67+ge94359d.d20221213'
+__version_tuple__ = version_tuple = (0, 1, 'dev67', 'ge94359d.d20221213')
