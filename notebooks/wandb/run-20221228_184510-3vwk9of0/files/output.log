Multiprocessing is handled by SLURM.
Using 16bit native Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Global seed set to 1234
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 1 processes
----------------------------------------------------------------------------------------------------
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/taufikmh/KAUST/fall_2022/external_repos/HCPINNsEikonal3D/src/hcpinnseikonal/distributed.py:690: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  scheduler = {'scheduler': ReduceLROnPlateau(self.optimizer, mode='min', factor=0.5, patience=self.num_epochs//self.reduce_after, verbose=True),
  | Name      | Type                  | Params
----------------------------------------------------
0 | tau_model | FullyConnectedNetwork | 6.7 K
1 | v_model   | FullyConnectedNetwork | 1.8 K
----------------------------------------------------
8.5 K     Trainable params
0         Non-trainable params
8.5 K     Total params
0.017     Total estimated model params size (MB)
SLURM auto-requeueing enabled. Setting signal handlers.
/home/taufikmh/miniconda3/envs/my_env_plot/lib/python3.8/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 24 worker processes in total. Our suggested max number of worker in current system is 3, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/taufikmh/miniconda3/envs/my_env_plot/lib/python3.8/site-packages/torch/autograd/__init__.py:173: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [1, 24], strides() = [1, 1]
bucket_view.sizes() = [1, 24], strides() = [24, 1] (Triggered internally at  ../torch/csrc/distributed/c10d/reducer.cpp:326.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Epoch 00185: reducing learning rate of group 0 to 2.5000e-04.
Epoch 00303: reducing learning rate of group 0 to 1.2500e-04.
Epoch 00458: reducing learning rate of group 0 to 6.2500e-05.
Epoch 00522: reducing learning rate of group 0 to 3.1250e-05.
Epoch 00558: reducing learning rate of group 0 to 1.5625e-05.
Epoch 00579: reducing learning rate of group 0 to 7.8125e-06.
Epoch 00600: reducing learning rate of group 0 to 3.9063e-06.
Epoch 00621: reducing learning rate of group 0 to 1.9531e-06.
Epoch 00642: reducing learning rate of group 0 to 9.7656e-07.
Epoch 00663: reducing learning rate of group 0 to 4.8828e-07.
Epoch 00684: reducing learning rate of group 0 to 2.4414e-07.
Epoch 00705: reducing learning rate of group 0 to 1.2207e-07.
Epoch 00726: reducing learning rate of group 0 to 6.1035e-08.
Epoch 00747: reducing learning rate of group 0 to 3.0518e-08.
Epoch 00768: reducing learning rate of group 0 to 1.5259e-08.
`Trainer.fit` stopped: `max_epochs=1000` reached.
