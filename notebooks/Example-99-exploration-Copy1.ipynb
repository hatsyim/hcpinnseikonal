{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ac30ec7-8aa2-4709-9ed5-a1adb1706e72",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2eb17f8-1950-48f9-b96a-6ec2cbca6ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lateral_spacing': 0.01, 'vertical_spacing': 0.01, 'max_offset': 5.0, 'max_depth': 1.0, 'rec_spacing': 10, 'sou_spacing': 10, 'num_epochs': 250, 'num_neurons': 20, 'num_layers': 10, 'learning_rate': 0.001, 'model_type': 'seam', 'data_type': 'full', 'middle_shot': 'n', 'until_cmb': 'n', 'earth_scale': 'n', 'scale_factor': 10, 'reduce_after': 15, 'seed': 123, 'initialization': 'varianceScaling', 'plotting_factor': 1, 'rescale_plot': 'n', 'depth_shift': 'n', 'tau_multiplier': 3.0, 'initial_velocity': 4, 'zid_source': 5, 'zid_receiver': 0, 'explode_reflector': 'n', 'field_synthetic': 'n', 'v_multiplier': 3, 'activation': 'elu', 'num_points': 1.0, 'irregular_grid': 'n', 'xid_well': 5, 'last_vmultiplier': 5, 'v_units': 'unitless', 'well_depth': None, 'exp_function': 'n', 'exp_factor': 1.0, 'exclude_topo': 'n', 'exclude_well': 'n', 'exclude_source': 'n', 'loss_function': 'mse', 'station_factor': 1.0, 'event_factor': 1.0, 'checker_size': 5.0, 'tau_act': 'None', 'empty_middle': 'n', 'factorization_type': 'multiplicative', 'causality_factor': 1.0, 'causality_weight': 'type_0', 'residual_network': 'n', 'velocity_loss': 'n', 'regular_station': 'n', 'data_neurons': 16, 'data_layers': 8, 'append_shot': 'n', 'use_wandb': 'n', 'save_folder': './', 'project_name': 'GFATT_PINNs-21-3d-field', 'regularization_type': 'None', 'regularization_weight': 0.0, 'optimizer': 'adam', 'mixed_precision': 'n', 'fast_loader': 'n', 'sampling_rate': 4, 'initial_mean': 0.5, 'initial_bias': 0.5, 'initial_deviation': 0.5, 'dual_optimizer': 'n', 'tau_function': 'l2', 'v_function': 'l2', 'with_well': 'n'}\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torchvision\n",
    "\n",
    "from argparse import ArgumentParser   \n",
    "from scipy import interpolate\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "from hcpinnseikonal.utils import *\n",
    "from hcpinnseikonal.model import *\n",
    "from hcpinnseikonal.train3dwell import *\n",
    "from hcpinnseikonal.plot import *\n",
    "from hcpinnseikonal.arguments import *\n",
    "\n",
    "args = parser.parse_args([])\n",
    "\n",
    "args.use_wandb='n'\n",
    "args.project_name='GFATT_PINNs-21-3d-field'\n",
    "\n",
    "dict_args = vars(args)\n",
    "print(dict_args)\n",
    "\n",
    "# Change these lines for the wandb setup\n",
    "if args.use_wandb=='y':\n",
    "    wandb.init(project=args.project_name)\n",
    "    wandb.run.log_code(\".\")\n",
    "    wandb_dir = wandb.run.dir\n",
    "else:\n",
    "    args.save_folder='../saves/saves_randomPINNs'\n",
    "    from pathlib import Path\n",
    "    Path(args.save_folder).mkdir(parents=True, exist_ok=True)\n",
    "    wandb_dir = args.save_folder\n",
    "    \n",
    "# plt.style.use(\"~/science.mplstyle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bb04d9-4e71-4043-82cf-c89e95cab2d4",
   "metadata": {},
   "source": [
    "# Random Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b21fe923-eb2d-4b00-8dc7-5612c213ff8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "from timeit import default_timer\n",
    "\n",
    "\n",
    "class GaussianRandomField(object):\n",
    "\n",
    "    def __init__(self, dim, size, alpha=2, tau=3, sigma=None, boundary=\"periodic\", device=None):\n",
    "\n",
    "        self.dim = dim\n",
    "        self.device = device\n",
    "\n",
    "        if sigma is None:\n",
    "            sigma = tau**(0.5*(2*alpha - self.dim))\n",
    "\n",
    "        k_max = size//2\n",
    "\n",
    "        if dim == 1:\n",
    "            k = torch.cat((torch.arange(start=0, end=k_max, step=1, device=device), \\\n",
    "                           torch.arange(start=-k_max, end=0, step=1, device=device)), 0)\n",
    "\n",
    "            self.sqrt_eig = size*math.sqrt(2.0)*sigma*((4*(math.pi**2)*(k**2) + tau**2)**(-alpha/2.0))\n",
    "            self.sqrt_eig[0] = 0.0\n",
    "\n",
    "        elif dim == 2:\n",
    "            wavenumers = torch.cat((torch.arange(start=0, end=k_max, step=1, device=device), \\\n",
    "                                    torch.arange(start=-k_max, end=0, step=1, device=device)), 0).repeat(size,1)\n",
    "\n",
    "            k_x = wavenumers.transpose(0,1)\n",
    "            k_y = wavenumers\n",
    "\n",
    "            self.sqrt_eig = (size**2)*math.sqrt(2.0)*sigma*((4*(math.pi**2)*(k_x**2 + k_y**2) + tau**2)**(-alpha/2.0))\n",
    "            self.sqrt_eig[0,0] = 0.0\n",
    "\n",
    "        elif dim == 3:\n",
    "            wavenumers = torch.cat((torch.arange(start=0, end=k_max, step=1, device=device), \\\n",
    "                                    torch.arange(start=-k_max, end=0, step=1, device=device)), 0).repeat(size,size,1)\n",
    "\n",
    "            k_x = wavenumers.transpose(1,2)\n",
    "            k_y = wavenumers\n",
    "            k_z = wavenumers.transpose(0,2)\n",
    "\n",
    "            self.sqrt_eig = (size**3)*math.sqrt(2.0)*sigma*((4*(math.pi**2)*(k_x**2 + k_y**2 + k_z**2) + tau**2)**(-alpha/2.0))\n",
    "            self.sqrt_eig[0,0,0] = 0.0\n",
    "\n",
    "        self.size = []\n",
    "        for j in range(self.dim):\n",
    "            self.size.append(size)\n",
    "\n",
    "        self.size = tuple(self.size)\n",
    "\n",
    "    def sample(self, N):\n",
    "\n",
    "        coeff = torch.randn(N, *self.size, dtype=torch.cfloat, device=self.device)\n",
    "        coeff = self.sqrt_eig * coeff\n",
    "\n",
    "        return torch.fft.ifftn(coeff, dim=list(range(-1, -self.dim - 1, -1))).real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "754727a8-0277-4e51-b915-88c4ccee8294",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 250.00 MiB (GPU 0; 47.45 GiB total capacity; 250.25 MiB already allocated; 79.25 MiB free; 252.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m random_field \u001b[38;5;241m=\u001b[39m GaussianRandomField(\u001b[38;5;241m2\u001b[39m, s, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2.5\u001b[39m, tau\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Sample random feilds\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[43mrandom_field\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Plot\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m100\u001b[39m):\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mGaussianRandomField.sample\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(\u001b[38;5;28mself\u001b[39m, N):\n\u001b[1;32m     55\u001b[0m     coeff \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(N, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcfloat, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 56\u001b[0m     coeff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt_eig\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcoeff\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfft\u001b[38;5;241m.\u001b[39mifftn(coeff, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)))\u001b[38;5;241m.\u001b[39mreal\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 250.00 MiB (GPU 0; 47.45 GiB total capacity; 250.25 MiB already allocated; 79.25 MiB free; 252.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Resolution\n",
    "s = 256\n",
    "\n",
    "# Number of solutions to generate\n",
    "N = 500\n",
    "\n",
    "#Set up 2d GRF with covariance parameters\n",
    "random_field = GaussianRandomField(2, s, alpha=2.5, tau=7, device='cuda')\n",
    "\n",
    "# Sample random feilds\n",
    "samples = random_field.sample(N) + 5\n",
    " \n",
    "# Plot\n",
    "for i in range(N//100):\n",
    "    plot_section(samples[i,:,:].detach().cpu().numpy(), 'samples_'+str(i)+'_.pdf', \n",
    "                 save_dir='./', aspect='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d3edce-604c-47ba-a82e-141716dfddfc",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5243627e-0539-4ad9-8f83-c0296964955f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, target, transform=None):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.target[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "            \n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbef3d0b-7b5b-468d-aa19-e85d8c9e9cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "C, H, W = 1, 256, 256\n",
    "\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToPILImage(), torchvision.transforms.ToTensor()])\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset = MyDataset(samples[:400,:,:].reshape(-1, 1, H, W,), samples[:400,:,:].reshape(-1, 1, H, W,), transform)\n",
    "val_dataset = MyDataset(samples[400:,:,:].reshape(-1, 1, H, W,), samples[400:,:,:].reshape(-1, 1, H, W,), transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46347844-7db0-40bd-ab66-0231df622255",
   "metadata": {},
   "source": [
    "# UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5210642-327a-40cc-be46-71402090dbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=1,\n",
    "        n_classes=2,\n",
    "        depth=5,\n",
    "        wf=6,\n",
    "        padding=False,\n",
    "        batch_norm=False,\n",
    "        up_mode='upconv',\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Implementation of\n",
    "        U-Net: Convolutional Networks for Biomedical Image Segmentation\n",
    "        (Ronneberger et al., 2015)\n",
    "        https://arxiv.org/abs/1505.04597\n",
    "        Using the default arguments will yield the exact version used\n",
    "        in the original paper\n",
    "        Args:\n",
    "            in_channels (int): number of input channels\n",
    "            n_classes (int): number of output channels\n",
    "            depth (int): depth of the network\n",
    "            wf (int): number of filters in the first layer is 2**wf\n",
    "            padding (bool): if True, apply padding such that the input shape\n",
    "                            is the same as the output.\n",
    "                            This may introduce artifacts\n",
    "            batch_norm (bool): Use BatchNorm after layers with an\n",
    "                               activation function\n",
    "            up_mode (str): one of 'upconv' or 'upsample'.\n",
    "                           'upconv' will use transposed convolutions for\n",
    "                           learned upsampling.\n",
    "                           'upsample' will use bilinear upsampling.\n",
    "        \"\"\"\n",
    "        super(UNet, self).__init__()\n",
    "        assert up_mode in ('upconv', 'upsample')\n",
    "        self.padding = padding\n",
    "        self.depth = depth\n",
    "        prev_channels = in_channels\n",
    "        self.down_path = nn.ModuleList()\n",
    "        for i in range(depth):\n",
    "            self.down_path.append(\n",
    "                UNetConvBlock(prev_channels, 2 ** (wf + i), padding, batch_norm)\n",
    "            )\n",
    "            prev_channels = 2 ** (wf + i)\n",
    "\n",
    "        self.up_path = nn.ModuleList()\n",
    "        for i in reversed(range(depth - 1)):\n",
    "            self.up_path.append(\n",
    "                UNetUpBlock(prev_channels, 2 ** (wf + i), up_mode, padding, batch_norm)\n",
    "            )\n",
    "            prev_channels = 2 ** (wf + i)\n",
    "\n",
    "        self.last = nn.Conv2d(prev_channels, n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        blocks = []\n",
    "        for i, down in enumerate(self.down_path):\n",
    "            x = down(x)\n",
    "            if i != len(self.down_path) - 1:\n",
    "                blocks.append(x)\n",
    "                x = F.max_pool2d(x, 2)\n",
    "\n",
    "        for i, up in enumerate(self.up_path):\n",
    "            x = up(x, blocks[-i - 1])\n",
    "\n",
    "        output = self.last(x)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class UNetConvBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, padding, batch_norm):\n",
    "        super(UNetConvBlock, self).__init__()\n",
    "        block = []\n",
    "\n",
    "        block.append(nn.Conv2d(in_size, out_size, kernel_size=3, padding=int(padding)))\n",
    "        block.append(nn.ReLU())\n",
    "        if batch_norm:\n",
    "            block.append(nn.BatchNorm2d(out_size))\n",
    "\n",
    "        block.append(nn.Conv2d(out_size, out_size, kernel_size=3, padding=int(padding)))\n",
    "        block.append(nn.ReLU())\n",
    "        block.append(nn.Dropout2d(p=0.15)) # edited\n",
    "        if batch_norm:\n",
    "            block.append(nn.BatchNorm2d(out_size))\n",
    "\n",
    "        self.block = nn.Sequential(*block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.block(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class UNetUpBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, up_mode, padding, batch_norm):\n",
    "        super(UNetUpBlock, self).__init__()\n",
    "        if up_mode == 'upconv':\n",
    "            self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=2, stride=2)\n",
    "        elif up_mode == 'upsample':\n",
    "            self.up = nn.Sequential(\n",
    "                nn.Upsample(mode='bilinear', scale_factor=2),\n",
    "                nn.Conv2d(in_size, out_size, kernel_size=1),\n",
    "            )\n",
    "\n",
    "        self.conv_block = UNetConvBlock(in_size, out_size, padding, batch_norm)\n",
    "\n",
    "    def center_crop(self, layer, target_size):\n",
    "        _, _, layer_height, layer_width = layer.size()\n",
    "        diff_y = (layer_height - target_size[0]) // 2\n",
    "        diff_x = (layer_width - target_size[1]) // 2\n",
    "        return layer[\n",
    "            :, :, diff_y : (diff_y + target_size[0]), diff_x : (diff_x + target_size[1])\n",
    "        ]\n",
    "\n",
    "    def forward(self, x, bridge):\n",
    "        up = self.up(x)\n",
    "        crop1 = self.center_crop(bridge, up.shape[2:])\n",
    "        out = torch.cat([up, crop1], 1)\n",
    "        out = self.conv_block(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a463ee98-a896-479c-bc3d-555a3156afb0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3f2f8f-b749-46bd-82ba-0af20851045c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, time, glob, time, pdb\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print('device: ', device)\n",
    "\n",
    "script_time = time.time()\n",
    "\n",
    "def plot_losses(running_train_loss, running_val_loss, train_epoch_loss, val_epoch_loss, epoch):\n",
    "    fig = plt.figure(figsize=(16,16))\n",
    "    fig.suptitle('loss trends', fontsize=20)\n",
    "    ax1 = fig.add_subplot(221)\n",
    "    ax2 = fig.add_subplot(222)\n",
    "    ax3 = fig.add_subplot(223)\n",
    "    ax4 = fig.add_subplot(224)\n",
    "\n",
    "    ax1.title.set_text('epoch train loss VS #epochs')\n",
    "    ax1.set_xlabel('#epochs')\n",
    "    ax1.set_ylabel('epoch train loss')\n",
    "    ax1.plot(train_epoch_loss)\n",
    "    \n",
    "    ax2.title.set_text('epoch val loss VS #epochs')\n",
    "    ax2.set_xlabel('#epochs')\n",
    "    ax2.set_ylabel('epoch val loss')\n",
    "    ax2.plot(val_epoch_loss)\n",
    " \n",
    "    ax3.title.set_text('batch train loss VS #batches')\n",
    "    ax3.set_xlabel('#batches')\n",
    "    ax3.set_ylabel('batch train loss')\n",
    "    ax3.plot(running_train_loss)\n",
    "\n",
    "    ax4.title.set_text('batch val loss VS #batches')\n",
    "    ax4.set_xlabel('#batches')\n",
    "    ax4.set_ylabel('batch val loss')\n",
    "    ax4.plot(running_val_loss)\n",
    "    \n",
    "    plt.savefig(os.path.join('./','losses_{}.png'.format(str(epoch + 1).zfill(2))))\n",
    "\n",
    "# defining the model\n",
    "model = UNet(n_classes = 1, depth = 4, padding = True).to(device) # try decreasing the depth value if there is a memory error\n",
    "\n",
    "train_epoch_loss = []\n",
    "val_epoch_loss = []\n",
    "running_train_loss = []\n",
    "running_val_loss = []\n",
    "epochs_till_now = 0\n",
    "\n",
    "lr = 1e-5\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr = lr)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "log_interval = 20\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs_till_now, epochs_till_now+epochs):\n",
    "    print('\\n===== EPOCH {}/{} ====='.format(epoch + 1, epochs_till_now + epochs))    \n",
    "    print('\\nTRAINING...')\n",
    "    epoch_train_start_time = time.time()\n",
    "    model.train()\n",
    "    for batch_idx, (imgs, noisy_imgs) in enumerate(train_loader):\n",
    "        batch_start_time = time.time()\n",
    "        imgs = imgs.to(device)\n",
    "        noisy_imgs = noisy_imgs.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(noisy_imgs)\n",
    "\n",
    "        loss = loss_fn(out, imgs)\n",
    "        running_train_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (batch_idx + 1)%log_interval == 0:\n",
    "            batch_time = time.time() - batch_start_time\n",
    "            m,s = divmod(batch_time, 60)\n",
    "            print('train loss @batch_idx {}/{}: {} in {} mins {} secs (per batch)'.format(str(batch_idx+1).zfill(len(str(len(train_loader)))), len(train_loader), loss.item(), int(m), round(s, 2)))\n",
    "\n",
    "    train_epoch_loss.append(np.array(running_train_loss).mean())\n",
    "\n",
    "    epoch_train_time = time.time() - epoch_train_start_time\n",
    "    m,s = divmod(epoch_train_time, 60)\n",
    "    h,m = divmod(m, 60)\n",
    "    print('\\nepoch train time: {} hrs {} mins {} secs'.format(int(h), int(m), int(s)))\n",
    "\n",
    "    print('\\nVALIDATION...')\n",
    "    epoch_val_start_time = time.time()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (imgs, noisy_imgs) in enumerate(val_loader):\n",
    "\n",
    "            imgs = imgs.to(device)\n",
    "            noisy_imgs = noisy_imgs.to(device)\n",
    "\n",
    "            out = model(noisy_imgs)\n",
    "            loss = loss_fn(out, imgs)\n",
    "\n",
    "            running_val_loss.append(loss.item())\n",
    "\n",
    "            if (batch_idx + 1)%log_interval == 0:\n",
    "                print('val loss   @batch_idx {}/{}: {}'.format(str(batch_idx+1).zfill(len(str(len(val_loader)))), len(val_loader), loss.item()))\n",
    "\n",
    "    val_epoch_loss.append(np.array(running_val_loss).mean())\n",
    "\n",
    "    epoch_val_time = time.time() - epoch_val_start_time\n",
    "    m,s = divmod(epoch_val_time, 60)\n",
    "    h,m = divmod(m, 60)\n",
    "    print('\\nepoch val   time: {} hrs {} mins {} secs'.format(int(h), int(m), int(s)))\n",
    "\n",
    "    plot_losses(running_train_loss, running_val_loss, train_epoch_loss, val_epoch_loss,  epoch)   \n",
    "\n",
    "    torch.save({'model_state_dict': model.state_dict(), \n",
    "                'losses': {'running_train_loss': running_train_loss, \n",
    "                           'running_val_loss': running_val_loss, \n",
    "                           'train_epoch_loss': train_epoch_loss, \n",
    "                           'val_epoch_loss': val_epoch_loss}, \n",
    "                'epochs_till_now': epoch+1}, \n",
    "                os.path.join('./', 'model{}.pth'.format(str(epoch + 1).zfill(2))))\n",
    "\n",
    "total_script_time = time.time() - script_time\n",
    "m, s = divmod(total_script_time, 60)\n",
    "h, m = divmod(m, 60)\n",
    "print(f'\\ntotal time taken for running this script: {int(h)} hrs {int(m)} mins {int(s)} secs')\n",
    "  \n",
    "print('\\nFin.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432404a4-f5e4-41a4-b320-3213e99fe2b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb74f5bb-33eb-4284-b993-37894a50e3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_MLP(din, dout, hidden=None, nonlin=nn.ELU, output_nonlin=None, bias=True, output_bias=None):\n",
    "    '''\n",
    "    :param din: int\n",
    "    :param dout: int\n",
    "    :param hidden: ordered list of int - each element corresponds to a FC layer with that width (empty means network is not deep)\n",
    "    :param nonlin: str - choose from options found in get_nonlinearity(), applied after each intermediate layer\n",
    "    :param output_nonlin: str - nonlinearity to be applied after the last (output) layer\n",
    "    :return: an nn.Sequential instance with the corresponding layers\n",
    "    '''\n",
    "\n",
    "    if hidden is None:\n",
    "        hidden = []\n",
    "\n",
    "    if output_bias is None:\n",
    "        output_bias = bias\n",
    "\n",
    "    flatten = False\n",
    "    reshape = None\n",
    "\n",
    "    if isinstance(din, (tuple, list)):\n",
    "        flatten = True\n",
    "        din = int(np.product(din))\n",
    "    if isinstance(dout, (tuple, list)):\n",
    "        reshape = dout\n",
    "        dout = int(np.product(dout))\n",
    "\n",
    "    nonlins = [nonlin] * len(hidden) + [output_nonlin]\n",
    "    biases = [bias] * len(hidden) + [output_bias]\n",
    "    hidden = din, *hidden, dout\n",
    "\n",
    "    layers = []\n",
    "    if flatten:\n",
    "        layers.append(nn.Flatten())\n",
    "\n",
    "    for in_dim, out_dim, nonlin, bias in zip(hidden, hidden[1:], nonlins, biases):\n",
    "        layer = nn.Linear(in_dim, out_dim, bias=bias)\n",
    "        layers.append(layer)\n",
    "        if nonlin is not None:\n",
    "            layers.append(nonlin())\n",
    "\n",
    "    if reshape is not None:\n",
    "        layers.append(Reshaper(reshape))\n",
    "\n",
    "\n",
    "    net = nn.Sequential(*layers)\n",
    "\n",
    "    net.din, net.dout = din, dout\n",
    "    return net\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int = 3, stride: int = 1, padding: int = 1,\n",
    "                 pool: Optional[str] = None, unpool: Optional[str] = None, pool_size: int = 2,\n",
    "                 nonlin: Optional[Type[nn.Module]] = nn.ELU, norm: Optional[str] = 'batch', **kwargs):\n",
    "        super().__init__()\n",
    "        self.unpool = None if unpool is None else nn.Upsample(scale_factor=pool_size, mode=unpool)\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, **kwargs)\n",
    "        self.pool = None if pool is None \\\n",
    "            else (nn.MaxPool2d(pool_size) if pool == 'max' else nn.AvgPool2d(pool_size))\n",
    "        if norm == 'batch':\n",
    "            self.norm = nn.BatchNorm2d(out_channels)\n",
    "        elif norm == 'instance':\n",
    "            self.norm = nn.InstanceNorm2d(out_channels)\n",
    "        elif norm == 'group':\n",
    "            self.norm = nn.GroupNorm(8, out_channels)\n",
    "        else:\n",
    "            self.norm = None\n",
    "        self.nonlin = nonlin()\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        c = self.unpool(x) if self.unpool is not None else x\n",
    "        c = self.conv(c)\n",
    "        if self.pool is not None:\n",
    "            c = self.pool(c)\n",
    "        if self.norm is not None:\n",
    "            c = self.norm(c)\n",
    "        if self.nonlin is not None:\n",
    "            c = self.nonlin(c)\n",
    "        return c\n",
    "\n",
    "class ConvAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoEncoder, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            ConvBlock( 3, 64, 5, 1, 2, pool='max', norm='group', nonlin=nn.ELU),\n",
    "            ConvBlock(64, 64, 3, 1, 1, pool='max', norm='group', nonlin=nn.ELU),\n",
    "            ConvBlock(64, 64, 3, 1, 1, pool='max', norm='group', nonlin=nn.ELU),\n",
    "            ConvBlock(64, 64, 3, 1, 1, pool='max', norm='group', nonlin=nn.ELU),\n",
    "            ConvBlock(64, 64, 3, 1, 1, pool='max', norm='group', nonlin=nn.ELU),\n",
    "            make_MLP((64, 2, 2), latent_dim*2, [256, 128], nonlin=nn.ELU),\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            make_MLP(latent_dim, (64, 2, 2), [128, 256], nonlin=nn.ELU),\n",
    "            ConvBlock(64, 64, 3, 1, 1, unpool='bilinear', norm='group', nonlin=nn.ELU),\n",
    "            ConvBlock(64, 64, 3, 1, 1, unpool='bilinear', norm='group', nonlin=nn.ELU),\n",
    "            ConvBlock(64, 64, 3, 1, 1, unpool='bilinear', norm='group', nonlin=nn.ELU),\n",
    "            ConvBlock(64, 64, 3, 1, 1, unpool='bilinear', norm='group', nonlin=nn.ELU),\n",
    "            ConvBlock(64,  3, 3, 1, 1, unpool='bilinear', norm=None, nonlin=nn.Sigmoid),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad017f77-2dbe-4c48-a278-751a33f2d051",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4464c0-8c1f-477c-8b50-8543344fcbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "if not os.path.exists('./mlp_img'):\n",
    "    os.mkdir('./mlp_img')\n",
    "\n",
    "\n",
    "def to_img(x):\n",
    "    x = 0.5 * (x + 1)\n",
    "    x = x.clamp(0, 1)\n",
    "    x = x.view(x.size(0), 1, 28, 28)\n",
    "    return x\n",
    "\n",
    "\n",
    "num_epochs = 100\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "dataset = MNIST('./data', transform=img_transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(True), nn.Linear(64, 12), nn.ReLU(True), nn.Linear(12, 3))\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(3, 12),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(12, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(True), nn.Linear(128, 28 * 28), nn.Tanh())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# model = AutoEncoder().cuda()\n",
    "\n",
    "model = ConvAutoEncoder.cuda()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for data in dataloader:\n",
    "        img, _ = data\n",
    "        img = img.view(img.size(0), -1)\n",
    "        img = Variable(img).cuda()\n",
    "        # ===================forward=====================\n",
    "        output = model(img)\n",
    "        loss = criterion(output, img)\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # ===================log========================\n",
    "    print('epoch [{}/{}], loss:{:.4f}'\n",
    "          .format(epoch + 1, num_epochs, loss.data[0]))\n",
    "    if epoch % 10 == 0:\n",
    "        pic = to_img(output.cpu().data)\n",
    "        save_image(pic, './mlp_img/image_{}.png'.format(epoch))\n",
    "\n",
    "torch.save(model.state_dict(), './sim_autoencoder.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
